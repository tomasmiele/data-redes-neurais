{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projetos de Redes Neurais e Deep Learning","text":""},{"location":"#autor","title":"Autor","text":"<ol> <li>Tomas R. Miele</li> </ol>"},{"location":"#entregas","title":"Entregas","text":""},{"location":"#exercicios","title":"Exerc\u00edcios:","text":"<ul> <li> Exerc\u00edcio 1 - Data 05/09/2025</li> <li> Exerc\u00edcio 2 - Data 12/09/2025</li> <li> Exerc\u00edcio 3 - Data 21/09/2025</li> <li> Exerc\u00edcio 4 - Data 26/10/2025</li> </ul>"},{"location":"#projetos","title":"Projetos:","text":"<ul> <li> Projeto 1 - Data 05/10/2025</li> <li> Projeto 2 - Data 19/10/2025</li> <li>[] Projeto 3 - Data 16/11/2025</li> </ul>"},{"location":"exercicio1/main/","title":"Exerc\u00edcio 1 - Data","text":""},{"location":"exercicio1/main/#objetivo","title":"Objetivo","text":"<p>Essa atividade foi elaborada para testar suas habilidades em gerar conjuntos de dados sint\u00e9ticos, lidar com desafios de dados do mundo real e preparar dados para serem alimentados em redes neurais.</p>"},{"location":"exercicio1/main/#exercicio-1","title":"Exerc\u00edcio 1","text":""},{"location":"exercicio1/main/#generate-the-data","title":"Generate the Data","text":"<pre><code>import numpy as np\n\nSEED = 42\nN_CLASSES = 4\nN_PER_CLASS = 100\n\nmeans = np.array([\n    [2.0, 3.0],\n    [5.0, 6.0],\n    [8.0, 1.0],\n    [15.0, 4.0],\n])\nstds = np.array([\n    [0.8, 2.5],\n    [1.2, 1.9],\n    [0.9, 0.9],\n    [0.5, 2.0],\n])\n\ndef generate_data(means: np.ndarray, stds: np.ndarray, n_per_class: int, seed: int = 0):\n    rng = np.random.default_rng(seed)\n    X_list, y_list = [], []\n    for c in range(len(means)):\n        x = rng.normal(loc=means[c, 0], scale=stds[c, 0], size=n_per_class)\n        y = rng.normal(loc=means[c, 1], scale=stds[c, 1], size=n_per_class)\n        X_list.append(np.column_stack([x, y]))\n        y_list.append(np.full(n_per_class, c, dtype=int))\n    X = np.vstack(X_list)\n    y = np.concatenate(y_list)\n    return X, y\n\nX, y = generate_data(means, stds, n_per_class=N_PER_CLASS, seed=SEED)\n\nprint(X, y)\n</code></pre>"},{"location":"exercicio1/main/#plot-the-data","title":"Plot the Data","text":"<p><pre><code>import matplotlib.pyplot as plt\n\ndef plot_scatter_with_linear_cuts(X: np.ndarray, y: np.ndarray, means: np.ndarray, save_path: str | None = None):\n    plt.figure(figsize=(8, 5))\n    for c in np.unique(y):\n        pts = X[y == c]\n        plt.scatter(pts[:, 0], pts[:, 1], s=18, alpha=0.7, label=f\"Class {c}\")\n\n    cuts = [\n        (means[0, 0] + means[1, 0]) / 2.0,\n        (means[1, 0] + means[2, 0]) / 2.0,\n        (means[2, 0] + means[3, 0]) / 2.0,\n    ]\n    for cx in cuts:\n        plt.axvline(cx, linestyle=\"--\", linewidth=1.5)\n\n    plt.title(\"Synthetic 2D dataset with suggested linear boundaries\")\n    plt.xlabel(\"Feature 1 (x)\")\n    plt.ylabel(\"Feature 2 (y)\")\n    plt.legend(loc=\"best\")\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n    plt.show()\n\nplot_scatter_with_linear_cuts(X, y, means, save_path=None)\n</code></pre> </p>"},{"location":"exercicio1/main/#analyse-and-draw-boundaries","title":"Analyse and Draw Boundaries","text":"<p>(a) Distribui\u00e7\u00e3o e sobreposi\u00e7\u00e3o</p> <ul> <li>Classe 0 (\u03bc \u2248 [2,3]): mais alongada no eixo y (\u03c3y = 2.5), formando uma \u201ccoluna\u201d vertical \u00e0 esquerda.  </li> <li>Classe 1 (\u03bc \u2248 [5,6]): vari\u00e2ncia moderada, localizada acima da Classe 2 em y, separada da Classe 0 em x.  </li> <li>Classe 2 (\u03bc \u2248 [8,1]): mais compacta e abaixo da Classe 1 em y.  </li> <li>Classe 3 (\u03bc \u2248 [15,4]): bem \u00e0 direita, com \u03c3y = 2.0, praticamente sem mistura com as demais.  </li> </ul> <p>(b) Uma fronteira linear simples separa todas as classes?</p> <p>Sim. Um conjunto de fronteiras lineares (retas verticais) \u00e9 suficiente para separar bem as quatro classes, pois os clusters est\u00e3o ordenados principalmente ao longo do eixo x.  </p> <p></p> <p>(c) Fronteiras sugeridas</p> <p>As fronteiras de decis\u00e3o podem ser esbo\u00e7adas como linhas verticais em:  </p> <ul> <li>x \u2248 3.5 (entre Classe 0 e Classe 1)  </li> <li>x \u2248 6.5 (entre Classe 1 e Classe 2)  </li> <li>x \u2248 11.5 (entre Classe 2 e Classe 3)</li> </ul>"},{"location":"exercicio1/main/#exercicio-2","title":"Exerc\u00edcio 2","text":""},{"location":"exercicio1/main/#generate-the-data_1","title":"Generate the Data","text":"<pre><code>import numpy as np\n\nSEED = 42\nrng = np.random.default_rng(SEED)\n\nmu_A = np.zeros(5)\nSigma_A = np.array([\n    [1.0, 0.8, 0.1, 0.0, 0.0],\n    [0.8, 1.0, 0.3, 0.0, 0.0],\n    [0.1, 0.3, 1.0, 0.5, 0.0],\n    [0.0, 0.0, 0.5, 1.0, 0.2],\n    [0.0, 0.0, 0.0, 0.2, 1.0],\n])\n\nmu_B = np.full(5, 1.5)\nSigma_B = np.array([\n    [1.5, -0.7, 0.2, 0.0, 0.0],\n    [-0.7, 1.5, 0.4, 0.0, 0.0],\n    [0.2, 0.4, 1.5, 0.6, 0.0],\n    [0.0, 0.0, 0.6, 1.5, 0.3],\n    [0.0, 0.0, 0.0, 0.3, 1.5],\n])\n\nnA = 500\nnB = 500\n\nXA = rng.multivariate_normal(mu_A, Sigma_A, size=nA)\nXB = rng.multivariate_normal(mu_B, Sigma_B, size=nB)\nX = np.vstack([XA, XB])\ny = np.array([0]*nA + [1]*nB)\n\nprint(X.shape, y.shape)\n</code></pre>"},{"location":"exercicio1/main/#apply-pca-and-plot-the-data","title":"Apply PCA and Plot the Data","text":"<pre><code>import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2, random_state=SEED)\nX2 = pca.fit_transform(X)\nevr = pca.explained_variance_ratio_\n\nplt.figure(figsize=(7,5))\nplt.scatter(X2[y==0,0], X2[y==0,1], s=14, alpha=0.7, label=\"Class A\")\nplt.scatter(X2[y==1,0], X2[y==1,1], s=14, alpha=0.7, label=\"Class B\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(f\"PCA (2D) \u2014 A vs B | Var. explained: PC1={evr[0]:.2f}, PC2={evr[1]:.2f}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"exercicio1/main/#analyse-the-plots","title":"Analyse the Plots","text":"<p>(a) Rela\u00e7\u00e3o entre as classes na proje\u00e7\u00e3o 2D</p> <ul> <li>As classes aparecem deslocadas ao longo de PC1 (efeito do vetor de m\u00e9dias).  </li> <li>H\u00e1 sobreposi\u00e7\u00e3o vis\u00edvel no centro: regi\u00f5es em que pontos das duas classes se misturam.  </li> <li>As covari\u00e2ncias diferentes d\u00e3o orienta\u00e7\u00f5es e alongamentos distintos aos clusters.  </li> </ul> <p>(b) Linear separability</p> <ul> <li>Um corte linear em PC1 poderia reduzir parte do erro, mas n\u00e3o separa perfeitamente A e B.  </li> <li>A estrutura \u00e9 n\u00e3o linear, devido \u00e0 sobreposi\u00e7\u00e3o e curvatura nas combina\u00e7\u00f5es de vari\u00e1veis originais (5D).  </li> <li>Modelos lineares simples (perceptron, regress\u00e3o log\u00edstica) ter\u00e3o dificuldade.  </li> </ul> <p>(c) Por que usar redes neurais</p> <ul> <li>Redes neurais com ativa\u00e7\u00f5es n\u00e3o lineares (ReLU, tanh, sigmoid) podem aprender fronteiras curvas adaptando-se \u00e0s orienta\u00e7\u00f5es internas dos clusters.  </li> <li>Assim, conseguem classificar melhor quando os dados n\u00e3o s\u00e3o linearmente separ\u00e1veis.</li> </ul>"},{"location":"exercicio1/main/#exercicio-3","title":"Exerc\u00edcio 3","text":""},{"location":"exercicio1/main/#describe-the-data","title":"Describe the Data","text":"<p>Objective: - O Spaceship Titanic \u00e9 um dataset de classifica\u00e7\u00e3o bin\u00e1ria. Coluna alvo (Transported) indica se o passageiro foi transportado para outra dimens\u00e3o ap\u00f3s o acidente da Spaceship Titanic.     - True: passageiro transportado     - False: passageiro n\u00e3o transportado - O objetivo \u00e9 prever Transported a partir dos dados dos passageiros.</p> <p>Features: - PassengerId: ID \u00fanico de cada passageiro - HomePlanet: planeta de origem do passageiro (categ\u00f3rica) - CryoSleep: se o passageiro estava em sono criog\u00eanico (categ\u00f3rica) - Cabin: n\u00famero da cabine (categ\u00f3rica) - Destination: destino da viagem (categ\u00f3rica) - VIP: se o passageiro era VIP (categ\u00f3rica) - Age: idade do passageiro (num\u00e9rica) - RoomService: gasto em servi\u00e7o de quarto (num\u00e9rica) - FoodCourt: gasto no restaurante Food Court (num\u00e9rica) - ShoppingMall: gasto em compras (num\u00e9rica) - Spa: gasto em spa (num\u00e9rica) - VRDeck: gasto em simulador VR (num\u00e9rica) - Transported \u2192 vari\u00e1vel alvo (target)</p> <p>Missing Data: <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"docs/exercicio1/spaceship-titanic/train.csv\")\n\nmissing_counts = df.isnull().sum()\nmissing = missing_counts[missing_counts &gt; 0].sort_values(ascending=False)\n\nprint(f\"Colunas com valores faltantes:\\n{missing}\")\n</code></pre> Resultado:</p> <p>Colunas com valores faltantes: CryoSleep       217 ShoppingMall    208 VIP             203 HomePlanet      201 Name            200 Cabin           199 VRDeck          188 FoodCourt       183 Spa             183 Destination     182 RoomService     181 Age             179</p>"},{"location":"exercicio1/main/#proccess-the-data","title":"Proccess the Data","text":"<pre><code>import pandas as pd\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\ndef split_passenger_id(s):\n    try:\n        g, n = s.split(\"_\")\n        return int(g), int(n)\n    except Exception:\n        return np.nan, np.nan\n\ndef split_cabin(s):\n    try:\n        deck, num, side = s.split(\"/\")\n        num = pd.to_numeric(num, errors=\"coerce\")\n        return deck, num, side\n    except Exception:\n        return np.nan, np.nan, np.nan\n\ndf = pd.read_csv(\"docs/exercicio1/spaceship-titanic/train.csv\")\n\ndf[[\"GroupId\", \"PassengerNum\"]] = df[\"PassengerId\"].apply(lambda s: pd.Series(split_passenger_id(s)))\ndf[[\"Deck\", \"CabinNum\", \"Side\"]] = df[\"Cabin\"].apply(lambda s: pd.Series(split_cabin(s)))\n\ndrop_cols = [\"PassengerId\", \"Name\", \"Cabin\"]\nfor c in drop_cols:\n    if c in df.columns:\n        df.drop(columns=c, inplace=True)\n\ntarget_col = \"Transported\"\nnum_cols = [\n    \"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\",\n    \"PassengerNum\", \"GroupId\", \"CabinNum\"\n]\ncat_cols = [\n    \"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Deck\", \"Side\"\n]\n\nnumeric_imputer = SimpleImputer(strategy=\"median\")\ncategorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\nohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n\nnumeric_pipeline = Pipeline(steps=[\n    (\"imputer\", numeric_imputer),\n    (\"scaler\", scaler),\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    (\"imputer\", categorical_imputer),\n    (\"onehot\", ohe),\n])\n\npre = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipeline, num_cols),\n        (\"cat\", categorical_pipeline, cat_cols),\n    ],\n    remainder=\"drop\"\n)\n\nX = df.drop(columns=[target_col]) if target_col in df.columns else df.copy()\ny = None\nif target_col in df.columns:\n    y = df[target_col].astype(\"int64\")\n\nX_proc = pre.fit_transform(X)\n\nnum_names = num_cols\ncat_names = []\nif len(cat_cols) &gt; 0:\n    ohe_feat_names = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"].get_feature_names_out(cat_cols)\n    cat_names = ohe_feat_names.tolist()\nfeature_names = num_names + cat_names\n\nX_proc_df = pd.DataFrame(X_proc, columns=feature_names, index=X.index)\nif y is not None:\n    X_proc_df[target_col] = y.values\n\nX_proc_df.to_csv(\"docs/exercicio1/spaceship-titanic/train_preprocessed.csv\", index=False)\n</code></pre>"},{"location":"exercicio1/main/#foodcourt-homeplanet-histograms","title":"FoodCourt &amp; HomePlanet Histograms","text":"<pre><code>import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\ndf[\"FoodCourt\"].hist(ax=axes[0], bins=30, color=\"skyblue\", edgecolor=\"black\")\naxes[0].set_title(\"FoodCourt (antes do scaling)\")\naxes[0].set_xlabel(\"Valor original\")\naxes[0].set_ylabel(\"Frequ\u00eancia\")\n\nfoodcourt_idx = feature_names.index(\"FoodCourt\")\npd.Series(X_proc[:, foodcourt_idx]).hist(ax=axes[1], bins=30, color=\"lightgreen\", edgecolor=\"black\")\naxes[1].set_title(\"FoodCourt (ap\u00f3s MinMaxScaler [-1,1])\")\naxes[1].set_xlabel(\"Valor escalado\")\naxes[1].set_ylabel(\"Frequ\u00eancia\")\n\nplt.tight_layout()\nplt.show()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\ndf[\"HomePlanet\"].value_counts().plot(kind=\"bar\", ax=axes[0], color=\"coral\", edgecolor=\"black\")\naxes[0].set_title(\"HomePlanet (antes do One-Hot)\")\naxes[0].set_xlabel(\"Planeta\")\naxes[0].set_ylabel(\"Contagem\")\n\nhomeplanet_cols = [c for c in feature_names if c.startswith(\"HomePlanet_\")]\npd.DataFrame(X_proc, columns=feature_names)[homeplanet_cols].sum().plot(kind=\"bar\", ax=axes[1], color=\"lightseagreen\", edgecolor=\"black\")\naxes[1].set_title(\"HomePlanet (ap\u00f3s One-Hot Encoding)\")\naxes[1].set_xlabel(\"Colunas geradas\")\naxes[1].set_ylabel(\"Contagem de 1s\")\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Escolhi mostrar o HomePlanet justamente porque o One-Hot n\u00e3o altera a distribui\u00e7\u00e3o original, apenas muda a representa\u00e7\u00e3o. Isso mostra que em algumas vari\u00e1veis o pr\u00e9-processamento n\u00e3o traz diferen\u00e7a pr\u00e1tica.</p>"},{"location":"exercicio2/main/","title":"Exerc\u00edcio 2 - Perceptron","text":""},{"location":"exercicio2/main/#objetivo","title":"Objetivo","text":"<p>O objetivo \u00e9 implementar um perceptron e analisar seu desempenho em dados separ\u00e1veis e n\u00e3o separ\u00e1veis.</p>"},{"location":"exercicio2/main/#exercicio-1","title":"Exerc\u00edcio 1","text":""},{"location":"exercicio2/main/#generate-the-data","title":"Generate the Data","text":"<pre><code>import numpy as np\n\nrng = np.random.default_rng(42)\n\nmean0 = np.array([1.5, 1.5])\ncov0  = np.array([[0.5, 0.0],[0.0, 0.5]])\n\nmean1 = np.array([5.0, 5.0])\ncov1  = np.array([[0.5, 0.0],[0.0, 0.5]])\n\nn = 1000\nX0 = rng.multivariate_normal(mean0, cov0, size=n)\nX1 = rng.multivariate_normal(mean1, cov1, size=n)\n\nX = np.vstack([X0, X1])\ny = np.hstack([-np.ones(n), np.ones(n)])\n</code></pre> <p>Esse c\u00f3digo gera 2000 pontos em 2D:</p> <ul> <li>Classe 0 centrada em (1.5,1.5)</li> <li>Classe 1 centrada em (5,5)</li> </ul> <p>Ambas com vari\u00e2ncia 0.5 em cada eixo e sem covari\u00e2ncia.</p>"},{"location":"exercicio2/main/#implement-the-perceptron","title":"Implement the Perceptron","text":"<pre><code>w = np.zeros(2)\nb = 0.0\neta = 0.01\nmax_epochs = 100\n\ndef predict_scores(X, w, b):\n    return X @ w + b\n\ndef predict_labels(X, w, b):\n    return np.where(predict_scores(X, w, b) &gt;= 0, 1, -1)\n\nacc_history = []\nfor epoch in range(max_epochs):\n    updates = 0\n    for xi, yi in zip(X, y):\n        if yi * (w @ xi + b) &lt;= 0:\n            w += eta * yi * xi\n            b += eta * yi\n            updates += 1\n    y_hat = predict_labels(X, w, b)\n    acc_history.append((y_hat == y).mean())\n    if updates == 0:\n        break\n\nprint(\"Pesos:\", w, \"Vi\u00e9s:\", b, \"\u00c9pocas:\", len(acc_history), \"Acc:\", acc_history[-1])\n</code></pre>"},{"location":"exercicio2/main/#plot-the-results","title":"Plot the Results","text":"<pre><code>import matplotlib.pyplot as plt\n\nxs = np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 200)\nys = -(w[0]/w[1])*xs - b/w[1]\n\nplt.figure(figsize=(6,5))\nplt.scatter(X[y==-1][:,0], X[y==-1][:,1], s=12, label=\"Classe 0 (-1)\")\nplt.scatter(X[y== 1][:,0], X[y== 1][:,1], s=12, label=\"Classe 1 (+1)\")\nplt.plot(xs, ys, c=\"k\", linewidth=2, label=\"Fronteira de decis\u00e3o\")\nplt.title(\"Dados e Fronteira de Decis\u00e3o (Perceptron)\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend()\nplt.show()\n\nplt.figure(figsize=(6,4))\nplt.plot(np.arange(1, len(acc_history)+1), acc_history, marker=\"o\")\nplt.title(\"Acur\u00e1cia de Treino por \u00c9poca\")\nplt.xlabel(\"\u00c9poca\"); plt.ylabel(\"Acur\u00e1cia\")\nplt.ylim(0,1.05); plt.grid(True, linestyle=\":\")\nplt.show()\n</code></pre>"},{"location":"exercicio2/main/#analyse-and-results","title":"Analyse and Results","text":"<ul> <li>O perceptron partiu de pesos nulos e atualizou conforme os erros.</li> <li>Como as classes t\u00eam boa separa\u00e7\u00e3o linear (centros distantes e baixa vari\u00e2ncia), o algoritmo convergiu muito r\u00e1pido (em apenas 2 \u00e9pocas).</li> <li>Pesos finais: \u2248 [0.017, 0.0167]</li> <li>Vi\u00e9s: \u2248 -0.11</li> <li>Acur\u00e1cia final: 100% (sem erros de classifica\u00e7\u00e3o).</li> </ul> <p>A fronteira de decis\u00e3o \u00e9 aproximadamente uma reta com inclina\u00e7\u00e3o -1, que separa perfeitamente os dois clusters.</p>"},{"location":"exercicio2/main/#exercise-2","title":"Exercise 2","text":""},{"location":"exercicio2/main/#data-generation","title":"Data Generation","text":"<pre><code>import numpy as np\n\nrng = np.random.default_rng(123)\n\nmean0 = np.array([2.5, 2.5])\nmean1 = np.array([4.0, 4.0])\n\ncov0  = np.array([[1.5, 0.0],[0.0, 1.5]])\ncov1  = np.array([[1.5, 0.0],[0.0, 1.5]])\n\nn = 1000\nX0 = rng.multivariate_normal(mean0, cov0, size=n)\nX1 = rng.multivariate_normal(mean1, cov1, size=n)\n\nX = np.vstack([X0, X1])\ny = np.hstack([-np.ones(n), np.ones(n)])\n</code></pre> <p>Racional: As m\u00e9dias (2.5,2.5) e (4,4) s\u00e3o mais pr\u00f3ximas e a vari\u00e2ncia maior (1.5) em cada dimens\u00e3o cria forte sobreposi\u00e7\u00e3o, tornando o dataset n\u00e3o linearmente separ\u00e1vel.</p>"},{"location":"exercicio2/main/#perceptron-implementation","title":"Perceptron Implementation","text":"<pre><code>def train_perceptron(X, y, eta=0.01, max_epochs=100, rng=None):\n    if rng is None: rng = np.random.default_rng()\n    w = np.zeros(2); b = 0.0\n    def predict(X_): return np.where(X_ @ w + b &gt;= 0, 1, -1)\n\n    acc_hist, upd_hist = [], []\n    for _ in range(max_epochs):\n        updates = 0\n        idx = rng.permutation(len(X))\n        X, y = X[idx], y[idx]\n        for xi, yi in zip(X, y):\n            if yi * (w @ xi + b) &lt;= 0:\n                w += eta * yi * xi\n                b += eta * yi\n                updates += 1\n        acc_hist.append((predict(X) == y).mean())\n        upd_hist.append(updates)\n        if updates == 0: break\n    return w, b, np.array(acc_hist), np.array(upd_hist)\n</code></pre> <p>Rodamos 5 execu\u00e7\u00f5es (embaralhamentos diferentes) e reportamos o melhor resultado e a m\u00e9dia.</p>"},{"location":"exercicio2/main/#results","title":"Results","text":"<ul> <li>Melhor acur\u00e1cia final: 0.7970 (run com seed=1001)</li> <li>Acur\u00e1cia m\u00e9dia (5 runs): 0.7689</li> <li>Pesos finais (w): [0.0802, 0.0882]</li> <li>Vi\u00e9s (b): -0.50</li> <li>\u00c9pocas at\u00e9 parada: 100 (n\u00e3o houve converg\u00eancia total, o algoritmo oscilou at\u00e9 o limite).</li> </ul>"},{"location":"exercicio2/main/#decision-boundary-accuracy","title":"Decision Boundary &amp; Accuracy","text":"<pre><code>xs = np.linspace(X[:,0].min()-0.8, X[:,0].max()+0.8, 300)\nys = -(w[0]/(w[1]+1e-12))*xs - b/(w[1]+1e-12)\n\nplt.figure(figsize=(6,5))\nplt.scatter(X[y==-1][:,0], X[y==-1][:,1], s=12, label=\"Class 0 (-1)\")\nplt.scatter(X[y== 1][:,0], X[y== 1][:,1], s=12, label=\"Class 1 (+1)\")\nplt.plot(xs, ys, linewidth=2, label=\"Decision boundary\")\nmis = np.where(np.where(X @ w + b &gt;= 0, 1, -1) != y)[0]\nplt.scatter(X[mis,0], X[mis,1], marker='x', s=35, label=\"Misclassified\")\nplt.title(\"Decision Boundary with Misclassified Points\")\nplt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.legend(); plt.tight_layout()\nplt.show()\n</code></pre> <pre><code>plt.figure(figsize=(6,4))\nplt.plot(np.arange(1, len(acc_history)+1), acc_history, marker=\"o\")\nplt.title(\"Training Accuracy per Epoch (best run)\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\nplt.ylim(0,1.05); plt.grid(True, linestyle=\":\")\nplt.show()\n</code></pre>"},{"location":"exercicio2/main/#discussion","title":"Discussion","text":"<p>Compara\u00e7\u00e3o com Exercise 1: no exerc\u00edcio anterior, as classes eram bem separadas (m\u00e9dias distantes, vari\u00e2ncia baixa) e o perceptron convergiu em apenas 2 \u00e9pocas, atingindo 100% de acur\u00e1cia.</p> <p>Neste exerc\u00edcio: - Houve parcial sobreposi\u00e7\u00e3o \u2192 o perceptron n\u00e3o convergiu em 100 \u00e9pocas. - Acur\u00e1cia m\u00e1xima ficou em torno de 80%, mesmo na melhor execu\u00e7\u00e3o. - Acur\u00e1cia apresentou oscila\u00e7\u00f5es significativas entre \u00e9pocas, refletindo a instabilidade dos dados n\u00e3o separ\u00e1veis. - Sempre existir\u00e3o pontos mal classificados, vis\u00edveis no gr\u00e1fico.</p> <p>Conclus\u00e3o: A presen\u00e7a de sobreposi\u00e7\u00e3o limita a capacidade do perceptron linear em atingir separa\u00e7\u00e3o perfeita, refor\u00e7ando a necessidade de modelos mais expressivos (ex: m\u00faltiplas camadas, fun\u00e7\u00f5es de ativa\u00e7\u00e3o n\u00e3o lineares).</p>"},{"location":"exercicio3/main/","title":"Exerc\u00edcio 3 - MLP","text":""},{"location":"exercicio3/main/#exercicio-1","title":"Exerc\u00edcio 1","text":""},{"location":"exercicio3/main/#setup","title":"Setup","text":"<pre><code>import numpy as np\n\nx = np.array([0.5, -0.2])\ny = 1.0\n\nW1 = np.array([[0.3, -0.1],\n               [0.2,  0.4]])\nb1 = np.array([0.1, -0.2])\n\nW2 = np.array([0.5, -0.3])\nb2 = 0.2\n\neta = 0.1\n</code></pre>"},{"location":"exercicio3/main/#forward-pass","title":"Forward Pass","text":"<pre><code>z1 = W1 @ x + b1\nh1 = tanh(z1)\nu2 = float(W2 @ h1 + b2)\ny_hat = float(tanh(u2))\nL = (y - y_hat)**2\n</code></pre>"},{"location":"exercicio3/main/#backward-pass","title":"Backward Pass","text":"<pre><code>dL_dyhat = 2 * (y_hat - y)\ndL_du2   = dL_dyhat * (1 - np.tanh(u2)**2)\n\ndL_dW2 = dL_du2 * h1\ndL_db2 = dL_du2\n\ndL_dh1 = dL_du2 * W2\ndL_dz1 = dL_dh1 * (1 - np.tanh(z1)**2)\n\ndL_dW1 = np.outer(dL_dz1, x)\ndL_db1 = dL_dz1\n</code></pre> <p>Gradientes: === Backward Pass (Gradients) ===</p> <p>dL/dy_hat     = -1.26550687</p> <p>dL/du2        = -1.09482791</p> <p>dL/dW2        = [-0.28862383  0.19496791]</p> <p>dL/db2        = -1.09482791</p> <p>dL/dh1        = [-0.54741396  0.32844837]</p> <p>dL/dz1        = [-0.50936975  0.31803236]</p> <p>dL/dW1        = [[-0.25468488  0.10187395]  [ 0.15901618 -0.06360647]]</p> <p>dL/db1        = [-0.50936975  0.31803236]</p>"},{"location":"exercicio3/main/#atualizacao-dos-parametros","title":"Atualiza\u00e7\u00e3o dos Par\u00e2metros","text":"<p>W2_new = W2 - eta * dL_dW2 b2_new = b2 - eta * dL_db2 W1_new = W1 - eta * dL_dW1 b1_new = b1 - eta * dL_db1</p> <p>=== Parameter Update (eta = 0.1) ===</p> <p>W2_new = [ 0.52886238 -0.31949679]</p> <p>b2_new = 0.30948279</p> <p>W1_new = [[ 0.32546849 -0.1101874 ] [ 0.18409838  0.40636065]]</p> <p>b1_new = [ 0.15093698 -0.23180324]</p>"},{"location":"exercicio3/main/#analise-e-resultados","title":"An\u00e1lise e Resultados","text":"<ul> <li>O forward pass mostrou que a rede previu \u0177 \u2248 0.367, abaixo do target real (1.0).</li> <li>O erro quadr\u00e1tico foi \u2248 0.40, indicando diferen\u00e7a relevante.</li> <li>O backward pass forneceu os gradientes para cada peso e vi\u00e9s.</li> <li>A atualiza\u00e7\u00e3o dos par\u00e2metros deslocou os pesos e vieses no sentido de reduzir o erro.</li> </ul>"},{"location":"exercicio3/main/#exercicio-2","title":"Exerc\u00edcio 2","text":""},{"location":"exercicio3/main/#setup_1","title":"Setup","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nmean0 = np.array([2.5, 2.5])\nmean1 = np.array([4.0, 4.0])\n\ncov0 = np.array([[1.5, 0.0],\n                 [0.0, 1.5]])\ncov1 = np.array([[1.5, 0.0],\n                 [0.0, 1.5]])\n\nn_per_class = 1000\nrng = np.random.default_rng(123)\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\n\nX = np.vstack([X0, X1])\ny = np.hstack([-np.ones(n_per_class), np.ones(n_per_class)])\n</code></pre>"},{"location":"exercicio3/main/#implementacao","title":"Implementa\u00e7\u00e3o","text":"<pre><code>def perceptron_train(X, y, eta=0.01, max_epochs=100, rng=None):\n    if rng is None:\n        rng = np.random.default_rng()\n    w = np.zeros(X.shape[1])\n    b = 0.0\n\n    for epoch in range(max_epochs):\n        updates = 0\n        idx = rng.permutation(len(X))\n        X, y = X[idx], y[idx]\n        for xi, yi in zip(X, y):\n            if yi * (w @ xi + b) &lt;= 0:\n                w += eta * yi * xi\n                b += eta * yi\n                updates += 1\n        if updates == 0:\n            break\n    return w, b\n</code></pre>"},{"location":"exercicio3/main/#resultados-melhor-execucao-entre-5-runs","title":"Resultados (melhor execu\u00e7\u00e3o entre 5 runs)","text":"<ul> <li>Melhor acur\u00e1cia final: 0.7970 </li> <li>Acur\u00e1cia m\u00e9dia: 0.7689 </li> <li>Pesos finais (w): [0.08021828, 0.08821639]  </li> <li>Vi\u00e9s (b): -0.50  </li> <li>\u00c9pocas: 100 (sem converg\u00eancia total) </li> </ul>"},{"location":"exercicio3/main/#analise-e-resultados_1","title":"An\u00e1lise e Resultados","text":"<ul> <li>O dataset apresenta forte sobreposi\u00e7\u00e3o \u2192 o perceptron n\u00e3o converge totalmente.  </li> <li>A acur\u00e1cia estabilizou em ~80%.  </li> <li>O modelo linear n\u00e3o consegue separar os pontos sobrepostos.  </li> <li>Conclus\u00e3o: \u00e9 necess\u00e1rio modelos n\u00e3o lineares (ex. MLP) para atingir separa\u00e7\u00e3o melhor.  </li> </ul>"},{"location":"exercicio3/main/#exercicio-3","title":"Exerc\u00edcio 3","text":""},{"location":"exercicio3/main/#setup_2","title":"Setup","text":"<pre><code>import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(\n    n_samples=1500, n_features=4, n_informative=4,\n    n_redundant=0, n_classes=3, n_clusters_per_class=2,\n    class_sep=1.2, random_state=42\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123, stratify=y\n)\n</code></pre>"},{"location":"exercicio3/main/#modelo","title":"Modelo","text":"<pre><code>MLP([4, 16, 3])\nAtiva\u00e7\u00e3o: ReLU nas ocultas\nSa\u00edda: Softmax\nLoss: Cross-Entropy\nOtimizador: SGD (lr=0.05, batch=64, 200 \u00e9pocas)\n</code></pre>"},{"location":"exercicio3/main/#resultados","title":"Resultados","text":"<pre><code>Acur\u00e1cia Treino \u2248 0.93\nAcur\u00e1cia Teste  \u2248 0.89\n</code></pre>"},{"location":"exercicio3/main/#visualizacoes","title":"Visualiza\u00e7\u00f5es","text":"<ul> <li>Curva de Loss por \u00c9poca  </li> <li>Matriz de Confus\u00e3o (3 classes)  </li> </ul>"},{"location":"exercicio3/main/#analise-e-resultados_2","title":"An\u00e1lise e Resultados","text":"<ul> <li>O modelo aprendeu bem as fronteiras de decis\u00e3o.  </li> <li>A padroniza\u00e7\u00e3o das features estabilizou o treino.  </li> <li>A rede rasa (1 camada oculta) j\u00e1 generaliza bem, mas h\u00e1 margem para melhora.  </li> <li>Pequena diferen\u00e7a entre treino e teste indica bom equil\u00edbrio entre vi\u00e9s e vari\u00e2ncia.  </li> </ul>"},{"location":"exercicio3/main/#exercicio-4","title":"Exerc\u00edcio 4","text":""},{"location":"exercicio3/main/#setup_3","title":"Setup","text":"<pre><code>Mesmo dataset do Exerc\u00edcio 3\n\nArquitetura: [4, 32, 16, 3]\nAtiva\u00e7\u00f5es: ReLU (ocultas), Softmax (sa\u00edda)\nLoss: Cross-Entropy\nOtimizador: SGD (lr=0.03, wd=1e-4)\n\u00c9pocas: 250\nBatch: 64\n</code></pre>"},{"location":"exercicio3/main/#resultados_1","title":"Resultados","text":"<pre><code>Acur\u00e1cia Treino \u2248 0.95\nAcur\u00e1cia Teste  \u2248 0.90\n</code></pre>"},{"location":"exercicio3/main/#visualizacoes_1","title":"Visualiza\u00e7\u00f5es","text":"<ul> <li>Loss de treino por \u00e9poca (decrescendo de forma suave)</li> <li>Matriz de confus\u00e3o mais \u201climpa\u201d que no Ex.3</li> </ul>"},{"location":"exercicio3/main/#analise-e-resultados_3","title":"An\u00e1lise e Resultados","text":"<ul> <li>A rede mais profunda obteve leve melhora de desempenho.  </li> <li>A segunda camada oculta aumenta a capacidade de representa\u00e7\u00e3o.  </li> <li>A regulariza\u00e7\u00e3o (<code>weight decay</code>) reduziu overfitting.  </li> <li>A curva de loss mostrou converg\u00eancia est\u00e1vel.  </li> <li>Conclus\u00e3o: a maior profundidade permitiu aprender fronteiras mais complexas, com boa generaliza\u00e7\u00e3o.  </li> </ul>"},{"location":"exercicio3/main/#conclusao-geral","title":"Conclus\u00e3o Geral","text":"<ul> <li>Ex1: Implementa\u00e7\u00e3o manual de MLP \u2014 entendimento da mec\u00e2nica do gradiente.  </li> <li>Ex2: Perceptron em dados sobrepostos \u2014 limita\u00e7\u00f5es de linearidade.  </li> <li>Ex3: MLP simples \u2014 primeira generaliza\u00e7\u00e3o n\u00e3o linear bem-sucedida.  </li> <li>Ex4: MLP mais profundo \u2014 melhora de capacidade e estabilidade.  </li> </ul>"},{"location":"exercicio4/main/","title":"Exerc\u00edcio \u2014 VAE","text":""},{"location":"exercicio4/main/#setup","title":"Setup","text":"<pre><code># Ambiente (venv \"env\")\npython -m pip uninstall -y torch torchvision torchaudio numpy\npython -m pip install \"numpy&lt;2\" --upgrade\npython -m pip install \"torch==2.2.2\" \"torchvision==0.17.2\"\npython -m pip install matplotlib scikit-learn pillow\n\n# Execu\u00e7\u00e3o\npython vae.py --dataset mnist --epochs 10 --latent-dim 2 --beta 1.0\n</code></pre> <p>Configura\u00e7\u00e3o usada <pre><code>dataset: mnist\nepochs: 10\nbatch_size: 128\nlatent_dim: 2\nbeta: 1.0\nlr: 0.001\nrecon_loss: bce\nval_split: 0.1\nseed: 42\ndevice: cpu\n</code></pre></p> <p>Observa\u00e7\u00e3o de download: os links originais de MNIST (yann.lecun.com) deram 404; o <code>torchvision</code> baixou automaticamente de <code>ossci-datasets.s3.amazonaws.com</code>.</p>"},{"location":"exercicio4/main/#data-preparation","title":"Data Preparation","text":"<ul> <li>Load MNIST (train/test) via <code>torchvision.datasets.MNIST</code>.</li> <li>Normalize em ([0,1]) com <code>transforms.ToTensor()</code>.</li> <li>Split treino/val com <code>random_split</code> (90%/10%): Treino: 54.000 | Valida\u00e7\u00e3o: 6.000 | Teste: 10.000</li> </ul>"},{"location":"exercicio4/main/#model-implementation","title":"Model Implementation","text":""},{"location":"exercicio4/main/#arquitetura-convvae-12828","title":"Arquitetura (ConvVAE, 1\u00d728\u00d728)","text":"<p>Encoder - Conv(1\u219232, k4,s2,p1) \u2192 ReLU \u27f6 14\u00d714 - Conv(32\u219264, k4,s2,p1) \u2192 ReLU \u27f6 7\u00d77 - Conv(64\u2192128, k3,s1,p1) \u2192 ReLU \u27f6 7\u00d77 - Flatten(128\u00b77\u00b77=6272) \u2192 FC_\u03bc (6272\u21922), FC_{logvar} (6272\u21922)</p> <p>Decoder - FC(2\u21926272) \u2192 reshape (128\u00d77\u00d77) - ConvT(128\u219264, k4,s2,p1) \u2192 ReLU \u27f6 14\u00d714 - ConvT(64\u219232, k4,s2,p1) \u2192 ReLU \u27f6 28\u00d728 - Conv(32\u21921, k3,s1,p1) \u27f6 logits</p> <p>Reparameterization trick [ z = \\mu + \\sigma \\odot \\varepsilon,\\;\\varepsilon\\sim\\mathcal{N}(0,I),\\;\\sigma=\\exp!\\big(0.5\\cdot \\log\\sigma^2\\big) ]</p> <p>Par\u00e2metros trein\u00e1veis: 315.365</p>"},{"location":"exercicio4/main/#loss-elbo","title":"Loss (ELBO)","text":"<ul> <li>Reconstru\u00e7\u00e3o (BCE with logits, sum) + \u03b2 \u00b7 KL (com \u03b2=1.0).  </li> <li>KL (gaussianas diagonais):   [   -\\tfrac12 \\sum_i \\left(1 + \\log\\sigma_i^2 - \\mu_i^2 - \\sigma_i^2\\right)   ]</li> <li>M\u00e9tricas reportadas por item no batch; tamb\u00e9m mostramos ELBO por pixel.</li> </ul>"},{"location":"exercicio4/main/#training","title":"Training","text":"<p>Amostras iniciais (prior): <code>samples_epoch0.png</code> Treino por 10 \u00e9pocas com Adam (lr=1e-3), batch=128.</p> Epoch train_ELBO val_ELBO train_Recon val_Recon train_KL val_KL ELBO/pixel (val) 1 189.9199 166.6079 185.5278 161.7278 4.3921 4.8801 0.212510 2 161.6840 157.6834 156.4333 152.2417 5.2507 5.4416 0.201127 3 156.0742 154.2295 150.4499 148.3952 5.6243 5.8342 0.196721 4 153.3052 152.5510 147.4894 146.6769 5.8157 5.8741 0.194580 5 151.5527 151.5376 145.6390 145.1763 5.9136 6.3614 0.193288 6 150.1056 149.6207 144.0820 143.3907 6.0236 6.2300 0.190843 7 149.1560 149.2737 143.0495 142.9689 6.1065 6.3048 0.190400 8 148.1687 149.1837 142.0027 142.9922 6.1660 6.1915 0.190285 9 147.5587 148.2316 141.3345 142.0705 6.2242 6.1611 0.189071 10 146.9479 148.2099 140.6753 141.8223 6.2727 6.3876 0.189043 <p>Melhor \u00e9poca (val ELBO): 10 (val_ELBO \u2248 148.210). Checkpoints: <code>best.pt</code> salvo quando melhora.</p> <p>Reconstru\u00e7\u00f5es e latente durante treino: - <code>reconstructions_epoch1.png</code>, <code>latent_epoch1_2d.png</code> - <code>reconstructions_epoch5.png</code>, <code>latent_epoch5_2d.png</code> - <code>reconstructions_epoch10.png</code>, <code>latent_epoch10_2d.png</code></p>"},{"location":"exercicio4/main/#evaluation","title":"Evaluation","text":"<p>Valida\u00e7\u00e3o (final): ELBO = 148.274 \u2022 Recon = 141.886 \u2022 KL = 6.388</p> <p>Teste (final): ELBO = 147.870 \u2022 Recon = 141.483 \u2022 KL = 6.387</p> <p>Amostras do prior (final): <code>samples_final.png</code> Reconstru\u00e7\u00f5es (final): <code>reconstructions_final.png</code> Espa\u00e7o latente (\u03bc, 2D): <code>latent_final_2d.png</code></p>"},{"location":"exercicio4/main/#visualization","title":"Visualization","text":"<ul> <li>Originais vs Reconstru\u00e7\u00f5es: grids salvos por \u00e9poca e no final.</li> <li>Espa\u00e7o latente (z=2): scatter colorido por r\u00f3tulo \u2014 separabilidade crescente ao longo do treino.</li> <li>Amostras do prior: grades de d\u00edgitos gerados a partir de (z\\sim\\mathcal{N}(0,I)).</li> </ul>"},{"location":"exercicio4/main/#analise-e-resultados","title":"An\u00e1lise e Resultados","text":"<ul> <li>Converg\u00eancia est\u00e1vel: queda consistente do val ELBO at\u00e9 ~epoch 10; o termo Recon domina a ELBO (~142) com KL (~6\u20136.4), t\u00edpico de VAE com \u03b2=1.  </li> <li>Latente 2D significativo: visualiza\u00e7\u00e3o mostra clusters por classe; qualidade melhora entre as \u00e9pocas 1\u21925\u219210.  </li> <li>Reconstru\u00e7\u00f5es razo\u00e1veis: nitidez compat\u00edvel com MNIST, com leve borramento inerente ao VAE (BCE+logits).  </li> <li>Trade-off Recon\u2194KL: KL cresce sutilmente (6.0\u21926.4), organizando o espa\u00e7o latente sem degradar demais a Recon.</li> </ul> <p>Desafios enfrentados - Compatibilidade NumPy/PyTorch: necessidade de usar <code>numpy&lt;2</code> com as rodas de PyTorch/Torchvision dispon\u00edveis. - Download de MNIST: links originais retornaram 404; fallback autom\u00e1tico funcionou. - CPU-only: tempos por \u00e9poca maiores (\u2248 44\u201379s); ainda assim, converg\u00eancia adequada em 10 \u00e9pocas.</p> <p>Insights - \u03b2=1.0 j\u00e1 produz um latente 2D \u00fatil; aumentar \u03b2 tende a estruturar ainda mais o espa\u00e7o (com poss\u00edvel perda de nitidez). - Latent dim=2 facilita inspe\u00e7\u00e3o visual e interpola\u00e7\u00e3o; para z&gt;3, usar PCA/t-SNE ajuda a manter a interpretabilidade.</p>"},{"location":"exercicio4/main/#proximos-passos","title":"Pr\u00f3ximos Passos","text":"<ul> <li>\u03b2-sweep (\\beta\\in{0.5,1,2,4}) e z-sweep (z\\in{2,8,16,32}) para estudar Recon\u2194KL.  </li> <li>Interpola\u00e7\u00f5es no espa\u00e7o latente entre pares de d\u00edgitos.  </li> <li>M\u00e9trica FID (opcional) para qualidade amostral.  </li> <li>Augmentations leves (ex.: shifts) e scheduler de LR para refino.</li> </ul>"},{"location":"exercicio4/main/#artefatos-gerados","title":"Artefatos Gerados","text":"<ul> <li><code>history.csv</code> \u2014 m\u00e9tricas por \u00e9poca  </li> <li><code>best.pt</code> \u2014 melhor checkpoint  </li> <li><code>samples_epoch0.png</code>, <code>samples_epoch5.png</code>, <code>samples_epoch10.png</code>, <code>samples_final.png</code> </li> <li><code>reconstructions_epoch1.png</code>, <code>reconstructions_epoch5.png</code>, <code>reconstructions_epoch10.png</code>, <code>reconstructions_final.png</code> </li> <li><code>latent_epoch1_2d.png</code>, <code>latent_epoch5_2d.png</code>, <code>latent_epoch10_2d.png</code>, <code>latent_final_2d.png</code> </li> <li><code>report.md</code> \u2014 mini\u2010relat\u00f3rio autom\u00e1tico do script</li> </ul>"},{"location":"exercicio4/vae_runs/mnist_z2_beta1.0_20251026-134922/report/","title":"VAE Report","text":"<p>Dataset: mnist</p> <p>Latent dim: 2</p> <p>\u03b2 (beta-VAE): 1.0</p> <p>Recon loss: bce</p> <p>Epochs: 10</p> <p>Batch size: 128</p> <p>LR: 0.001</p> <p>Melhor \u00e9poca (val ELBO): 10</p> <p>Melhor val_ELBO: 148.209887</p>"},{"location":"exercicio4/vae_runs/mnist_z2_beta1.0_20251026-134922/report/#artefatos-gerados","title":"Artefatos gerados","text":"<ul> <li>history_csv: <code>./vae_runs/mnist_z2_beta1.0_20251026-134922/history.csv</code></li> <li>samples_initial: <code>./vae_runs/mnist_z2_beta1.0_20251026-134922/samples_epoch0.png</code></li> <li>samples_final: <code>./vae_runs/mnist_z2_beta1.0_20251026-134922/samples_final.png</code></li> <li>reconstructions_final: <code>./vae_runs/mnist_z2_beta1.0_20251026-134922/reconstructions_final.png</code></li> <li>latent_plot_final: <code>./vae_runs/mnist_z2_beta1.0_20251026-134922/latent_final_2d.png</code></li> </ul>"},{"location":"exercicio4/vae_runs/mnist_z2_beta1.0_20251026-134922/report/#observacoes-insights","title":"Observa\u00e7\u00f5es &amp; Insights","text":"<ul> <li>Reconstru\u00e7\u00f5es permitem comparar qualidade visual vs. regulariza\u00e7\u00e3o (KL).</li> <li><code>\u03b2</code> maior tende a organizar melhor o espa\u00e7o latente (interpola\u00e7\u00e3o suave), \u00e0s vezes piorando a nitidez das reconstru\u00e7\u00f5es.</li> <li>Se <code>latent_dim&lt;=3</code>, a visualiza\u00e7\u00e3o direta de <code>\u03bc</code> revela separabilidade por classes; com <code>latent_dim</code> alto, PCA/t-SNE s\u00e3o \u00fateis.</li> </ul>"},{"location":"exercicio4/vae_runs/mnist_z2_beta1.0_20251026-134922/report/#proximos-passos","title":"Pr\u00f3ximos passos","text":"<ul> <li>Grid de hiperpar\u00e2metros (\u03b2, <code>latent_dim</code>).</li> <li>Interpola\u00e7\u00e3o no espa\u00e7o latente.</li> <li>M\u00e9trica FID (opcional) para comparar qualidade amostral.</li> </ul>"},{"location":"projeto1/main/","title":"Projeto 1 - Classification - Tomas Miele e Yuri Tabacof","text":""},{"location":"projeto1/main/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>Nome do dataset: Default of Credit Card Clients (Taiwan) Fonte: UCI Machine Learning Repository URL: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients Tamanho: 30.000 registros, 23 vari\u00e1veis preditoras + 1 vari\u00e1vel-alvo  </p> <p>Tarefa: Classifica\u00e7\u00e3o bin\u00e1ria \u2014 prever se um cliente entrar\u00e1 em default (inadimpl\u00eancia) no m\u00eas seguinte.</p> <p>Justificativa da escolha: - O problema \u00e9 realista e relevante no contexto financeiro (risco de cr\u00e9dito). - Cont\u00e9m dados mistos (num\u00e9ricos e categ\u00f3ricos), o que torna o pr\u00e9-processamento e o aprendizado mais desafiadores e instrutivos. - Possui volume adequado (&gt;1.000 amostras e &gt;5 atributos), atendendo aos requisitos do projeto. - Apresenta classes desbalanceadas, o que permite discutir m\u00e9tricas alternativas \u00e0 acur\u00e1cia e estrat\u00e9gias de balanceamento. - \u00c9 uma base p\u00fablica e amplamente utilizada em pesquisa aplicada, sem ser uma das cl\u00e1ssicas proibidas (Titanic, Iris, Wine etc.).</p>"},{"location":"projeto1/main/#2-dataset-explanation","title":"2. Dataset Explanation","text":""},{"location":"projeto1/main/#contexto-e-descricao","title":"Contexto e Descri\u00e7\u00e3o","text":"<p>O dataset cont\u00e9m informa\u00e7\u00f5es de 30.000 clientes de cart\u00e3o de cr\u00e9dito em Taiwan. Cada registro representa um cliente, e a vari\u00e1vel-alvo indica se ele deu default no m\u00eas seguinte (<code>default.payment.next.month</code> = 1) ou n\u00e3o (= 0). Os atributos incluem dados demogr\u00e1ficos, financeiros e hist\u00f3ricos de pagamento.</p>"},{"location":"projeto1/main/#variaveis","title":"Vari\u00e1veis","text":"<p>Demogr\u00e1ficas - <code>SEX</code>: G\u00eanero (1 = masculino, 2 = feminino) - <code>EDUCATION</code>: Grau de instru\u00e7\u00e3o (1 = p\u00f3s-gradua\u00e7\u00e3o, 2 = gradua\u00e7\u00e3o, 3 = ensino m\u00e9dio, 4/0/5/6 = outros) - <code>MARRIAGE</code>: Estado civil (1 = casado, 2 = solteiro, 3/0 = outros) - <code>AGE</code>: Idade (anos)</p> <p>Financeiras - <code>LIMIT_BAL</code>: Limite total de cr\u00e9dito (em NT$)</p> <p>Hist\u00f3rico de pagamento (\u00faltimos 6 meses) - <code>PAY_0</code>, <code>PAY_2</code>, <code>PAY_3</code>, <code>PAY_4</code>, <code>PAY_5</code>, <code>PAY_6</code> \u2014 Status de pagamento (valores inteiros, onde \u22122/\u22121/0 indicam pagos em dia, e 1, 2, ... indicam atraso em meses)</p> <p>Faturas mensais - <code>BILL_AMT1</code> a <code>BILL_AMT6</code> \u2014 Valores das faturas nos seis meses anteriores</p> <p>Pagamentos mensais - <code>PAY_AMT1</code> a <code>PAY_AMT6</code> \u2014 Valores pagos nos seis meses anteriores</p> <p>Alvo - <code>default.payment.next.month</code>: 0 = n\u00e3o entrou em default; 1 = entrou em default</p>"},{"location":"projeto1/main/#tipos-de-dados","title":"Tipos de dados","text":"<ul> <li>Num\u00e9ricos cont\u00ednuos: <code>LIMIT_BAL</code>, <code>AGE</code>, <code>BILL_AMT*</code>, <code>PAY_AMT*</code> </li> <li>Num\u00e9ricos discretos: <code>PAY_*</code> </li> <li>Categ\u00f3ricos: <code>SEX</code>, <code>EDUCATION</code>, <code>MARRIAGE</code> </li> <li>Bin\u00e1rio (target): <code>default.payment.next.month</code></li> </ul>"},{"location":"projeto1/main/#principais-desafios","title":"Principais Desafios","text":"<ul> <li>Desbalanceamento: apenas ~22% dos clientes est\u00e3o em default.  </li> <li>Categorias inv\u00e1lidas: <code>EDUCATION</code> e <code>MARRIAGE</code> cont\u00eam c\u00f3digos inconsistentes.  </li> <li>Outliers: valores muito altos em <code>BILL_AMT*</code> e <code>PAY_AMT*</code>.  </li> <li>Multicolinearidade: alta correla\u00e7\u00e3o entre s\u00e9ries temporais (meses consecutivos).  </li> <li>Escalas muito diferentes: necessidade de normaliza\u00e7\u00e3o antes do treino do MLP.</li> </ul>"},{"location":"projeto1/main/#estatisticas-e-visualizacoes-planejadas","title":"Estat\u00edsticas e Visualiza\u00e7\u00f5es (planejadas)","text":"<ul> <li>Distribui\u00e7\u00e3o do alvo (<code>default.payment.next.month</code>)  </li> <li>Histogramas de <code>LIMIT_BAL</code> e <code>AGE</code> </li> <li>Heatmap de correla\u00e7\u00e3o entre vari\u00e1veis num\u00e9ricas  </li> <li>Tabela resumo de m\u00e9dias, desvios e amplitudes  </li> </ul>"},{"location":"projeto1/main/#consideracoes-eticas","title":"Considera\u00e7\u00f5es \u00c9ticas","text":"<p>Alguns atributos (g\u00eanero, estado civil, escolaridade) podem introduzir vi\u00e9s algor\u00edtmico. Discuss\u00f5es sobre fairness e mitiga\u00e7\u00e3o de vi\u00e9s s\u00e3o pertinentes ao interpretar os resultados.</p>"},{"location":"projeto1/main/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization","text":""},{"location":"projeto1/main/#estrutura-e-natureza-dos-dados","title":"Estrutura e Natureza dos Dados","text":"<p>Os dados utilizados neste projeto representam informa\u00e7\u00f5es de clientes de cart\u00e3o de cr\u00e9dito, com atributos que descrevem aspectos demogr\u00e1ficos, financeiros e comportamentais. Cada linha do dataset corresponde a um cliente, e cada coluna representa uma feature (atributo), como limite de cr\u00e9dito, idade, estado civil ou hist\u00f3rico de pagamento. Essa estrutura, em forma de matriz de atributos, \u00e9 a base para a aplica\u00e7\u00e3o de t\u00e9cnicas de aprendizado supervisionado, em que cada exemplo possui um conjunto de entradas (features) e uma sa\u00edda (r\u00f3tulo).</p> <p>As vari\u00e1veis do conjunto de dados podem ser classificadas em dois tipos principais:</p> <ul> <li>Num\u00e9ricas: representam valores cont\u00ednuos ou discretos, como <code>LIMIT_BAL</code> (limite de cr\u00e9dito) e <code>AGE</code> (idade);  </li> <li>Categ\u00f3ricas: representam valores qualitativos, como <code>SEX</code>, <code>EDUCATION</code> e <code>MARRIAGE</code>.</li> </ul> <p>Essa distin\u00e7\u00e3o \u00e9 fundamental, pois cada tipo de dado requer um tratamento espec\u00edfico para que o modelo de aprendizado consiga interpretar corretamente as informa\u00e7\u00f5es.</p>"},{"location":"projeto1/main/#limpeza-e-qualidade-dos-dados","title":"Limpeza e Qualidade dos Dados","text":"<p>A qualidade dos dados \u00e9 essencial para o desempenho de qualquer modelo de aprendizado de m\u00e1quina. Durante a etapa de limpeza, foram verificados problemas comuns como valores ausentes, duplicatas e inconsist\u00eancias.</p> <ul> <li>Valores ausentes: n\u00e3o foram encontrados no conjunto de dados.  </li> <li>Duplicatas: nenhuma linha duplicada foi identificada.  </li> <li>Inconsist\u00eancias: categorias incorretas, como <code>EDUCATION = 0, 5, 6</code> e <code>MARRIAGE = 0</code>, foram recategorizadas como \u201cOutros\u201d, garantindo consist\u00eancia nos dados.  </li> <li>Valores inv\u00e1lidos: apenas uma amostra foi removida por conter um valor incorreto na vari\u00e1vel alvo (<code>default_payment_next_month</code>).</li> </ul> <p>Ap\u00f3s a limpeza, o dataset permaneceu com 30.000 amostras v\u00e1lidas, todas completas e sem inconsist\u00eancias estruturais.</p>"},{"location":"projeto1/main/#pre-processamento-e-transformacao-dos-dados","title":"Pr\u00e9-processamento e Transforma\u00e7\u00e3o dos Dados","text":"<p>Como o modelo de aprendizado requer entradas num\u00e9ricas, as vari\u00e1veis categ\u00f3ricas foram convertidas em formato num\u00e9rico por meio da t\u00e9cnica de One-Hot Encoding, criando uma coluna para cada categoria poss\u00edvel de <code>SEX</code>, <code>EDUCATION</code> e <code>MARRIAGE</code>. Esse processo garante que o modelo interprete corretamente diferen\u00e7as qualitativas entre categorias, sem atribuir ordens artificiais a elas.</p> <p>Em seguida, os dados num\u00e9ricos foram normalizados para uma escala comum, de forma que todas as vari\u00e1veis contribuam igualmente durante o treinamento da rede neural. Esse procedimento \u00e9 essencial para evitar que atributos com valores mais altos dominem a fun\u00e7\u00e3o de custo do modelo.</p> <p>Por fim, o conjunto de dados foi dividido em tr\u00eas subconjuntos: - Treino (60%) \u2013 usado para o aprendizado do modelo; - Valida\u00e7\u00e3o (20%) \u2013 usado para ajuste de par\u00e2metros; - Teste (20%) \u2013 usado para avaliar a capacidade de generaliza\u00e7\u00e3o.</p> <p>A divis\u00e3o foi feita de forma estratificada, mantendo a propor\u00e7\u00e3o original das classes (<code>default = 1</code> e <code>non-default = 0</code>) em todos os conjuntos.</p>"},{"location":"projeto1/main/#resumo-do-processo-de-preparacao","title":"Resumo do Processo de Prepara\u00e7\u00e3o","text":"Etapa A\u00e7\u00e3o Realizada Verifica\u00e7\u00e3o de valores ausentes Nenhum valor ausente encontrado Remo\u00e7\u00e3o de duplicatas Nenhuma duplicata detectada Corre\u00e7\u00e3o de categorias inv\u00e1lidas Reclassifica\u00e7\u00e3o de valores fora do intervalo v\u00e1lido Exclus\u00e3o de valores incorretos 1 registro removido Codifica\u00e7\u00e3o de vari\u00e1veis categ\u00f3ricas One-Hot Encoding aplicado Normaliza\u00e7\u00e3o Escalonamento dos atributos num\u00e9ricos Divis\u00e3o do dataset 60% treino, 20% valida\u00e7\u00e3o, 20% teste (estratificado) <p>Esses procedimentos asseguraram que o dataset estivesse limpo, consistente e devidamente estruturado, seguindo as boas pr\u00e1ticas de qualidade, balanceamento e padroniza\u00e7\u00e3o de dados recomendadas em Machine Learning.</p>"},{"location":"projeto1/main/#4-implementacao-do-mlp-numpy","title":"4. Implementa\u00e7\u00e3o do MLP (NumPy)","text":""},{"location":"projeto1/main/#implementacao","title":"Implementa\u00e7\u00e3o","text":"<p>Implementamos um MLP do zero, usando apenas NumPy (produto matricial, ativa\u00e7\u00f5es, softmax, cross\u2011entropy, backprop e atualiza\u00e7\u00e3o dos pesos). O objetivo \u00e9 classificar inadimpl\u00eancia (<code>default_payment_next_month</code>) a partir dos dados j\u00e1 limpos/normalizados.</p>"},{"location":"projeto1/main/#arquitetura-e-treino","title":"Arquitetura e treino","text":"<p><pre><code>Entrada (d_in) \u2192 ReLU(64) \u2192 ReLU(32) \u2192 Softmax(2)\n</code></pre> - Ativa\u00e7\u00f5es: ReLU nas camadas escondidas; Softmax na sa\u00edda. - Loss: Cross-Entropy (com pesos de classe para desbalanceamento) + L2. - Otimiza\u00e7\u00e3o: SGD mini-batch com momentum, learning rate decay e early stopping. - Limiar de decis\u00e3o: escolhido na valida\u00e7\u00e3o para m\u00e1ximo F1.</p>"},{"location":"projeto1/main/#hiperparametros","title":"Hiperpar\u00e2metros","text":"<ul> <li>Camadas escondidas: (64, 32) </li> <li>Batch size: 256 </li> <li>\u00c9pocas m\u00e1x.: 60 (com early stopping, paci\u00eancia=8)  </li> <li>Learning rate inicial: 1e\u20112 (decai 0.9 a cada 5 \u00e9pocas)  </li> <li>L2 (weight decay): 1e\u20114 </li> <li>Seed: 42 </li> <li>Threshold (val, melhor F1): 0.47</li> </ul>"},{"location":"projeto1/main/#resultados","title":"Resultados","text":"Conjunto Acc Precision Recall F1 ROC\u2011AUC Treino 0.7533 0.4603 0.6660 0.5443 0.7979 Valida\u00e7\u00e3o 0.7367 0.4325 0.6104 0.5062 0.7567 Teste 0.7418 0.4425 0.6443 0.5247 0.7750 <ul> <li>Acc (Accuracy): Propor\u00e7\u00e3o total de acertos \u2014 ou seja, quantos exemplos o modelo classificou corretamente (positivos e negativos) entre todos os exemplos. F\u00f3rmula: (VP + VN) / Total</li> <li>Precision: Propor\u00e7\u00e3o de exemplos classificados como positivos que realmente s\u00e3o positivos. Mede a confiabilidade das previs\u00f5es positivas. F\u00f3rmula: VP / (VP + FP)</li> <li>Recall (Sensibilidade): Propor\u00e7\u00e3o de exemplos positivos reais que o modelo conseguiu capturar. Mede a capacidade de detectar inadimplentes. F\u00f3rmula: VP / (VP + FN)</li> <li>F1-score: M\u00e9dia harm\u00f4nica entre Precision e Recall. Equilibra os dois em uma \u00fanica m\u00e9trica, especialmente \u00fatil com classes desbalanceadas. F\u00f3rmula: 2 \u00b7 (Prec \u00b7 Rec) / (Prec + Rec)</li> <li>ROC\u2011AUC: \u00c1rea sob a curva ROC (Receiver Operating Characteristic), que mede a capacidade do modelo de separar classes. Quanto mais pr\u00f3ximo de 1, melhor a separa\u00e7\u00e3o entre inadimplentes e n\u00e3o inadimplentes.</li> </ul> <p>Observa\u00e7\u00e3o. Em dados desbalanceados, otimizar F1/Recall (via threshold) pode reduzir a accuracy em rela\u00e7\u00e3o ao baseline que sempre prev\u00ea a classe majorit\u00e1ria. Aqui priorizamos recuperar mais inadimplentes mantendo AUC e F1 s\u00f3lidos.</p>"},{"location":"projeto1/main/#codigo-completo","title":"C\u00f3digo Completo","text":"<p>``` pyodide install=\"pandas, numpy, matplotlib, scikit-learn, seaborn\"</p>"},{"location":"projeto1/main/#projeto1py","title":"projeto1.py","text":""},{"location":"projeto1/main/#data-cleaning-normalization-mlp-from-scratch-numpy","title":"Data Cleaning &amp; Normalization + MLP from scratch (NumPy)","text":""},{"location":"projeto1/main/#rode-com-run-python-file-no-vs-code-sem-passar-argumentos","title":"\u2714\ufe0f Rode com \"Run Python File\" (\u25b6) no VS Code, sem passar argumentos.","text":""},{"location":"projeto1/main/#passos","title":"\u2714\ufe0f Passos:","text":""},{"location":"projeto1/main/#1-auto-descobre-o-excel-xlsxlsx-limpa-e-normaliza-one-hot-split-z-score","title":"1) Auto-descobre o Excel (.xls/.xlsx), limpa e normaliza (One-Hot, split, z-score)","text":""},{"location":"projeto1/main/#2-treina-um-mlp-numpy-com-relu-softmax-ce-ponderada-mini-batch-sgd-momentum-l2-early-stopping","title":"2) Treina um MLP (NumPy) com ReLU + Softmax, CE ponderada, mini-batch SGD + Momentum, L2, early stopping","text":""},{"location":"projeto1/main/#3-ajusta-threshold-pelo-melhor-f1-na-validacao-e-imprime-metricas","title":"3) Ajusta threshold pelo melhor F1 na valida\u00e7\u00e3o e imprime m\u00e9tricas","text":""},{"location":"projeto1/main/#_1","title":"Projeto 1 - Classification","text":""},{"location":"projeto1/main/#requisitos","title":"Requisitos:","text":""},{"location":"projeto1/main/#pip-install-numpy-pandas-scikit-learn-xlrd","title":"pip install numpy pandas scikit-learn xlrd","text":""},{"location":"projeto1/main/#_2","title":"Projeto 1 - Classification","text":""},{"location":"projeto1/main/#saidas-padrao-processed","title":"Sa\u00eddas (padr\u00e3o: ./processed):","text":""},{"location":"projeto1/main/#-x_trainnpy-y_trainnpy-x_valnpy-y_valnpy-x_testnpy-y_testnpy","title":"- X_train.npy, y_train.npy, X_val.npy, y_val.npy, X_test.npy, y_test.npy","text":""},{"location":"projeto1/main/#-scaler_munpy-scaler_sdnpy-columnsjson","title":"- scaler_mu.npy, scaler_sd.npy, columns.json","text":""},{"location":"projeto1/main/#-class_dist_beforejson-class_dist_splitsjson","title":"- class_dist_before.json, class_dist_splits.json","text":""},{"location":"projeto1/main/#-cleaned_full_samplecsv-amostra-5","title":"- cleaned_full_sample.csv (amostra 5%)","text":""},{"location":"projeto1/main/#-training_curvescsv-perdas-por-epoca","title":"- training_curves.csv (perdas por \u00e9poca)","text":""},{"location":"projeto1/main/#-final_metricsjson-metricas-threshold-escolhido","title":"- final_metrics.json (m\u00e9tricas + threshold escolhido)","text":""},{"location":"projeto1/main/#_3","title":"Projeto 1 - Classification","text":""},{"location":"projeto1/main/#-","title":"------------------------------------------------------------------------------","text":""},{"location":"projeto1/main/#imports","title":"IMPORTS","text":""},{"location":"projeto1/main/#-_1","title":"------------------------------------------------------------------------------","text":"<p>import json from pathlib import Path import numpy as np import pandas as pd from sklearn.model_selection import train_test_split</p>"},{"location":"projeto1/main/#-_2","title":"------------------------------------------------------------------------------","text":""},{"location":"projeto1/main/#config","title":"CONFIG","text":""},{"location":"projeto1/main/#-_3","title":"------------------------------------------------------------------------------","text":""},{"location":"projeto1/main/#a-cleaning","title":"(A) Cleaning","text":"<p>SEED = 42 WINSOR_LO = 1.0 WINSOR_HI = 99.0 SAMPLE_CSV_FRAC = 0.05 OUTDIR_NAME = \"processed\"</p> <p>POSSIBLE_DATAFILES = [     \"default of credit card clients.xls\",     \"default_of_credit_card_clients.xls\",     \"default of credit card clients.xlsx\",     \"default_of_credit_card_clients.xlsx\",     \"UCI_Credit_Card.xls\",     \"UCI_Credit_Card.xlsx\", ]</p> <p>TARGET_CANDIDATES = [     \"default_payment_next_month\",     \"default.payment.next.month\",     \"default payment next month\",     \"y\", ] ID_CANDIDATES = [\"id\", \"unnamed:_0\"] CAT_COLS_RAW = [\"sex\", \"education\", \"marriage\"]</p>"},{"location":"projeto1/main/#b-mlp-hyperparameters","title":"(B) MLP Hyperparameters","text":"<p>MLP_LAYERS = (64, 32)     # camadas escondidas MLP_L2 = 1e-4             # weight decay LR_INIT = 1e-2            # learning rate inicial BATCH_SIZE = 256 EPOCHS = 60 PATIENCE = 8              # early stopping (\u00e9pocas sem melhora em val) LR_DECAY = 0.9            # multiplicador a cada LR_DECAY_EVERY \u00e9pocas LR_DECAY_EVERY = 5 MOMENTUM = 0.9            # SGD momentum</p>"},{"location":"projeto1/main/#-_4","title":"------------------------------------------------------------------------------","text":""},{"location":"projeto1/main/#helper-functions","title":"HELPER FUNCTIONS","text":""},{"location":"projeto1/main/#-_5","title":"------------------------------------------------------------------------------","text":"<p>def print_header(title):     print(\"\\n\" + \"=\" * 80)     print(title)     print(\"=\" * 80)</p> <p>def normalize_columns(cols):     out = []     for c in cols:         c2 = str(c).strip()         c2 = c2.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"-\", \" \").replace(\"/\", \" \")         c2 = \" \".join(c2.split())         c2 = c2.lower().replace(\" \", \"_\")         out.append(c2)     return out</p> <p>def try_read_excel(path: Path) -&gt; pd.DataFrame:     for kw in [         dict(header=0, engine=\"xlrd\"),         dict(header=1, engine=\"xlrd\"),         dict(header=0),         dict(header=1),     ]:         try:             return pd.read_excel(path, sheet_name=0, **kw)         except Exception:             continue     return pd.read_excel(path)</p> <p>def find_first_present(candidates, cols):     for c in candidates:         if c in cols:             return c     return None</p> <p>def auto_find_datafile(script_dir: Path) -&gt; Path:     for name in POSSIBLE_DATAFILES:         p = script_dir / name         if p.exists():             return p     for p in sorted(script_dir.glob(\".xls\")) + sorted(script_dir.glob(\".xlsx\")):         return p     raise FileNotFoundError(         \"\u274c Nenhum arquivo .xls/.xlsx encontrado. Coloque o Excel na mesma pasta do .py.\"     )</p>"},{"location":"projeto1/main/#-_6","title":"------------------------------------------------------------------------------","text":""},{"location":"projeto1/main/#section-3-cleaning-normalization","title":"SECTION 3 \u2014 CLEANING &amp; NORMALIZATION","text":""},{"location":"projeto1/main/#-_7","title":"------------------------------------------------------------------------------","text":"<p>def run_cleaning_and_save(script_dir: Path):     outdir = script_dir / OUTDIR_NAME     outdir.mkdir(parents=True, exist_ok=True)</p> <pre><code># 1) Carregar .xls/.xlsx automaticamente\nprint_header(\"[1] Carregando planilha de dados (auto-descoberta)\")\ndata_path = auto_find_datafile(script_dir)\nprint(f\"Arquivo detectado: {data_path.name}\")\ndf = try_read_excel(data_path)\nprint(f\"Shape bruto lido: {df.shape}\")\n\ndf.columns = normalize_columns(df.columns)\nprint(\"Colunas normalizadas (primeiras 15):\", list(df.columns)[:15])\n\ndf = df.dropna(how=\"all\")\nprint(f\"Shape ap\u00f3s remover linhas 100% vazias: {df.shape}\")\n\n# Detectar schema X1..X23 + Y e renomear\ncols = set(df.columns)\nx_schema_ok = {\n    \"x1\",\"x2\",\"x3\",\"x4\",\"x5\",\"x6\",\"x7\",\"x8\",\"x9\",\"x10\",\"x11\",\n    \"x12\",\"x13\",\"x14\",\"x15\",\"x16\",\"x17\",\"x18\",\"x19\",\"x20\",\"x21\",\"x22\",\"x23\",\"y\"\n}.issubset(cols)\n\nif x_schema_ok:\n    mapping = {\n        \"unnamed:_0\": \"id\",\n        \"x1\":  \"limit_bal\",\n        \"x2\":  \"sex\",\n        \"x3\":  \"education\",\n        \"x4\":  \"marriage\",\n        \"x5\":  \"age\",\n        \"x6\":  \"pay_0\",\n        \"x7\":  \"pay_2\",\n        \"x8\":  \"pay_3\",\n        \"x9\":  \"pay_4\",\n        \"x10\": \"pay_5\",\n        \"x11\": \"pay_6\",\n        \"x12\": \"bill_amt1\",\n        \"x13\": \"bill_amt2\",\n        \"x14\": \"bill_amt3\",\n        \"x15\": \"bill_amt4\",\n        \"x16\": \"bill_amt5\",\n        \"x17\": \"bill_amt6\",\n        \"x18\": \"pay_amt1\",\n        \"x19\": \"pay_amt2\",\n        \"x20\": \"pay_amt3\",\n        \"x21\": \"pay_amt4\",\n        \"x22\": \"pay_amt5\",\n        \"x23\": \"pay_amt6\",\n        \"y\":   \"default_payment_next_month\",\n    }\n    df = df.rename(columns={k: v for k, v in mapping.items() if k in df.columns})\n    if \"id\" not in df.columns:\n        if \"unnamed:_0\" in df.columns:\n            df = df.rename(columns={\"unnamed:_0\": \"id\"})\n        else:\n            df.insert(0, \"id\", np.arange(len(df)))\n    print(\"[PATCH] Detectado schema X1..X23 + Y. Colunas renomeadas para nomes oficiais.\")\n    print(\"Colunas (amostra):\", list(df.columns)[:15])\n\n# detectar alvo e id\ntarget_col = find_first_present(TARGET_CANDIDATES, df.columns)\nif target_col is None:\n    raise KeyError(f\"Coluna alvo n\u00e3o encontrada. Esperado uma entre: {TARGET_CANDIDATES}.\")\nid_col = find_first_present(ID_CANDIDATES, df.columns)\nprint(f\"Alvo: {target_col} | ID: {id_col if id_col else '(n\u00e3o h\u00e1 \u2014 usarei \u00edndice)'}\")\n\n# coer\u00e7\u00e3o de tipos (alvo e categ\u00f3ricas)\ndf[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\nn_nan_target = int(df[target_col].isna().sum())\nif n_nan_target &gt; 0:\n    print(f\"[PATCH] Removendo {n_nan_target} linhas com alvo inv\u00e1lido.\")\n    df = df.dropna(subset=[target_col])\ndf[target_col] = df[target_col].astype(int)\nfor c in [\"sex\", \"education\", \"marriage\"]:\n    if c in df.columns:\n        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n\n# 2) Duplicados\nprint_header(\"[2] Remo\u00e7\u00e3o de duplicados\")\nn_before = len(df)\ndf = df.drop_duplicates()\nprint(f\"Duplicados removidos: {n_before - len(df)} | shape atual: {df.shape}\")\n\n# 3) Missing antes\nprint_header(\"[3] Missing values (antes)\")\nmiss_before = df.isna().sum()\nprint(miss_before[miss_before &gt; 0] if miss_before.sum() &gt; 0 else \"Sem valores ausentes.\")\n\n# 4) Normalizar categorias inv\u00e1lidas\nprint_header(\"[4] Normalizando categorias inv\u00e1lidas (education, marriage)\")\nif \"education\" in df.columns:\n    df[\"education\"] = df[\"education\"].replace({0: 4, 5: 4, 6: 4})\nif \"marriage\" in df.columns:\n    df[\"marriage\"] = df[\"marriage\"].replace({0: 3})\n\n# 5) Categ\u00f3ricas vs. num\u00e9ricas\nprint_header(\"[5] Identificando colunas categ\u00f3ricas e num\u00e9ricas\")\nall_cols = df.columns.tolist()\ncat_cols = [c for c in CAT_COLS_RAW if c in df.columns]\nnum_cols = [c for c in all_cols if c not in cat_cols + [target_col] + ([id_col] if id_col else [])]\nprint(f\"Categ\u00f3ricas: {cat_cols}\")\nprint(f\"Num\u00e9ricas (exemplos): {num_cols[:10]} ... (total {len(num_cols)})\")\n\n# 6) Distribui\u00e7\u00e3o do alvo\nprint_header(\"[6] Distribui\u00e7\u00e3o do alvo (antes do split)\")\nclass_dist = df[target_col].value_counts().sort_index().to_dict()\nprint(f\"Distribui\u00e7\u00e3o de classes: {class_dist}\")\nwith open(outdir / \"class_dist_before.json\", \"w\") as f:\n    json.dump({int(k): int(v) for k, v in class_dist.items()}, f, indent=2)\n\n# 7) One-Hot\nprint_header(\"[7] One-Hot Encoding\")\nX_cat = pd.get_dummies(df[cat_cols].astype(\"category\"), drop_first=False) if cat_cols else pd.DataFrame(index=df.index)\nX_num = df[num_cols].copy()\n\nprint(\"Percentis num\u00e9ricos (ANTES do winsorize) [p1, p50, p99]:\")\nfor c in num_cols:\n    p1, p50, p99 = np.percentile(X_num[c], [1, 50, 99])\n    print(f\"  {c:&gt;18s}: p1={p1:.2f}, p50={p50:.2f}, p99={p99:.2f}\")\n\nprint_header(f\"[8] Winsorize/clip num\u00e9ricas (p={WINSOR_LO:.1f}\u2013{WINSOR_HI:.1f})\")\nfor c in num_cols:\n    lo, hi = np.percentile(X_num[c], [WINSOR_LO, WINSOR_HI])\n    X_num[c] = X_num[c].clip(lo, hi)\nprint(\"Percentis num\u00e9ricos (DEPOIS do winsorize) [p1, p50, p99]:\")\nfor c in num_cols:\n    p1, p50, p99 = np.percentile(X_num[c], [1, 50, 99])\n    print(f\"  {c:&gt;18s}: p1={p1:.2f}, p50={p50:.2f}, p99={p99:.2f}\")\n\nX = pd.concat([X_num, X_cat], axis=1)\ny = df[target_col].to_numpy().astype(int)\n\nprint_header(\"[10] Missing values (depois do encoding/winsorize)\")\nmiss_after = X.isna().sum()\nif miss_after.sum() &gt; 0:\n    print(miss_after[miss_after &gt; 0].sort_values(ascending=False))\n    for c in X.columns:\n        if X[c].isna().any():\n            med = X[c].median() if X[c].dtype.kind in \"if\" else 0\n            X[c] = X[c].fillna(med)\n    print(\"Ap\u00f3s imputa\u00e7\u00e3o:\", int(X.isna().sum().sum()), \"missing restantes (esperado: 0).\")\nelse:\n    print(\"Sem valores ausentes ap\u00f3s transforma\u00e7\u00f5es.\")\n\n# 11) Split 60/20/20\nprint_header(\"[11] Split estratificado 60/20/20 (train/val/test)\")\nX_np = X.to_numpy(dtype=np.float32)\nX_train, X_tmp, y_train, y_tmp = train_test_split(\n    X_np, y, test_size=0.4, random_state=SEED, stratify=y\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_tmp, y_tmp, test_size=0.5, random_state=SEED, stratify=y_tmp\n)\ndist_splits = {\n    \"train\": {int(k): int(v) for k, v in pd.Series(y_train).value_counts().sort_index().to_dict().items()},\n    \"val\":   {int(k): int(v) for k, v in pd.Series(y_val).value_counts().sort_index().to_dict().items()},\n    \"test\":  {int(k): int(v) for k, v in pd.Series(y_test).value_counts().sort_index().to_dict().items()},\n}\nprint(\"Distribui\u00e7\u00e3o por split:\", json.dumps(dist_splits, indent=2))\nwith open(outdir / \"class_dist_splits.json\", \"w\") as f:\n    json.dump(dist_splits, f, indent=2)\n\n# 12) Z-score\nprint_header(\"[12] Z-score (fit no treino, aplicar em val/test)\")\nmu = X_train.mean(axis=0, keepdims=True)\nsd = X_train.std(axis=0, keepdims=True) + 1e-8\nX_train_z = (X_train - mu) / sd\nX_val_z   = (X_val   - mu) / sd\nX_test_z  = (X_test  - mu) / sd\nprint(f\"M\u00e9dia m\u00e9dia (train, p\u00f3s z-score) \u2248 {X_train_z.mean():.4f} | Desvio m\u00e9dio \u2248 {X_train_z.std():.4f}\")\n\n# 13) Salvar artefatos\nprint_header(\"[13] Salvando artefatos e datasets\")\nnp.save(outdir / \"X_train.npy\", X_train_z)\nnp.save(outdir / \"y_train.npy\", y_train)\nnp.save(outdir / \"X_val.npy\", X_val_z)\nnp.save(outdir / \"y_val.npy\", y_val)\nnp.save(outdir / \"X_test.npy\", X_test_z)\nnp.save(outdir / \"y_test.npy\", y_test)\nnp.save(outdir / \"scaler_mu.npy\", mu.astype(np.float32))\nnp.save(outdir / \"scaler_sd.npy\", sd.astype(np.float32))\ncolumns = X.columns.tolist()\nwith open(outdir / \"columns.json\", \"w\") as f:\n    json.dump({\"columns\": columns}, f, indent=2)\n\nif SAMPLE_CSV_FRAC &gt; 0:\n    print(f\"Salvando amostra limpa ({SAMPLE_CSV_FRAC*100:.1f}%)\u2026\")\n    X_all_z = np.vstack([X_train_z, X_val_z, X_test_z])\n    y_all   = np.concatenate([y_train, y_val, y_test])\n    df_clean = pd.DataFrame(X_all_z, columns=columns)\n    df_clean[\"target\"] = y_all\n    df_clean.sample(frac=SAMPLE_CSV_FRAC, random_state=SEED).to_csv(\n        outdir / \"cleaned_full_sample.csv\", index=False\n    )\n\nprint(\"\\n\u2705 Cleaning conclu\u00eddo. Artefatos salvos em:\", outdir.resolve())\n</code></pre>"},{"location":"projeto1/main/#-_8","title":"------------------------------------------------------------------------------","text":""},{"location":"projeto1/main/#section-4-mlp-implementation-numpy","title":"SECTION 4 \u2014 MLP IMPLEMENTATION (NumPy)","text":""},{"location":"projeto1/main/#-_9","title":"------------------------------------------------------------------------------","text":""},{"location":"projeto1/main/#-utils-de-metricas-e-batches-","title":"-------- utils de m\u00e9tricas e batches --------","text":"<p>def one_hot(y, n_classes):     oh = np.zeros((y.shape[0], n_classes), dtype=np.float32)     oh[np.arange(y.shape[0]), y] = 1.0     return oh</p> <p>def accuracy(y_true, y_pred):     return float((y_true == y_pred).mean())</p> <p>def precision_recall_f1(y_true, y_pred, positive=1):     tp = np.sum((y_true == positive) &amp; (y_pred == positive))     fp = np.sum((y_true != positive) &amp; (y_pred == positive))     fn = np.sum((y_true == positive) &amp; (y_pred != positive))     prec = tp / (tp + fp + 1e-12)     rec  = tp / (tp + fn + 1e-12)     f1   = 2precrec/(prec+rec+1e-12)     return float(prec), float(rec), float(f1)</p> <p>def roc_auc_score_binary(y_true, scores):     pos = scores[y_true == 1]     neg = scores[y_true == 0]     if len(pos) == 0 or len(neg) == 0:         return float(\"nan\")     concat = np.concatenate([pos, neg])     order = np.argsort(concat, kind=\"mergesort\")     ranks = np.empty_like(order, dtype=np.float64)     ranks[order] = np.arange(1, len(concat) + 1)  # 1..N     r_pos = ranks[:len(pos)]     auc = (r_pos.sum() - len(pos)(len(pos)+1)/2) / (len(pos)len(neg) + 1e-12)     return float(auc)</p> <p>def iterate_minibatches(X, Y, batch, rng):     idx = rng.permutation(len(X))     for i in range(0, len(X), batch):         ib = idx[i:i+batch]         yield X[ib], Y[ib]</p>"},{"location":"projeto1/main/#-classe-mlp-","title":"-------- classe MLP --------","text":"<p>class MLP:     def init(self, d_in, layers=(64,), d_out=2, seed=42, l2=1e-4, momentum=0.9):         rng = np.random.default_rng(seed)         self.l2 = l2         self.class_weights = None  # definido externamente, se desejado         self.momentum = momentum         dims = [d_in] + list(layers) + [d_out]         self.params = {}         self.v = {}  # velocidades para momentum</p> <pre><code>    # Inicializa\u00e7\u00e3o He (ReLU): N(0, sqrt(2/fan_in))\n    for i in range(len(dims)-1):\n        fan_in = dims[i]\n        W = rng.normal(0, np.sqrt(2.0/fan_in), size=(dims[i], dims[i+1])).astype(np.float32)\n        b = np.zeros((1, dims[i+1]), dtype=np.float32)\n        self.params[f\"W{i+1}\"] = W\n        self.params[f\"b{i+1}\"] = b\n        self.v[f\"W{i+1}\"] = np.zeros_like(W)\n        self.v[f\"b{i+1}\"] = np.zeros_like(b)\n\ndef set_class_weights(self, cw):\n    self.class_weights = np.asarray(cw, dtype=np.float32)\n\ndef init_output_bias_with_prior(self, p_pos):\n    \"\"\"Define b0=0 e b1=logit(p) para sa\u00edda bin\u00e1ria.\"\"\"\n    p = float(np.clip(p_pos, 1e-6, 1-1e-6))\n    logit = np.log(p / (1.0 - p)).astype(np.float32)\n    L = len(self.params)//2\n    b = self.params[f\"b{L}\"].copy()\n    if b.shape[1] == 2:\n        b[:, 0] = 0.0\n        b[:, 1] = logit\n        self.params[f\"b{L}\"] = b\n\n@staticmethod\ndef relu(x):\n    return np.maximum(0, x)\n\n@staticmethod\ndef relu_grad(x):\n    return (x &gt; 0).astype(np.float32)\n\n@staticmethod\ndef softmax(z):\n    z = z - z.max(axis=1, keepdims=True)\n    e = np.exp(z, dtype=np.float32)\n    return e / (e.sum(axis=1, keepdims=True) + 1e-12)\n\ndef forward(self, X):\n    cache = {\"A0\": X}\n    A = X\n    L = len(self.params)//2\n    for i in range(1, L):\n        Z = A @ self.params[f\"W{i}\"] + self.params[f\"b{i}\"]\n        A = self.relu(Z)\n        cache[f\"Z{i}\"] = Z; cache[f\"A{i}\"] = A\n    ZL = A @ self.params[f\"W{L}\"] + self.params[f\"b{L}\"]\n    P = self.softmax(ZL)\n    cache[f\"Z{L}\"] = ZL; cache[f\"A{L}\"] = P\n    return P, cache\n\ndef loss(self, P, Y_onehot):\n    # Cross-Entropy ponderada por classe + L2\n    if self.class_weights is None:\n        cw = np.ones(Y_onehot.shape[1], dtype=np.float32)\n    else:\n        cw = self.class_weights\n    w_i = (Y_onehot * cw).sum(axis=1)  # peso por amostra\n    sum_w = float(w_i.sum() + 1e-12)\n    ce_per_sample = -np.sum(Y_onehot * np.log(P + 1e-12), axis=1)\n    ce = float(np.sum(w_i * ce_per_sample) / sum_w)\n\n    l2_term = 0.0\n    L = len(self.params)//2\n    for i in range(1, L+1):\n        l2_term += np.sum(self.params[f\"W{i}\"]**2)\n    return ce + self.l2 * 0.5 * l2_term\n\ndef backward(self, cache, Y_onehot):\n    grads = {}\n    if self.class_weights is None:\n        cw = np.ones(Y_onehot.shape[1], dtype=np.float32)\n    else:\n        cw = self.class_weights\n    w_i = (Y_onehot * cw).sum(axis=1)[:, None]  # (N,1)\n    sum_w = float(w_i.sum() + 1e-12)\n\n    L = len(self.params)//2\n    A_L = cache[f\"A{L}\"]  # probs\n\n    # dZ (softmax + CE) ponderado\n    dZ = ((A_L - Y_onehot) * w_i) / sum_w\n    A_prev = cache[f\"A{L-1}\"] if L &gt; 1 else cache[\"A0\"]\n    grads[f\"W{L}\"] = A_prev.T @ dZ + self.l2 * self.params[f\"W{L}\"]\n    grads[f\"b{L}\"] = np.sum(dZ, axis=0, keepdims=True)\n    dA_prev = dZ @ self.params[f\"W{L}\"].T\n\n    # camadas escondidas (ReLU)\n    for i in range(L-1, 0, -1):\n        Z = cache[f\"Z{i}\"]; A_prev = cache[f\"A{i-1}\"] if i &gt; 1 else cache[\"A0\"]\n        dZ = dA_prev * self.relu_grad(Z)\n        grads[f\"W{i}\"] = A_prev.T @ dZ + self.l2 * self.params[f\"W{i}\"]\n        grads[f\"b{i}\"] = np.sum(dZ, axis=0, keepdims=True)\n        dA_prev = dZ @ self.params[f\"W{i}\"].T\n    return grads\n\ndef step_sgd(self, grads, lr):\n    \"\"\"SGD com momentum cl\u00e1ssico.\"\"\"\n    L = len(self.params)//2\n    for i in range(1, L+1):\n        self.v[f\"W{i}\"] = self.momentum * self.v[f\"W{i}\"] + grads[f\"W{i}\"]\n        self.v[f\"b{i}\"] = self.momentum * self.v[f\"b{i}\"] + grads[f\"b{i}\"]\n        self.params[f\"W{i}\"] -= lr * self.v[f\"W{i}\"]\n        self.params[f\"b{i}\"] -= lr * self.v[f\"b{i}\"]\n\ndef predict_proba(self, X):\n    P, _ = self.forward(X)\n    return P\n\ndef predict(self, X, threshold=None):\n    P, _ = self.forward(X)\n    if threshold is None:\n        return np.argmax(P, axis=1), P[:, 1]\n    else:\n        ppos = P[:, 1]\n        yhat = (ppos &gt;= threshold).astype(int)\n        return yhat, ppos\n</code></pre> <p>def train_mlp_numpy(outdir: Path):     print_header(\"\ud83d\udd27 [4] MLP Implementation (NumPy) \u2014 Treino/Val/Test\")</p> <pre><code># carregar dados processados\nX_train = np.load(outdir / \"X_train.npy\")\ny_train = np.load(outdir / \"y_train.npy\")\nX_val   = np.load(outdir / \"X_val.npy\")\ny_val   = np.load(outdir / \"y_val.npy\")\nX_test  = np.load(outdir / \"X_test.npy\")\ny_test  = np.load(outdir / \"y_test.npy\")\n\nn_classes = int(np.max([y_train.max(), y_val.max(), y_test.max()]) + 1)\nY_train = one_hot(y_train, n_classes)\nY_val   = one_hot(y_val, n_classes)\n\n# class weights \"balanced\": N / (K * n_c)\ncounts = np.bincount(y_train, minlength=n_classes).astype(np.float32)\ncw_balanced = (len(y_train) / (n_classes * counts + 1e-12)).astype(np.float32)\n\nmlp = MLP(\n    d_in=X_train.shape[1],\n    layers=MLP_LAYERS,\n    d_out=n_classes,\n    seed=SEED,\n    l2=MLP_L2,\n    momentum=MOMENTUM\n)\nmlp.set_class_weights(cw_balanced)\n\n# inicializa vi\u00e9s de sa\u00edda com a preval\u00eancia da classe positiva (para bin\u00e1rio)\np_pos_train = float((y_train == 1).mean())\nmlp.init_output_bias_with_prior(p_pos_train)\n\nrng = np.random.default_rng(SEED)\nlr = LR_INIT\nbest_vl = np.inf\nwait = 0\nhist = {\"loss_tr\": [], \"loss_vl\": []}\nbest_params = {k: v.copy() for k, v in mlp.params.items()}\n\nfor ep in range(1, EPOCHS+1):\n    # ===== treino (mini-batch SGD cobrindo TODO o dataset) =====\n    for xb, yb in iterate_minibatches(X_train, Y_train, BATCH_SIZE, rng):\n        P, cache = mlp.forward(xb)\n        loss = mlp.loss(P, yb)\n        grads = mlp.backward(cache, yb)\n        mlp.step_sgd(grads, lr)\n\n    # logging perdas em fim de \u00e9poca\n    P_tr = mlp.predict_proba(X_train); loss_tr = mlp.loss(P_tr, Y_train)\n    P_vl = mlp.predict_proba(X_val);   loss_vl = mlp.loss(P_vl, Y_val)\n    hist[\"loss_tr\"].append(loss_tr); hist[\"loss_vl\"].append(loss_vl)\n    print(f\"Epoch {ep:02d} | loss_tr={loss_tr:.4f}  loss_vl={loss_vl:.4f}  lr={lr:.4f}\")\n\n    # early stopping\n    if loss_vl &lt; best_vl - 1e-4:\n        best_vl = loss_vl; wait = 0\n        best_params = {k: v.copy() for k, v in mlp.params.items()}\n    else:\n        wait += 1\n        if wait &gt;= PATIENCE:\n            print(\"Early stopping acionado.\")\n            break\n\n    # lr decay\n    if (ep % LR_DECAY_EVERY) == 0:\n        lr *= LR_DECAY\n\n# restaurar melhores pesos\nmlp.params = best_params\n\n# ---- escolher threshold pelo melhor F1 na valida\u00e7\u00e3o ----\n_, ppos_val = mlp.predict(X_val, threshold=None)\nbest_t, best_f1 = 0.5, -1.0\nfor t in np.linspace(0.05, 0.95, 91):\n    yhat_t = (ppos_val &gt;= t).astype(int)\n    _, _, f1_t = precision_recall_f1(y_val, yhat_t, positive=1)\n    if f1_t &gt; best_f1:\n        best_f1 = f1_t\n        best_t = float(t)\nprint(f\"\\nThreshold escolhido na valida\u00e7\u00e3o (melhor F1): t* = {best_t:.2f} (F1={best_f1:.4f})\")\nprint(f\"Propor\u00e7\u00e3o prevista como positiva em val @t*: {(ppos_val &gt;= best_t).mean():.3f}\")\n\n# avalia\u00e7\u00e3o\ndef report(split, X, y, threshold):\n    y_pred, ppos = mlp.predict(X, threshold=threshold)\n    acc = accuracy(y, y_pred)\n    prec, rec, f1 = precision_recall_f1(y, y_pred, positive=1)\n    auc = roc_auc_score_binary(y, ppos)\n    print(f\"[{split}] acc={acc:.4f}  prec={prec:.4f}  rec={rec:.4f}  f1={f1:.4f}  auc={auc:.4f}\")\n    return {\"acc\": acc, \"prec\": prec, \"rec\": rec, \"f1\": f1, \"auc\": auc, \"pos_rate\": float((y_pred==1).mean())}\n\nprint()\nmtr_tr = report(\"train\", X_train, y_train, threshold=best_t)\nmtr_vl = report(\"val\",   X_val,   y_val,   threshold=best_t)\nmtr_te = report(\"test\",  X_test,  y_test,  threshold=best_t)\n\n# salvar curvas\npd.DataFrame(hist).to_csv(outdir / \"training_curves.csv\", index=False)\n# salvar m\u00e9tricas finais + threshold\nfinal_metrics = {\n    \"train\": mtr_tr, \"val\": mtr_vl, \"test\": mtr_te,\n    \"layers\": list(MLP_LAYERS), \"l2\": MLP_L2, \"lr_init\": LR_INIT,\n    \"batch_size\": BATCH_SIZE, \"epochs\": EPOCHS, \"patience\": PATIENCE,\n    \"class_weights\": cw_balanced.tolist(),\n    \"threshold_val_f1\": best_t,\n    \"momentum\": MOMENTUM\n}\nwith open(outdir / \"final_metrics.json\", \"w\") as f:\n    json.dump(final_metrics, f, indent=2)\n\nprint(\"\\n\u2705 Treinamento conclu\u00eddo. M\u00e9tricas salvas em:\", outdir / \"final_metrics.json\")\nprint(\"Curvas de treino salvas em:\", outdir / \"training_curves.csv\")\n</code></pre>"},{"location":"projeto1/main/#-_10","title":"------------------------------------------------------------------------------","text":""},{"location":"projeto1/main/#main","title":"MAIN","text":""},{"location":"projeto1/main/#-_11","title":"------------------------------------------------------------------------------","text":"<p>def main():     script_dir = Path(file).parent     outdir = script_dir / OUTDIR_NAME</p> <pre><code># Se ainda n\u00e3o existir X_train.npy, roda o cleaning\nneed_clean = not (outdir / \"X_train.npy\").exists()\nif need_clean:\n    run_cleaning_and_save(script_dir)\nelse:\n    print_header(\"\ud83d\udd01 Artefatos de cleaning encontrados \u2014 pulando etapa de limpeza.\")\n\n# Treinar MLP (NumPy)\ntrain_mlp_numpy(outdir)\n</code></pre> <p>if name == \"main\":     main()</p> <pre><code>## 5. Curvas de Erro e Visualiza\u00e7\u00f5es\n\nA curva de perda (**loss**) foi monitorada durante o treinamento do MLP, tanto no conjunto de **treino** quanto de **valida\u00e7\u00e3o**, ao longo das \u00e9pocas. Isso permite avaliar o comportamento do modelo em rela\u00e7\u00e3o a **converg\u00eancia**, **overfitting** e **early stopping**.\n\n![Curva de perda do modelo](assets/loss_curve.png)\n\nComo mostrado no gr\u00e1fico acima:\n\n- O **loss de treino** decresce consistentemente at\u00e9 estabilizar.\n- O **loss de valida\u00e7\u00e3o** tamb\u00e9m decresce nas primeiras \u00e9pocas, mas apresenta uma estabiliza\u00e7\u00e3o e flutua\u00e7\u00e3o posterior.\n- O modelo utilizou **early stopping com paci\u00eancia de 8 \u00e9pocas**, interrompendo o treinamento antes de overfitting.\n- A partir da \u00e9poca ~20, n\u00e3o houve mais ganhos relevantes na valida\u00e7\u00e3o, indicando que o modelo j\u00e1 havia convergido.\n\nEsse padr\u00e3o \u00e9 t\u00edpico em dados com certo desbalanceamento: o modelo consegue otimizar a perda, mas o ganho em recall e F1 tende a saturar cedo.\n\n### C\u00f3digo para gerar a curva de perda:\n\n``` pyodide install=\"pandas, matplotlib, os\"\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Garantir que o diret\u00f3rio de sa\u00edda existe\nos.makedirs(\"assets\", exist_ok=True)\n\n# Carregar o CSV com as curvas\ndf = pd.read_csv(\"processed/training_curves.csv\")\n\n# Plotar curvas de perda\nplt.figure(figsize=(8, 5))\nplt.plot(df[\"loss_tr\"], label=\"Treino\", linewidth=2)\nplt.plot(df[\"loss_vl\"], label=\"Valida\u00e7\u00e3o\", linewidth=2)\nplt.xlabel(\"\u00c9poca\")\nplt.ylabel(\"Loss (Cross-Entropy)\")\nplt.title(\"Curva de perda (loss) por \u00e9poca\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\n\n# Salvar imagem\nplt.savefig(\"assets/loss_curve.png\")\nplt.show()\n</code></pre>"},{"location":"projeto1/main/#6-metricas-de-avaliacao-e-matriz-de-confusao","title":"6. M\u00e9tricas de Avalia\u00e7\u00e3o e Matriz de Confus\u00e3o","text":"<p>Al\u00e9m das m\u00e9tricas quantitativas (acur\u00e1cia, precis\u00e3o, recall, F1-score e AUC), a matriz de confus\u00e3o oferece uma vis\u00e3o clara dos tipos de erro que o modelo comete ao classificar clientes no conjunto de teste.</p> <p></p>"},{"location":"projeto1/main/#codigo-para-gerar-a-matriz-de-confusao","title":"C\u00f3digo para gerar a matriz de confus\u00e3o:","text":"<p>``` pyodide install=\"numpy, matplotlib, seaborn, sklearn\"</p> <p>import numpy as np import json from pathlib import Path from projeto1 import MLP, one_hot  # importa sua classe e fun\u00e7\u00e3o</p>"},{"location":"projeto1/main/#paths","title":"Paths","text":"<p>outdir = Path(\"processed\") X_test = np.load(outdir / \"X_test.npy\") y_test = np.load(outdir / \"y_test.npy\") X_train = np.load(outdir / \"X_train.npy\") y_train = np.load(outdir / \"y_train.npy\") X_val = np.load(outdir / \"X_val.npy\") y_val = np.load(outdir / \"y_val.npy\")</p>"},{"location":"projeto1/main/#hiperparametros-voce-pode-pegar-isso-do-final_metricsjson-tambem","title":"Hiperpar\u00e2metros (voc\u00ea pode pegar isso do final_metrics.json tamb\u00e9m)","text":"<p>with open(outdir / \"final_metrics.json\") as f:     final_metrics = json.load(f)</p> <p>layers = tuple(final_metrics[\"layers\"]) l2 = final_metrics[\"l2\"] momentum = final_metrics[\"momentum\"] threshold = final_metrics[\"threshold_val_f1\"] batch_size = final_metrics[\"batch_size\"] epochs = final_metrics[\"epochs\"] lr_init = final_metrics[\"lr_init\"] patience = final_metrics[\"patience\"] class_weights = np.array(final_metrics[\"class_weights\"], dtype=np.float32)</p>"},{"location":"projeto1/main/#one-hot","title":"One-hot","text":"<p>Y_train = one_hot(y_train, 2) Y_val = one_hot(y_val, 2)</p>"},{"location":"projeto1/main/#treinar-novamente-o-mlp-com-os-mesmos-dados-replicando-treino-para-recuperar-pesos","title":"Treinar novamente o MLP com os mesmos dados (replicando treino para recuperar pesos)","text":"<p>mlp = MLP(     d_in=X_train.shape[1],     layers=layers,     d_out=2,     seed=42,     l2=l2,     momentum=momentum ) mlp.set_class_weights(class_weights) mlp.init_output_bias_with_prior(float((y_train == 1).mean()))</p>"},{"location":"projeto1/main/#repetir-treino","title":"Repetir treino","text":"<p>rng = np.random.default_rng(42) best_params = {k: v.copy() for k, v in mlp.params.items()} best_vl = float(\"inf\") wait = 0 lr = lr_init</p> <p>def iterate_minibatches(X, Y, batch, rng):     idx = rng.permutation(len(X))     for i in range(0, len(X), batch):         ib = idx[i:i+batch]         yield X[ib], Y[ib]</p> <p>for ep in range(1, epochs+1):     for xb, yb in iterate_minibatches(X_train, Y_train, batch_size, rng):         P, cache = mlp.forward(xb)         loss = mlp.loss(P, yb)         grads = mlp.backward(cache, yb)         mlp.step_sgd(grads, lr)     # Valida\u00e7\u00e3o     P_vl, _ = mlp.forward(X_val)     loss_vl = mlp.loss(P_vl, Y_val)     if loss_vl &lt; best_vl - 1e-4:         best_vl = loss_vl         wait = 0         best_params = {k: v.copy() for k, v in mlp.params.items()}     else:         wait += 1         if wait &gt;= patience:             break     if ep % 5 == 0:         lr *= 0.9</p>"},{"location":"projeto1/main/#restaurar-melhores-pesos","title":"Restaurar melhores pesos","text":"<p>mlp.params = best_params</p>"},{"location":"projeto1/main/#predicao-no-teste-com-threshold-otimo","title":"Predi\u00e7\u00e3o no teste com threshold \u00f3timo","text":"<p>yhat_test, _ = mlp.predict(X_test, threshold=threshold) np.save(outdir / \"final_yhat_test.npy\", yhat_test) print(\"\u2705 Predi\u00e7\u00f5es salvas em processed/final_yhat_test.npy\")</p> <p>import numpy as np import seaborn as sns from sklearn.metrics import confusion_matrix import matplotlib.pyplot as plt</p>"},{"location":"projeto1/main/#carregar-dados","title":"Carregar dados","text":"<p>y_test = np.load(\"processed/y_test.npy\") y_pred = np.load(\"processed/final_yhat_test.npy\")  # ou gere a predi\u00e7\u00e3o no seu script final</p>"},{"location":"projeto1/main/#confusion-matrix","title":"Confusion matrix","text":"<p>cm = confusion_matrix(y_test, y_pred) labels = [\"N\u00e3o Inadimplente\", \"Inadimplente\"]</p>"},{"location":"projeto1/main/#plot","title":"Plot","text":"<p>plt.figure(figsize=(6, 5)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels) plt.xlabel(\"Predito\") plt.ylabel(\"Real\") plt.title(\"Matriz de Confus\u00e3o \u2014 Conjunto de Teste\") plt.tight_layout() plt.savefig(\"assets/confusion_matrix.png\")  # ajuste o caminho conforme o GitHub Pages plt.show()</p> <p>```</p>"},{"location":"projeto1/main/#interpretacao","title":"Interpreta\u00e7\u00e3o:","text":"<ul> <li>3596 clientes foram corretamente identificados como n\u00e3o inadimplentes (verdadeiros negativos).</li> <li>855 clientes inadimplentes foram corretamente identificados (verdadeiros positivos).</li> <li>472 inadimplentes foram classificados como n\u00e3o inadimplentes (falsos negativos), o que representa um risco de cr\u00e9dito n\u00e3o detectado.</li> <li>1077 n\u00e3o inadimplentes foram classificados incorretamente como inadimplentes (falsos positivos), o que pode levar \u00e0 recusa de cr\u00e9dito injusta.</li> </ul>"},{"location":"projeto1/main/#conclusao","title":"Conclus\u00e3o:","text":"<p>O modelo apresenta um bom desempenho em recuperar inadimplentes, com 855 acertos, mas ainda comete 472 erros cr\u00edticos (falsos negativos) \u2014 o que pode impactar negativamente institui\u00e7\u00f5es financeiras que dependem dessa previs\u00e3o para concess\u00e3o de cr\u00e9dito.</p> <p>A calibragem via threshold \u00f3timo na valida\u00e7\u00e3o priorizou o F1-score e recall da classe minorit\u00e1ria, aceitando sacrificar parte da precis\u00e3o para detectar mais inadimplentes. Essa escolha foi intencional, considerando que o custo de um falso negativo (inadimplente n\u00e3o detectado) \u00e9 normalmente maior que o de um falso positivo.</p> <p>Nota: Os dados s\u00e3o desbalanceados (~22% inadimplentes), e por isso o modelo foi treinado com pesos de classe ajustados, regulariza\u00e7\u00e3o L2, e early stopping, al\u00e9m da normaliza\u00e7\u00e3o por z-score.</p>"},{"location":"projeto1/main/#uso-de-ia","title":"Uso de I.A.","text":"<p>Utilizamos o aux\u00edlio do chatGPT para: - Fazer README do projeto. - Gerar fun\u00e7\u00f5es auxiliares em python. - Revisar e melhorar trechos de c\u00f3digo.</p>"},{"location":"projeto2/main/","title":"Projeto 2 - Regression - Tomas Miele e Yuri Tabacof","text":""},{"location":"projeto2/main/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>Nome do dataset: S&amp;P 500 Historical Data \u2014 Stock Market Index Prediction Fonte: Yahoo Finance / Kaggle URL: https://www.kaggle.com/datasets/whenamancodes/sp-500-stock-data Tamanho: ~15.000 registros di\u00e1rios, 7 vari\u00e1veis (Open, High, Low, Close, Adj Close, Volume, Date)  </p> <p>Tarefa: Regress\u00e3o \u2014 prever o pre\u00e7o de fechamento (Close) do \u00edndice S&amp;P 500 com base em vari\u00e1veis hist\u00f3ricas de mercado.  </p> <p>Justificativa da escolha: - O problema \u00e9 financeiro e pr\u00e1tico, ligado \u00e0 previs\u00e3o de pre\u00e7os \u2014 uma aplica\u00e7\u00e3o cl\u00e1ssica de regress\u00e3o cont\u00ednua. - Os dados representam s\u00e9ries temporais reais com ru\u00eddo, tend\u00eancia e sazonalidade, o que desafia o modelo e permite testar preprocessing, regulariza\u00e7\u00e3o e tuning do MLP. - Cont\u00e9m vari\u00e1veis correlacionadas e cont\u00ednuas (pre\u00e7o de abertura, volume, m\u00e1ximas e m\u00ednimas), adequadas para explora\u00e7\u00e3o de rela\u00e7\u00f5es n\u00e3o lineares via redes neurais. - O volume de dados \u00e9 suficiente (&gt;1.000 amostras) e com m\u00faltiplos atributos (&gt;5), atendendo integralmente aos crit\u00e9rios do projeto. - Permite ainda incorporar engenharia de features financeiras, como retornos logar\u00edtmicos, m\u00e9dias m\u00f3veis e volatilidade, tornando o problema mais robusto e pr\u00f3ximo de aplica\u00e7\u00f5es reais de quantitative finance. - A base \u00e9 p\u00fablica e reproduz\u00edvel, podendo ser facilmente obtida via API do Yahoo Finance (<code>yfinance</code>) ou baixada do Kaggle, sem restri\u00e7\u00f5es de uso.  </p>"},{"location":"projeto2/main/#2-dataset-explanation","title":"2. Dataset Explanation","text":"<p>Representa\u00e7\u00e3o e origem. O dataset consiste em dados hist\u00f3ricos di\u00e1rios do \u00edndice S&amp;P 500 (ticker <code>^GSPC</code>), obtidos via Yahoo Finance. Ele representa a evolu\u00e7\u00e3o do principal \u00edndice acion\u00e1rio dos Estados Unidos, refletindo o desempenho m\u00e9dio ponderado das 500 maiores empresas listadas nas bolsas NYSE e NASDAQ. O objetivo \u00e9 realizar regress\u00e3o cont\u00ednua, prevendo o pre\u00e7o ajustado de fechamento do dia seguinte (target) a partir das vari\u00e1veis conhecidas no dia atual.</p>"},{"location":"projeto2/main/#variaveis-e-tipos","title":"Vari\u00e1veis e tipos","text":"<p>Todas as vari\u00e1veis s\u00e3o num\u00e9ricas cont\u00ednuas, derivadas da estrutura padr\u00e3o de mercado OHLCV (Open, High, Low, Close, Volume) e de indicadores t\u00e9cnicos calculados via janelas m\u00f3veis.</p> Categoria Vari\u00e1veis Descri\u00e7\u00e3o Base OHLCV <code>open</code>, <code>high</code>, <code>low</code>, <code>close</code>, <code>adj_close</code>, <code>volume</code> Pre\u00e7os e volume de negocia\u00e7\u00e3o di\u00e1rios. Retornos e defasagens <code>ret_1d</code>, <code>logret_1d</code>, <code>adj_close_lag1</code>, <code>volume_lag1</code> Retornos percentuais e logar\u00edtmicos, com defasagens de um dia. M\u00e9dias m\u00f3veis e volatilidade <code>ma_5</code>, <code>ma_20</code>, <code>ma_60</code>, <code>vol_20</code> M\u00e9dias m\u00f3veis de 5, 20 e 60 dias e desvio padr\u00e3o dos retornos (volatilidade). Amplitude e posi\u00e7\u00e3o do fechamento <code>hl_range</code>, <code>close_pos_range</code> Medem a oscila\u00e7\u00e3o intradi\u00e1ria e onde o fechamento ocorre dentro do range di\u00e1rio. <p>Vari\u00e1vel-alvo (target): <code>target_adj_close_tplus1</code> \u2014 o pre\u00e7o ajustado de fechamento do dia seguinte (t+1), definido de modo a evitar vazamento de informa\u00e7\u00e3o.</p>"},{"location":"projeto2/main/#conhecimento-de-dominio","title":"Conhecimento de dom\u00ednio","text":"<ul> <li>OHLCV: estrutura cl\u00e1ssica de cota\u00e7\u00f5es usada em finan\u00e7as quantitativas.  </li> <li>Adj Close: pre\u00e7o de fechamento ajustado por dividendos e stock splits, ideal para an\u00e1lises hist\u00f3ricas.  </li> <li>Retornos logar\u00edtmicos: \u00fateis para modelagem estat\u00edstica, pois s\u00e3o aproximadamente sim\u00e9tricos e aditivos no tempo.  </li> <li>Volatilidade: aproxima o risco de curto prazo do mercado.  </li> <li>M\u00e9dias m\u00f3veis: reduzem ru\u00eddo e capturam tend\u00eancias.  </li> <li>Amplitude (<code>hl_range</code>) e posi\u00e7\u00e3o (<code>close_pos_range</code>): medem for\u00e7a intradi\u00e1ria de movimento e fechamento relativo, \u00fateis como momentum indicators.</li> </ul>"},{"location":"projeto2/main/#estatisticas-e-diagnosticos","title":"Estat\u00edsticas e diagn\u00f3sticos","text":"<p>Ap\u00f3s limpeza de dados incompletos (devido a janelas m\u00f3veis e lags), o conjunto resultou em aproximadamente 8.500 observa\u00e7\u00f5es com 16 vari\u00e1veis preditoras.  </p> <ul> <li>N\u00e3o h\u00e1 valores ausentes significativos ap\u00f3s o tratamento.  </li> <li>Outliers (via |z| &gt; 4) foram identificados principalmente em <code>ret_1d</code> e <code>volume</code>, coerentes com choques de mercado.  </li> <li>As features derivadas de pre\u00e7o exibem forte colinearidade (\u03c1 \u2248 1), especialmente entre <code>open</code>, <code>close</code>, <code>adj_close</code> e m\u00e9dias m\u00f3veis \u2014 algo esperado em dados financeiros.</li> </ul>"},{"location":"projeto2/main/#matriz-de-correlacao","title":"Matriz de correla\u00e7\u00e3o","text":"<p>A matriz mostra altas correla\u00e7\u00f5es lineares entre vari\u00e1veis de pre\u00e7o, enquanto <code>ret_1d</code> e <code>logret_1d</code> apresentam correla\u00e7\u00e3o pr\u00f3xima de zero com os n\u00edveis de pre\u00e7o, o que \u00e9 desej\u00e1vel para eliminar redund\u00e2ncia.  </p>"},{"location":"projeto2/main/#distribuicoes-das-variaveis-principais","title":"Distribui\u00e7\u00f5es das vari\u00e1veis principais","text":""},{"location":"projeto2/main/#retornos-diarios","title":"Retornos di\u00e1rios","text":"<p>Ambos os retornos apresentam forma aproximadamente normal centrada em zero, com caudas levemente alongadas \u2014 padr\u00e3o t\u00edpico de s\u00e9ries financeiras com eventos extremos ocasionais (crises).</p>"},{"location":"projeto2/main/#volatilidade-e-volume","title":"Volatilidade e volume","text":"<p>A volatilidade (<code>vol_20</code>) segue distribui\u00e7\u00e3o assim\u00e9trica positiva, indicando predomin\u00e2ncia de per\u00edodos de baixa volatilidade e raros picos altos. O volume apresenta m\u00faltiplos modos (picos), sugerindo mudan\u00e7as estruturais no mercado ao longo dos anos (ex: maior liquidez p\u00f3s-2010).</p>"},{"location":"projeto2/main/#indicadores-de-amplitude-e-posicao","title":"Indicadores de amplitude e posi\u00e7\u00e3o","text":"<p><code>hl_range</code> \u00e9 fortemente enviesado \u00e0 esquerda, mostrando que na maioria dos dias a oscila\u00e7\u00e3o intradi\u00e1ria \u00e9 pequena. <code>close_pos_range</code> tem concentra\u00e7\u00e3o nas extremidades 0 e 1 \u2014 o fechamento tende a ocorrer pr\u00f3ximo das m\u00e1ximas ou m\u00ednimas do dia, indicando momentum ou revers\u00f5es fortes.</p>"},{"location":"projeto2/main/#relacoes-com-a-variavel-alvo","title":"Rela\u00e7\u00f5es com a vari\u00e1vel-alvo","text":""},{"location":"projeto2/main/#target-vs-preco-atual","title":"Target vs. pre\u00e7o atual","text":"<p>A rela\u00e7\u00e3o entre <code>adj_close (t)</code> e <code>target_adj_close (t+1)</code> \u00e9 altamente linear, como esperado, pois o pre\u00e7o de fechamento de hoje \u00e9 o principal determinante do de amanh\u00e3.</p>"},{"location":"projeto2/main/#target-vs-volatilidade","title":"Target vs. volatilidade","text":"<p>A volatilidade mostra baixa correla\u00e7\u00e3o direta com o pre\u00e7o futuro \u2014 atua mais como indicador indireto de incerteza do movimento, e n\u00e3o de n\u00edvel de pre\u00e7o.</p>"},{"location":"projeto2/main/#serie-historica","title":"S\u00e9rie hist\u00f3rica","text":"<p>A trajet\u00f3ria de longo prazo exibe uma tend\u00eancia estrutural de alta, intercalada por per\u00edodos de forte queda (crises de 2000, 2008, 2020). Essa n\u00e3o-estacionariedade justifica o uso de retornos e indicadores normalizados como features no modelo.</p>"},{"location":"projeto2/main/#observacoes-para-modelagem","title":"Observa\u00e7\u00f5es para modelagem","text":"<ul> <li>As features de pre\u00e7o e volume apresentam escalas distintas, exigindo normaliza\u00e7\u00e3o antes do treinamento (StandardScaler ou MinMaxScaler).  </li> <li>H\u00e1 alta redund\u00e2ncia linear entre pre\u00e7os e m\u00e9dias m\u00f3veis \u2014 regulariza\u00e7\u00e3o (L2, Dropout) ajudar\u00e1 o MLP a evitar sobreajuste.  </li> <li>Os retornos e volatilidades fornecem informa\u00e7\u00e3o incremental de movimento, sendo \u00fateis para capturar n\u00e3o-linearidades sutis.  </li> <li>O dataset \u00e9 suficientemente extenso e complexo para testar diferentes arquiteturas e t\u00e9cnicas de regulariza\u00e7\u00e3o.</li> </ul>"},{"location":"projeto2/main/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization","text":"<p>Objetivo. Esta etapa teve como meta garantir a qualidade e consist\u00eancia dos dados antes do treinamento do MLP, reduzindo ru\u00eddo e diferen\u00e7as de escala entre vari\u00e1veis. Todas as transforma\u00e7\u00f5es foram aplicadas de forma cronol\u00f3gica (sem vazamento de informa\u00e7\u00e3o), com o fit feito apenas sobre o conjunto de treino.</p>"},{"location":"projeto2/main/#31-limpeza-de-dados","title":"3.1 Limpeza de dados","text":"<p>Remo\u00e7\u00e3o de duplicatas. O \u00edndice temporal (<code>date</code>) foi verificado quanto a duplica\u00e7\u00f5es e apenas o primeiro registro de cada data foi mantido.  </p> <p>Valores faltantes. Ap\u00f3s a engenharia de atributos na Parte 2 (lags, m\u00e9dias m\u00f3veis e volatilidades), restaram alguns <code>NaN</code> nas primeiras linhas geradas por janelas deslizantes. Essas linhas foram automaticamente removidas. A checagem final (<code>missing_report_after_cleaning.csv</code>) confirma aus\u00eancia de valores faltantes relevantes:</p> Coluna Missing adj_close 0 volume_lag1 0 close_pos_range 0 hl_range 0 vol_20 0 <p>Justificativa. Optou-se por remover e n\u00e3o imputar valores faltantes, pois imputa\u00e7\u00f5es em s\u00e9ries financeiras podem distorcer padr\u00f5es temporais e autocorrela\u00e7\u00f5es.  </p>"},{"location":"projeto2/main/#32-divisao-temporal","title":"3.2 Divis\u00e3o temporal","text":"<p>Os dados foram divididos cronologicamente (sem embaralhamento) para preservar a depend\u00eancia temporal t\u00edpica de s\u00e9ries financeiras:</p> <ul> <li>Treino: 70% (6.270 amostras)  </li> <li>Valida\u00e7\u00e3o: 15% (1.344 amostras)  </li> <li>Teste: 15% (1.344 amostras)</li> </ul>"},{"location":"projeto2/main/#33-tratamento-de-outliers","title":"3.3 Tratamento de outliers","text":"<p>An\u00e1lise explorat\u00f3ria revelou assimetria e caudas longas em vari\u00e1veis como <code>volume</code>, <code>vol_20</code> e <code>hl_range</code>. Esses picos extremos foram suavizados por winsoriza\u00e7\u00e3o (1\u00ba e 99\u00ba percentil), limitando os valores a esse intervalo sem remov\u00ea-los.</p> <p>Racional: - Preserva a distribui\u00e7\u00e3o geral; - Evita que valores an\u00f4malos distor\u00e7am o gradiente durante o treinamento; - Evita perda de informa\u00e7\u00e3o (ao contr\u00e1rio de remo\u00e7\u00e3o de linhas).</p>"},{"location":"projeto2/main/#34-normalizacao-das-features","title":"3.4 Normaliza\u00e7\u00e3o das features","text":"<p>Como o MLP utiliza fun\u00e7\u00f5es de ativa\u00e7\u00e3o sens\u00edveis \u00e0 escala (ReLU), aplicou-se StandardScaler \u2014 centrando os dados com m\u00e9dia 0 e desvio padr\u00e3o 1. O ajuste (<code>fit</code>) foi realizado somente sobre o conjunto de treino, garantindo independ\u00eancia entre parti\u00e7\u00f5es.</p> Tipo de transforma\u00e7\u00e3o Descri\u00e7\u00e3o Escopo StandardScaler $(x - \\mu) / \\sigma$ Fit no treino; transform em val/test Winsoriza\u00e7\u00e3o (1\u201399%) Clipping por quantis Mesmos limites replicados no val/test"},{"location":"projeto2/main/#35-efeitos-da-normalizacao","title":"3.5 Efeitos da normaliza\u00e7\u00e3o","text":"<p>Abaixo, observa-se o efeito da normaliza\u00e7\u00e3o sobre a vari\u00e1vel <code>volume</code> \u2014 originalmente medida em bilh\u00f5es, agora reescalonada para m\u00e9dia 0 e desvio padr\u00e3o 1.</p>"},{"location":"projeto2/main/#antes-da-normalizacao","title":"Antes da normaliza\u00e7\u00e3o","text":""},{"location":"projeto2/main/#apos-a-normalizacao-standardscaler","title":"Ap\u00f3s a normaliza\u00e7\u00e3o (StandardScaler)","text":"<p>Observa\u00e7\u00f5es: - A distribui\u00e7\u00e3o manteve sua forma assim\u00e9trica, mas o centro foi deslocado para 0. - Os valores agora est\u00e3o em uma faixa mais adequada \u00e0 converg\u00eancia do otimizador (tipicamente entre -3 e +3). - Esse reescalonamento evita que vari\u00e1veis com unidades grandes (como volume) dominem o c\u00e1lculo do gradiente.</p>"},{"location":"projeto2/main/#36-resultados-intermediarios","title":"3.6 Resultados intermedi\u00e1rios","text":"<p>Todos os conjuntos e relat\u00f3rios foram salvos em <code>preproc_outputs/</code>:</p> Arquivo Descri\u00e7\u00e3o <code>X_train.csv</code>, <code>y_train.csv</code> Conjunto de treino (70%) <code>X_val.csv</code>, <code>y_val.csv</code> Conjunto de valida\u00e7\u00e3o (15%) <code>X_test.csv</code>, <code>y_test.csv</code> Conjunto de teste (15%) <code>before_after_summary.csv</code> Estat\u00edsticas comparativas (raw vs scaled) <code>missing_report_after_cleaning.csv</code> Diagn\u00f3stico de faltantes <code>hist_volume_before.png</code> / <code>hist_volume_after.png</code> Histogramas de distribui\u00e7\u00e3o"},{"location":"projeto2/main/#37-conclusoes","title":"3.7 Conclus\u00f5es","text":"<ul> <li>Sem vazamento de dados: todas as transforma\u00e7\u00f5es foram ajustadas apenas no conjunto de treino.  </li> <li>Dados prontos para o MLP: as features agora s\u00e3o homog\u00eaneas em escala, centradas e limitadas em amplitude.  </li> <li>Ganhos esperados: converg\u00eancia mais est\u00e1vel e r\u00e1pida, e gradientes num\u00e9ricos mais equilibrados.  </li> </ul> <p>Essa prepara\u00e7\u00e3o garante que o modelo na pr\u00f3xima etapa (Parte 4 \u2014 Model Architecture and Training) receba entradas bem condicionadas e compar\u00e1veis entre si.</p>"},{"location":"projeto2/main/#4-mlp-implementation-numpy","title":"4. MLP Implementation (NumPy)","text":"<p>Objetivo. Implementar um Multi-Layer Perceptron (MLP) do zero, utilizando apenas NumPy para opera\u00e7\u00f5es matriciais, fun\u00e7\u00f5es de ativa\u00e7\u00e3o e c\u00e1lculo dos gradientes. O modelo foi desenvolvido e treinado sobre o conjunto de dados do \u00edndice S&amp;P 500, com o objetivo de prever o pre\u00e7o ajustado de fechamento do dia seguinte (next-day adjusted close).</p>"},{"location":"projeto2/main/#41-arquitetura-e-implementacao","title":"4.1 Arquitetura e implementa\u00e7\u00e3o","text":"<p>O modelo foi inteiramente codificado em Python puro, sem uso de bibliotecas de deep learning. A arquitetura segue a estrutura cl\u00e1ssica de um feedforward network:</p> Camada Tipo N\u00ba de unidades Fun\u00e7\u00e3o de ativa\u00e7\u00e3o Observa\u00e7\u00f5es 1 Input 14 features \u2014 Dados normalizados (<code>StandardScaler</code>) 2 Hidden 64 ReLU Inicializa\u00e7\u00e3o He, dropout 10% 3 Hidden 32 ReLU Regulariza\u00e7\u00e3o L2 (<code>\u03bb = 1e-4</code>) 4 Output 1 Linear Sa\u00edda escalar cont\u00ednua (pre\u00e7o futuro) <p>Fun\u00e7\u00f5es implementadas: - <code>relu</code>, <code>tanh</code>, <code>sigmoid</code> e suas derivadas. - <code>init_weights</code> com inicializa\u00e7\u00e3o He para ReLU e Xavier para camadas lineares. - Mini-batch SGD com regulariza\u00e7\u00e3o L2 e dropout invertido. - Early stopping baseado em RMSE de valida\u00e7\u00e3o com <code>patience = 25</code>. - M\u00e9tricas de regress\u00e3o (<code>RMSE</code>, <code>MAE</code>, <code>R\u00b2</code>).</p> <p>Hiperpar\u00e2metros principais:</p> Par\u00e2metro Valor Justificativa Learning rate <code>5e-4</code> Est\u00e1vel para dados padronizados Batch size <code>128</code> Bom equil\u00edbrio entre estabilidade e generaliza\u00e7\u00e3o Epochs m\u00e1ximos <code>300</code> Permite converg\u00eancia sem overfitting L2 regularization <code>1e-4</code> Penaliza pesos grandes, suaviza overfitting Dropout <code>0.10</code> Reduz coadapta\u00e7\u00e3o das camadas intermedi\u00e1rias Patience (early stopping) <code>25</code> Evita overfitting e desperd\u00edcio de treino Otimizador Mini-batch SGD Simples e eficiente em NumPy Ativa\u00e7\u00e3o ReLU Melhor estabilidade para m\u00faltiplas camadas"},{"location":"projeto2/main/#42-treinamento-e-validacao","title":"4.2 Treinamento e valida\u00e7\u00e3o","text":"<p>O modelo foi treinado no alvo padronizado (<code>z-score</code>) e reescalonado para a unidade original (pre\u00e7o) ao calcular as m\u00e9tricas. A tabela abaixo resume os resultados obtidos:</p> Conjunto RMSE MAE R\u00b2 Treino 63.85 47.07 0.9775 Valida\u00e7\u00e3o 60.27 40.34 0.9748 Teste 79.20 57.77 0.9919 <p>Tempo de treinamento: ~ poucos segundos em CPU. Early stopping: atingido na epoch 68, com melhor <code>val_RMSE = 0.1417</code> no espa\u00e7o normalizado.</p>"},{"location":"projeto2/main/#43-curvas-de-aprendizado","title":"4.3 Curvas de aprendizado","text":"<p>O gr\u00e1fico abaixo mostra a evolu\u00e7\u00e3o do erro quadr\u00e1tico m\u00e9dio (RMSE) em treino e valida\u00e7\u00e3o ao longo das \u00e9pocas.</p> <p></p> <p>An\u00e1lise: - R\u00e1pida redu\u00e7\u00e3o do erro at\u00e9 ~30 \u00e9pocas, seguida de estabiliza\u00e7\u00e3o. - Pequena diverg\u00eancia entre treino e valida\u00e7\u00e3o ap\u00f3s 50 \u00e9pocas, indicando o in\u00edcio de overfitting. - Early stopping atuou corretamente, interrompendo o treino quando o erro de valida\u00e7\u00e3o parou de melhorar.</p>"},{"location":"projeto2/main/#44-dispersao-das-previsoes","title":"4.4 Dispers\u00e3o das previs\u00f5es","text":"<p>A seguir, a rela\u00e7\u00e3o entre valores reais e previstos no conjunto de teste:</p> <p></p> <p>Interpreta\u00e7\u00e3o: Os pontos se distribuem pr\u00f3ximos da diagonal, mostrando boa capacidade do modelo em prever o movimento do pre\u00e7o ajustado di\u00e1rio. Pequenas dispers\u00f5es ocorrem em valores extremos de mercado, onde a volatilidade \u00e9 maior.</p>"},{"location":"projeto2/main/#45-conclusoes-parciais","title":"4.5 Conclus\u00f5es parciais","text":"<ul> <li>O MLP implementado em NumPy apresentou excelente desempenho com <code>R\u00b2 &gt; 0.97</code> em todos os conjuntos.  </li> <li>A normaliza\u00e7\u00e3o do alvo e das features foi crucial para evitar explos\u00e3o de gradientes.  </li> <li>O uso de regulariza\u00e7\u00e3o L2 e dropout leve (10%) garantiu boa generaliza\u00e7\u00e3o.  </li> <li>O modelo converge de forma est\u00e1vel e explica mais de 97% da vari\u00e2ncia do pre\u00e7o ajustado do S&amp;P 500 di\u00e1rio.</li> </ul>"},{"location":"projeto2/main/#46-proximos-passos","title":"4.6 Pr\u00f3ximos passos","text":"<ul> <li>Aumentar o n\u00famero de camadas (ex.: 128\u201364\u201332) para testar depth sensitivity.  </li> <li>Experimentar otimizadores mais avan\u00e7ados (Adam ou RMSProp).  </li> <li>Avaliar janelas de entrada maiores (ex.: incluir retornos de 5 dias) para incorporar mem\u00f3ria de curto prazo.  </li> <li>Comparar com modelos lineares e redes recorrentes (RNN/LSTM) em trabalhos futuros.</li> </ul>"},{"location":"projeto2/main/#5-conclusion","title":"5. Conclusion","text":""},{"location":"projeto2/main/#51-resumo-geral","title":"5.1 Resumo geral.","text":"<p>O projeto cumpriu todas as etapas do pipeline de regress\u00e3o com redes neurais, desde a coleta e engenharia de dados at\u00e9 a implementa\u00e7\u00e3o e avalia\u00e7\u00e3o completa de um Multi-Layer Perceptron (MLP) desenvolvido integralmente em NumPy. O modelo foi aplicado a uma tarefa realista \u2014 previs\u00e3o do pre\u00e7o ajustado do \u00edndice S&amp;P 500 para o dia seguinte \u2014, demonstrando alto desempenho e estabilidade.</p>"},{"location":"projeto2/main/#52-codigo-completo","title":"5.2 C\u00f3digo completo","text":"<p>``` pyodide install=\"numpy, matplotlib\" import os import numpy as np import pandas as pd import matplotlib.pyplot as plt from scipy.stats import zscore import json import time from sklearn.preprocessing import StandardScaler import warnings import yfinance as yf</p>"},{"location":"projeto2/main/#_1","title":"======================","text":""},{"location":"projeto2/main/#1-download-dos-dados-ajustes-importantes","title":"1) Download dos dados (ajustes importantes)","text":""},{"location":"projeto2/main/#_2","title":"======================","text":""},{"location":"projeto2/main/#-auto_adjustfalse-para-manter-adj-close","title":"- auto_adjust=False para manter \"Adj Close\"","text":""},{"location":"projeto2/main/#-group_bycolumn-para-evitar-multiindex-nas-colunas","title":"- group_by='column' para evitar MultiIndex nas colunas","text":""},{"location":"projeto2/main/#-actionsfalse-porque-nao-precisamos-de-dividendossplits-neste-passo","title":"- actions=False porque n\u00e3o precisamos de dividendos/splits neste passo","text":"<p>df = yf.download(     \"^GSPC\",     start=\"1990-01-01\",     progress=False,     auto_adjust=False,     group_by=\"column\",     actions=False, )</p> <p>if df is None or df.empty:     raise SystemExit(         \"Sem dados baixados. Verifique conex\u00e3o/ambiente ou ajuste o intervalo de datas.\"     )</p>"},{"location":"projeto2/main/#garante-colunas-simples-nao-multiindex-por-seguranca-extra","title":"Garante colunas simples (n\u00e3o-MultiIndex), por seguran\u00e7a extra","text":"<p>if isinstance(df.columns, pd.MultiIndex):     df.columns = df.columns.get_level_values(0)</p>"},{"location":"projeto2/main/#renomeia-o-que-existir","title":"Renomeia o que existir","text":"<p>rename_map = {     \"Open\": \"open\",     \"High\": \"high\",     \"Low\": \"low\",     \"Close\": \"close\",     \"Adj Close\": \"adj_close\",     \"Volume\": \"volume\", } existing_map = {k: v for k, v in rename_map.items() if k in df.columns} df = df.rename(columns=existing_map) df.index.name = \"date\"</p>"},{"location":"projeto2/main/#se-por-algum-motivo-adj_close-nao-existir-use-close","title":"Se por algum motivo \"adj_close\" n\u00e3o existir, use \"close\"","text":"<p>if \"adj_close\" not in df.columns and \"close\" in df.columns:     df[\"adj_close\"] = df[\"close\"]</p>"},{"location":"projeto2/main/#_3","title":"======================","text":""},{"location":"projeto2/main/#2-garantir-tipos-numericos-apenas-no-que-existir","title":"2) Garantir tipos num\u00e9ricos (apenas no que existir)","text":""},{"location":"projeto2/main/#_4","title":"======================","text":"<p>numeric_candidates = [\"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"] numeric_cols = [c for c in numeric_candidates if c in df.columns] for c in numeric_cols:     # to_numeric espera Series/array; df[c] \u00e9 Series aqui     df[c] = pd.to_numeric(df[c], errors=\"coerce\")</p>"},{"location":"projeto2/main/#_5","title":"======================","text":""},{"location":"projeto2/main/#3-engenharia-de-features","title":"3) Engenharia de features","text":""},{"location":"projeto2/main/#_6","title":"======================","text":""},{"location":"projeto2/main/#retornos-t-simples-e-log","title":"Retornos (t): simples e log","text":"<p>df[\"ret_1d\"] = df[\"adj_close\"].pct_change() df[\"logret_1d\"] = np.log(df[\"adj_close\"]).diff()</p>"},{"location":"projeto2/main/#lags-t-1-de-preco-ajustado-e-volume","title":"Lags (t-1) de pre\u00e7o ajustado e volume","text":"<p>if \"adj_close\" in df.columns:     df[\"adj_close_lag1\"] = df[\"adj_close\"].shift(1) if \"volume\" in df.columns:     df[\"volume_lag1\"] = df[\"volume\"].shift(1)</p>"},{"location":"projeto2/main/#medias-moveis-e-volatilidade-janelas","title":"M\u00e9dias m\u00f3veis e volatilidade (janelas)","text":"<p>for w in (5, 20, 60):     df[f\"ma_{w}\"] = df[\"adj_close\"].rolling(window=w, min_periods=w).mean() df[\"vol_20\"] = df[\"logret_1d\"].rolling(window=20, min_periods=20).std()</p>"},{"location":"projeto2/main/#indicadores-de-amplitude-e-posicao-do-fechamento","title":"Indicadores de amplitude e posi\u00e7\u00e3o do fechamento","text":""},{"location":"projeto2/main/#usa-denominador-com-protecao-para-zero","title":"Usa denominador com prote\u00e7\u00e3o para zero","text":"<p>rng = df[\"high\"] - df[\"low\"] df[\"hl_range\"] = (df[\"high\"] - df[\"low\"]) / df[\"low\"].where(df[\"low\"] != 0, np.nan) df[\"close_pos_range\"] = (df[\"close\"] - df[\"low\"]) / rng.where(rng != 0, np.nan)</p>"},{"location":"projeto2/main/#remove-linhas-incompletas-introduzidas-por-rollingshift","title":"Remove linhas incompletas introduzidas por rolling/shift","text":"<p>df = df.dropna().copy()</p>"},{"location":"projeto2/main/#_7","title":"======================","text":""},{"location":"projeto2/main/#4-target-e-features","title":"4) Target e features","text":""},{"location":"projeto2/main/#_8","title":"======================","text":"<p>df[\"target_adj_close_tplus1\"] = df[\"adj_close\"].shift(-1) df = df.dropna(subset=[\"target_adj_close_tplus1\"]).copy()</p> <p>feature_cols = [     c     for c in [         \"open\",         \"high\",         \"low\",         \"close\",         \"adj_close\",         \"volume\",         \"ret_1d\",         \"logret_1d\",         \"adj_close_lag1\",         \"volume_lag1\",         \"ma_5\",         \"ma_20\",         \"ma_60\",         \"vol_20\",         \"hl_range\",         \"close_pos_range\",     ]     if c in df.columns ]</p> <p>X = df[feature_cols].copy() y = df[\"target_adj_close_tplus1\"].copy()</p>"},{"location":"projeto2/main/#_9","title":"======================","text":""},{"location":"projeto2/main/#5-diagnosticos-faltantes-e-outliers","title":"5) Diagn\u00f3sticos: faltantes e outliers","text":""},{"location":"projeto2/main/#_10","title":"======================","text":"<p>missing = X.isna().sum().sort_values(ascending=False) missing_pct = (X.isna().mean() * 100).round(3)</p>"},{"location":"projeto2/main/#outliers-por-z-score-z4-diagnostico","title":"Outliers por z-score |z|&gt;4 (diagn\u00f3stico)","text":"<p>zscores = X.apply(zscore, nan_policy=\"omit\") outlier_mask = np.abs(zscores) &gt; 4 outliers_por_col = outlier_mask.sum().sort_values(ascending=False)</p>"},{"location":"projeto2/main/#_11","title":"======================","text":""},{"location":"projeto2/main/#6-estatisticas-e-salvamento","title":"6) Estat\u00edsticas e salvamento","text":""},{"location":"projeto2/main/#_12","title":"======================","text":"<p>desc_X = X.describe().T desc_y = y.describe()</p> <p>os.makedirs(\"eda_outputs\", exist_ok=True) desc_X.to_csv(\"eda_outputs/summary_stats_features.csv\") pd.DataFrame({\"missing_count\": missing, \"missing_pct\": missing_pct}).to_csv(     \"eda_outputs/missing_values_report.csv\" ) outliers_por_col.to_csv(\"eda_outputs/outliers_by_feature_z4.csv\") df.to_csv(\"eda_outputs/sp500_features_target.csv\")</p>"},{"location":"projeto2/main/#_13","title":"======================","text":""},{"location":"projeto2/main/#7-graficos-matplotlib-sem-seaborn","title":"7) Gr\u00e1ficos (Matplotlib \u2014 sem seaborn)","text":""},{"location":"projeto2/main/#_14","title":"======================","text":"<p>plt.figure(figsize=(10, 4)) plt.plot(df.index, df[\"adj_close\"]) plt.title(\"S&amp;P 500 - Adj Close (Hist\u00f3rico)\") plt.xlabel(\"Data\") plt.ylabel(\"Adj Close\") plt.tight_layout() plt.savefig(\"eda_outputs/timeseries_adj_close.png\", dpi=150) plt.close()</p> <p>def save_hist(series, fname, bins=50):     plt.figure(figsize=(6, 4))     plt.hist(series.dropna().values, bins=bins)     plt.title(f\"Histograma - {series.name}\")     plt.xlabel(series.name)     plt.ylabel(\"Frequ\u00eancia\")     plt.tight_layout()     plt.savefig(fname, dpi=150)     plt.close()</p> <p>for col in [     c     for c in [\"ret_1d\", \"logret_1d\", \"volume\", \"vol_20\", \"hl_range\", \"close_pos_range\"]     if c in X.columns ]:     save_hist(X[col], f\"eda_outputs/hist_{col}.png\")</p>"},{"location":"projeto2/main/#matriz-de-correlacao_1","title":"Matriz de correla\u00e7\u00e3o","text":"<p>corr = X.corr(numeric_only=True) plt.figure(figsize=(9, 8)) im = plt.imshow(corr.values, aspect=\"auto\", interpolation=\"nearest\") plt.colorbar(im, fraction=0.046, pad=0.04) plt.xticks(ticks=np.arange(len(corr.columns)), labels=corr.columns, rotation=90) plt.yticks(ticks=np.arange(len(corr.index)), labels=corr.index) plt.title(\"Matriz de Correla\u00e7\u00e3o \u2014 Features (num\u00e9ricas)\") plt.tight_layout() plt.savefig(\"eda_outputs/corr_matrix_features.png\", dpi=150) plt.close()</p> <p>def save_scatter(x, y, fname, xlabel, ylabel):     plt.figure(figsize=(6, 4))     plt.scatter(x, y, s=3, alpha=0.5)     plt.xlabel(xlabel)     plt.ylabel(ylabel)     plt.title(f\"{ylabel} vs. {xlabel}\")     plt.tight_layout()     plt.savefig(fname, dpi=150)     plt.close()</p> <p>if \"adj_close\" in X.columns:     save_scatter(         X[\"adj_close\"],         y,         \"eda_outputs/scatter_target_vs_adj_close.png\",         \"adj_close (t)\",         \"target_adj_close (t+1)\",     ) if \"vol_20\" in X.columns:     save_scatter(         X[\"vol_20\"],         y,         \"eda_outputs/scatter_target_vs_vol_20.png\",         \"vol_20 (t)\",         \"target_adj_close (t+1)\",     )</p> <p>print(\"\u2705 EDA conclu\u00edda. Sa\u00eddas salvas na pasta: eda_outputs/\") for f in sorted(os.listdir(\"eda_outputs\")):     print(\"-\", f)</p>"},{"location":"projeto2/main/#_15","title":"================================================================","text":""},{"location":"projeto2/main/#parte-3-data-cleaning-and-normalization","title":"PARTE 3 \u2014 Data Cleaning and Normalization","text":""},{"location":"projeto2/main/#_16","title":"================================================================","text":"<p>print(\"\\n=== PARTE 3 \u2014 Data Cleaning and Normalization ===\")</p> <p>warnings.filterwarnings(\"ignore\")</p>"},{"location":"projeto2/main/#-","title":"-------------------------","text":""},{"location":"projeto2/main/#1-checar-duplicatas-e-faltantes","title":"1. Checar duplicatas e faltantes","text":""},{"location":"projeto2/main/#-_1","title":"-------------------------","text":"<p>print(\"\\nChecando duplicatas e faltantes...\")</p> <p>df = df[~df.index.duplicated(keep=\"first\")].copy() missing_report = df.isna().sum().sort_values(ascending=False) print(\"Valores faltantes ap\u00f3s EDA:\\n\", missing_report.head())</p> <p>missing_report.to_csv(\"eda_outputs/missing_report_after_cleaning.csv\")</p>"},{"location":"projeto2/main/#-_2","title":"-------------------------","text":""},{"location":"projeto2/main/#2-split-temporal-701515","title":"2. Split temporal: 70/15/15","text":""},{"location":"projeto2/main/#-_3","title":"-------------------------","text":"<p>print(\"\\nRealizando split temporal...\")</p> <p>dates = df.index.sort_values() n = len(dates) train_end = dates[int(0.70 * n)] val_end = dates[int(0.85 * n)]</p> <p>train_df = df.loc[:train_end].copy() val_df = df.loc[train_end:].loc[:val_end].copy() test_df = df.loc[val_end:].copy()</p> <p>print(f\"Tamanhos \u2192 train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}\")</p>"},{"location":"projeto2/main/#-_4","title":"-------------------------","text":""},{"location":"projeto2/main/#3-winsorizacao-limite-1-e-99-para-suavizar-outliers","title":"3. Winsoriza\u00e7\u00e3o (limite 1% e 99%) para suavizar outliers","text":""},{"location":"projeto2/main/#-_5","title":"-------------------------","text":"<p>print(\"\\nAplicando winsoriza\u00e7\u00e3o (1\u201399%) em colunas assim\u00e9tricas...\")</p> <p>def winsorize(df_in, cols, lower=0.01, upper=0.99, ref=None):     df = df_in.copy()     ref = ref if ref is not None else df     for c in cols:         lo, hi = ref[c].quantile([lower, upper])         df[c] = df[c].clip(lower=lo, upper=hi)     return df</p> <p>winsor_cols = [c for c in [\"volume\", \"vol_20\", \"hl_range\"] if c in df.columns]</p> <p>train_df = winsorize(train_df, winsor_cols) val_df = winsorize(val_df, winsor_cols, ref=train_df) test_df = winsorize(test_df, winsor_cols, ref=train_df)</p>"},{"location":"projeto2/main/#-_6","title":"-------------------------","text":""},{"location":"projeto2/main/#4-normalizacao-standardscaler-media-0-std-1","title":"4. Normaliza\u00e7\u00e3o (StandardScaler \u2014 m\u00e9dia 0, std 1)","text":""},{"location":"projeto2/main/#-_7","title":"-------------------------","text":"<p>print(\"\\nNormalizando features num\u00e9ricas (StandardScaler)...\")</p> <p>feature_cols = [     c     for c in [         \"open\",         \"high\",         \"low\",         \"close\",         \"adj_close\",         \"volume\",         \"ret_1d\",         \"logret_1d\",         \"adj_close_lag1\",         \"volume_lag1\",         \"ma_5\",         \"ma_20\",         \"ma_60\",         \"vol_20\",         \"hl_range\",         \"close_pos_range\",     ]     if c in df.columns ]</p> <p>target_col = \"target_adj_close_tplus1\"</p> <p>X_train, y_train = train_df[feature_cols], train_df[target_col] X_val, y_val = val_df[feature_cols], val_df[target_col] X_test, y_test = test_df[feature_cols], test_df[target_col]</p> <p>scaler = StandardScaler() scaler.fit(X_train)</p> <p>X_train_scaled = pd.DataFrame(     scaler.transform(X_train), index=X_train.index, columns=X_train.columns ) X_val_scaled = pd.DataFrame(     scaler.transform(X_val), index=X_val.index, columns=X_val.columns ) X_test_scaled = pd.DataFrame(     scaler.transform(X_test), index=X_test.index, columns=X_test.columns )</p>"},{"location":"projeto2/main/#-_8","title":"-------------------------","text":""},{"location":"projeto2/main/#5-salvar-conjuntos-processados","title":"5. Salvar conjuntos processados","text":""},{"location":"projeto2/main/#-_9","title":"-------------------------","text":"<p>os.makedirs(\"preproc_outputs\", exist_ok=True) X_train_scaled.to_csv(\"preproc_outputs/X_train.csv\") y_train.to_csv(\"preproc_outputs/y_train.csv\") X_val_scaled.to_csv(\"preproc_outputs/X_val.csv\") y_val.to_csv(\"preproc_outputs/y_val.csv\") X_test_scaled.to_csv(\"preproc_outputs/X_test.csv\") y_test.to_csv(\"preproc_outputs/y_test.csv\")</p> <p>print(\"\\nArquivos escalonados salvos em 'preproc_outputs/'\")</p>"},{"location":"projeto2/main/#-_10","title":"-------------------------","text":""},{"location":"projeto2/main/#6-comparacao-beforeafter","title":"6. Compara\u00e7\u00e3o Before/After","text":""},{"location":"projeto2/main/#-_11","title":"-------------------------","text":"<p>before_after = pd.concat(     [         X_train[winsor_cols].describe().T.add_prefix(\"raw_\"),         pd.DataFrame(X_train_scaled[winsor_cols]).describe().T.add_prefix(\"scaled_\"),     ],     axis=1, ) before_after.to_csv(\"preproc_outputs/before_after_summary.csv\")</p>"},{"location":"projeto2/main/#-_12","title":"-------------------------","text":""},{"location":"projeto2/main/#7-visualizacoes-beforeafter","title":"7. Visualiza\u00e7\u00f5es Before/After","text":""},{"location":"projeto2/main/#-_13","title":"-------------------------","text":"<p>plt.figure(figsize=(6, 4)) plt.hist(X_train[\"volume\"], bins=60) plt.title(\"Volume - Antes da Normaliza\u00e7\u00e3o\") plt.tight_layout() plt.savefig(\"preproc_outputs/hist_volume_before.png\") plt.close()</p> <p>plt.figure(figsize=(6, 4)) plt.hist(X_train_scaled[\"volume\"], bins=60) plt.title(\"Volume - Ap\u00f3s Normaliza\u00e7\u00e3o (StandardScaler)\") plt.tight_layout() plt.savefig(\"preproc_outputs/hist_volume_after.png\") plt.close()</p> <p>print(\"\u2705 Parte 3 finalizada com sucesso \u2014 dados limpos, winsorizados e normalizados.\")</p> <p>print(\"\\n=== PARTE 4 \u2014 MLP Implementation (NumPy) ===\")</p>"},{"location":"projeto2/main/#-_14","title":"-------------------------","text":""},{"location":"projeto2/main/#0-carregar-dados-pre-processados-parte-3","title":"0) Carregar dados pr\u00e9-processados (Parte 3)","text":""},{"location":"projeto2/main/#-_15","title":"-------------------------","text":"<p>def _load_xy(prefix_dir=\"preproc_outputs\"):     X_train = pd.read_csv(f\"{prefix_dir}/X_train.csv\", index_col=0)     y_train = pd.read_csv(f\"{prefix_dir}/y_train.csv\", index_col=0).values.reshape(         -1, 1     )     X_val = pd.read_csv(f\"{prefix_dir}/X_val.csv\", index_col=0)     y_val = pd.read_csv(f\"{prefix_dir}/y_val.csv\", index_col=0).values.reshape(-1, 1)     X_test = pd.read_csv(f\"{prefix_dir}/X_test.csv\", index_col=0)     y_test = pd.read_csv(f\"{prefix_dir}/y_test.csv\", index_col=0).values.reshape(-1, 1)     return (X_train, y_train, X_val, y_val, X_test, y_test)</p> <p>Xtr_df, y_train, Xva_df, y_val, Xte_df, y_test = _load_xy()</p>"},{"location":"projeto2/main/#-_16","title":"-------------------------","text":""},{"location":"projeto2/main/#1-sanity-checks-e-saneamento-evita-naninfcolunas-com-std0","title":"1) Sanity checks e saneamento (evita NaN/Inf/colunas com std=0)","text":""},{"location":"projeto2/main/#-_17","title":"-------------------------","text":""},{"location":"projeto2/main/#remover-colunas-com-desvio-0-podem-causar-nan-no-scaler-anterior","title":"remover colunas com desvio 0 (podem causar NaN no scaler anterior)","text":"<p>zero_std_cols = Xtr_df.columns[     (Xtr_df.std(axis=0, ddof=0) == 0) | (~np.isfinite(Xtr_df.std(axis=0, ddof=0))) ] if len(zero_std_cols) &gt; 0:     print(\"\u26a0\ufe0f Removendo colunas com std=0:\", list(zero_std_cols))     Xtr_df = Xtr_df.drop(columns=zero_std_cols)     Xva_df = Xva_df.drop(columns=zero_std_cols, errors=\"ignore\")     Xte_df = Xte_df.drop(columns=zero_std_cols, errors=\"ignore\")</p> <p>def sanitize(df, name):     n_nan = np.isnan(df.values).sum()     n_inf = np.isinf(df.values).sum()     if n_nan or n_inf:         print(f\"\u26a0\ufe0f {name}: substituindo {n_nan} NaN e {n_inf} Inf por 0.0\")         df = df.replace([np.inf, -np.inf], np.nan).fillna(0.0)     return df</p> <p>Xtr_df = sanitize(Xtr_df, \"X_train\") Xva_df = sanitize(Xva_df, \"X_val\") Xte_df = sanitize(Xte_df, \"X_test\")</p> <p>X_train = Xtr_df.values X_val = Xva_df.values X_test = Xte_df.values</p> <p>assert (     np.isfinite(X_train).all()     and np.isfinite(X_val).all()     and np.isfinite(X_test).all() ), \"Ainda h\u00e1 valores n\u00e3o finitos nas features!\"</p>"},{"location":"projeto2/main/#padronizar-o-alvo-z-score-para-estabilizar-gradientes","title":"Padronizar o alvo (z-score) para estabilizar gradientes","text":"<p>y_mean, y_std = float(np.mean(y_train)), float(np.std(y_train)) if y_std == 0 or not np.isfinite(y_std):     y_std = 1.0 y_train_z = (y_train - y_mean) / y_std y_val_z = (y_val - y_mean) / y_std y_test_z = (y_test - y_mean) / y_std</p>"},{"location":"projeto2/main/#-_18","title":"-------------------------","text":""},{"location":"projeto2/main/#2-metricas-de-regressao","title":"2) M\u00e9tricas de regress\u00e3o","text":""},{"location":"projeto2/main/#-_19","title":"-------------------------","text":"<p>def mse(y_true, y_pred):     return float(np.mean((y_true - y_pred) ** 2))</p> <p>def rmse(y_true, y_pred):     return float(np.sqrt(mse(y_true, y_pred)))</p> <p>def mae(y_true, y_pred):     return float(np.mean(np.abs(y_true - y_pred)))</p> <p>def r2_score(y_true, y_pred):     ss_res = np.sum((y_true - y_pred) ** 2)     ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)     return float(1 - ss_res / ss_tot)</p>"},{"location":"projeto2/main/#-_20","title":"-------------------------","text":""},{"location":"projeto2/main/#3-ativacoes-e-derivadas","title":"3) Ativa\u00e7\u00f5es e derivadas","text":""},{"location":"projeto2/main/#-_21","title":"-------------------------","text":"<p>def relu(z):     return np.maximum(0, z)</p> <p>def drelu(z):     return (z &gt; 0).astype(z.dtype)</p> <p>def sigmoid(z):     return 1.0 / (1.0 + np.exp(-z))</p> <p>def dsigmoid(z):     s = sigmoid(z)     return s * (1 - s)</p> <p>def tanh(z):     return np.tanh(z)</p> <p>def dtanh(z):     return 1 - np.tanh(z) ** 2</p> <p>ACTS = {\"relu\": (relu, drelu), \"tanh\": (tanh, dtanh), \"sigmoid\": (sigmoid, dsigmoid)}</p>"},{"location":"projeto2/main/#-_22","title":"-------------------------","text":""},{"location":"projeto2/main/#4-inicializacao-he-para-relu-xavier-aproximado-para-demais","title":"4) Inicializa\u00e7\u00e3o (He para ReLU; Xavier aproximado para demais)","text":""},{"location":"projeto2/main/#compativel-com-default_rng-standard_normal","title":"compat\u00edvel com default_rng (standard_normal)","text":""},{"location":"projeto2/main/#-_23","title":"-------------------------","text":"<p>def init_weights(in_dim, out_dim, act_name, rng):     if act_name == \"relu\":         std = np.sqrt(2.0 / in_dim)     else:         std = np.sqrt(1.0 / in_dim)     if hasattr(rng, \"standard_normal\"):         W = rng.standard_normal((in_dim, out_dim)) * std     else:         W = rng.randn(in_dim, out_dim) * std     b = np.zeros((1, out_dim))     return W, b</p>"},{"location":"projeto2/main/#-_24","title":"-------------------------","text":""},{"location":"projeto2/main/#5-mlp-from-scratch-mini-batch-sgd-l2-dropout-early-stopping","title":"5) MLP from scratch (mini-batch SGD + L2 + dropout + early stopping)","text":""},{"location":"projeto2/main/#-_25","title":"-------------------------","text":"<p>class MLPRegressorScratch:     def init(         self,         input_dim,         hidden_dims=(64, 32),         activation=\"relu\",         lr=5e-4,         batch_size=128,         epochs=300,         l2=1e-4,         dropout=0.0,         seed=42,         early_stopping=True,         patience=25,         verbose=True,     ):         self.input_dim = input_dim         self.hidden_dims = list(hidden_dims)         self.activation = activation         self.lr = lr         self.batch_size = batch_size         self.epochs = epochs         self.l2 = l2         self.dropout = dropout         self.seed = seed         self.early_stopping = early_stopping         self.patience = patience         self.verbose = verbose</p> <pre><code>    assert activation in ACTS, f\"Ativa\u00e7\u00e3o inv\u00e1lida: {activation}\"\n    self.act, self.dact = ACTS[activation]\n    self.rng = np.random.default_rng(seed)\n    self._init_params()\n\ndef _init_params(self):\n    dims = [self.input_dim] + self.hidden_dims + [1]  # sa\u00edda escalar\n    self.W, self.b = [], []\n    # ocultas\n    for i in range(len(dims) - 2):\n        Wi, bi = init_weights(dims[i], dims[i + 1], self.activation, self.rng)\n        self.W.append(Wi)\n        self.b.append(bi)\n    # sa\u00edda linear (Xavier ok)\n    Wi, bi = init_weights(dims[-2], dims[-1], \"tanh\", self.rng)\n    self.W.append(Wi)\n    self.b.append(bi)\n\ndef _forward(self, X, train_mode=False):\n    a = X\n    caches = {\"A0\": a, \"Z\": [], \"A\": [], \"drop_masks\": []}\n    # ocultas\n    for L in range(len(self.hidden_dims)):\n        z = a @ self.W[L] + self.b[L]\n        a = self.act(z)\n        mask = None\n        if train_mode and self.dropout &gt; 0:\n            mask = (self.rng.random(a.shape) &gt; self.dropout).astype(a.dtype)\n            a = a * mask / (1.0 - self.dropout)  # inverted dropout\n        caches[\"Z\"].append(z)\n        caches[\"A\"].append(a)\n        caches[\"drop_masks\"].append(mask)\n    # sa\u00edda linear\n    z_out = a @ self.W[-1] + self.b[-1]\n    y_hat = z_out\n    caches[\"Z_out\"] = z_out\n    caches[\"A_out\"] = y_hat\n    return y_hat, caches\n\ndef _backward(self, y_hat, y_true, caches):\n    N = y_true.shape[0]\n    grad_W = [None] * len(self.W)\n    grad_b = [None] * len(self.b)\n\n    # sa\u00edda\n    dL_dy = 2.0 * (y_hat - y_true) / N\n    a_prev = caches[\"A\"][-1] if len(self.hidden_dims) &gt; 0 else caches[\"A0\"]\n    grad_W[-1] = a_prev.T @ dL_dy + self.l2 * self.W[-1]\n    grad_b[-1] = np.sum(dL_dy, axis=0, keepdims=True)\n\n    da = dL_dy @ self.W[-1].T\n    for L in reversed(range(len(self.hidden_dims))):\n        zL = caches[\"Z\"][L]\n        a_prev = caches[\"A\"][L - 1] if L &gt; 0 else caches[\"A0\"]\n        mask = caches[\"drop_masks\"][L]\n        if mask is not None:\n            da = da * mask / (1.0 - self.dropout)\n        dz = da * self.dact(zL)\n        grad_W[L] = a_prev.T @ dz + self.l2 * self.W[L]\n        grad_b[L] = np.sum(dz, axis=0, keepdims=True)\n        if L &gt; 0:\n            da = dz @ self.W[L].T\n    return grad_W, grad_b\n\ndef _update(self, grad_W, grad_b):\n    for i in range(len(self.W)):\n        self.W[i] -= self.lr * grad_W[i]\n        self.b[i] -= self.lr * grad_b[i]\n\ndef fit(self, X_train, y_train, X_val=None, y_val=None):\n    rng = self.rng\n    N = X_train.shape[0]\n    bsz = self.batch_size\n    self.history_ = {\"train_rmse\": [], \"val_rmse\": []}\n    best_val = np.inf\n    best_state = None\n    patience_left = self.patience\n\n    t0 = time.time()\n    for epoch in range(1, self.epochs + 1):\n        idx = rng.permutation(N)\n        X_train, y_train = X_train[idx], y_train[idx]\n        for s in range(0, N, bsz):\n            e = min(s + bsz, N)\n            xb, yb = X_train[s:e], y_train[s:e]\n            y_hat, cache = self._forward(xb, train_mode=True)\n            gW, gB = self._backward(y_hat, yb, cache)\n            self._update(gW, gB)\n\n        # m\u00e9tricas por \u00e9poca (no espa\u00e7o z)\n        tr_pred, _ = self._forward(X_train, train_mode=False)\n        tr_rmse = rmse(y_train, tr_pred)\n        self.history_[\"train_rmse\"].append(tr_rmse)\n\n        if X_val is not None:\n            v_pred, _ = self._forward(X_val, train_mode=False)\n            v_rmse = rmse(y_val, v_pred)\n            self.history_[\"val_rmse\"].append(v_rmse)\n        else:\n            v_rmse = np.nan\n\n        if self.verbose and (epoch == 1 or epoch % 10 == 0):\n            print(\n                f\"[Epoch {epoch:4d}] train_RMSE={tr_rmse:.4f}  val_RMSE={v_rmse:.4f}\"\n            )\n\n        if self.early_stopping and X_val is not None:\n            if v_rmse + 1e-9 &lt; best_val:\n                best_val = v_rmse\n                patience_left = self.patience\n                best_state = {\n                    \"W\": [w.copy() for w in self.W],\n                    \"b\": [b.copy() for b in self.b],\n                    \"epoch\": epoch,\n                }\n            else:\n                patience_left -= 1\n                if patience_left &lt;= 0:\n                    if self.verbose:\n                        print(\n                            f\"Early stopping em epoch {epoch} (melhor val_RMSE={best_val:.4f})\"\n                        )\n                    if best_state is not None:\n                        self.W = [w.copy() for w in best_state[\"W\"]]\n                        self.b = [b.copy() for b in best_state[\"b\"]]\n                    break\n\n    self.train_time_ = time.time() - t0\n    return self\n\ndef predict(self, X):\n    y_hat, _ = self._forward(X, train_mode=False)\n    return y_hat\n</code></pre>"},{"location":"projeto2/main/#-_26","title":"-------------------------","text":""},{"location":"projeto2/main/#6-configuracao-treino-e-avaliacao","title":"6) Configura\u00e7\u00e3o, treino e avalia\u00e7\u00e3o","text":""},{"location":"projeto2/main/#-_27","title":"-------------------------","text":"<p>INPUT_DIM = X_train.shape[1] mlp = MLPRegressorScratch(     input_dim=INPUT_DIM,     hidden_dims=(64, 32),  # experimente (128, 64, 32)     activation=\"relu\",  # \"relu\" | \"tanh\" | \"sigmoid\"     lr=5e-4,  # taxa de aprendizado (est\u00e1vel)     batch_size=128,     epochs=300,     l2=1e-4,  # weight decay     dropout=0.10,  # 10% na(s) oculta(s) \u2014 pode come\u00e7ar com 0.0     seed=42,     early_stopping=True,     patience=25,     verbose=True, )</p>"},{"location":"projeto2/main/#treino-no-espaco-z-do-alvo","title":"treino no espa\u00e7o z do alvo","text":"<p>mlp.fit(X_train, y_train_z, X_val, y_val_z)</p>"},{"location":"projeto2/main/#predicoes-no-espaco-z","title":"predi\u00e7\u00f5es no espa\u00e7o z","text":"<p>yhat_tr_z = mlp.predict(X_train) yhat_va_z = mlp.predict(X_val) yhat_te_z = mlp.predict(X_test)</p>"},{"location":"projeto2/main/#retornar-a-escala-original-do-preco","title":"retornar \u00e0 escala original do pre\u00e7o","text":"<p>yhat_tr = yhat_tr_z * y_std + y_mean yhat_va = yhat_va_z * y_std + y_mean yhat_te = yhat_te_z * y_std + y_mean</p> <p>def eval_and_print(tag, y_true, y_pred):     res = {         \"RMSE\": rmse(y_true, y_pred),         \"MAE\": mae(y_true, y_pred),         \"R2\": r2_score(y_true, y_pred),     }     print(f\"{tag}: RMSE={res['RMSE']:.4f}  MAE={res['MAE']:.4f}  R2={res['R2']:.4f}\")     return res</p> <p>os.makedirs(\"model_outputs\", exist_ok=True)</p> <p>metrics = {     \"train\": eval_and_print(\"TRAIN\", y_train, yhat_tr),     \"val\": eval_and_print(\"VAL  \", y_val, yhat_va),     \"test\": eval_and_print(\"TEST \", y_test, yhat_te),     \"train_time_sec\": mlp.train_time_, } with open(\"model_outputs/metrics.json\", \"w\") as f:     json.dump(metrics, f, indent=2)</p>"},{"location":"projeto2/main/#curvas-de-rmse-treinoval-ainda-no-espaco-z","title":"curvas de RMSE (treino/val) \u2014 ainda no espa\u00e7o z","text":"<p>plt.figure(figsize=(7, 4)) plt.plot(mlp.history_.get(\"train_rmse\", []), label=\"Train RMSE (z)\") if len(mlp.history_.get(\"val_rmse\", [])) &gt; 0:     plt.plot(mlp.history_[\"val_rmse\"], label=\"Val RMSE (z)\") plt.xlabel(\"Epoch\") plt.ylabel(\"RMSE\") plt.title(\"Curvas de RMSE \u2014 MLP (NumPy)\") plt.legend() plt.tight_layout() plt.savefig(\"model_outputs/loss_curves_rmse.png\", dpi=150) plt.close()</p>"},{"location":"projeto2/main/#scatter-no-teste-escala-real","title":"scatter no teste (escala real)","text":"<p>plt.figure(figsize=(5, 5)) plt.scatter(y_test, yhat_te, s=6, alpha=0.4) plt.xlabel(\"y_true (test)\") plt.ylabel(\"y_pred (test)\") plt.title(\"Dispers\u00e3o \u2014 Test (y_true vs y_pred)\") plt.tight_layout() plt.savefig(\"model_outputs/scatter_test_true_vs_pred.png\", dpi=150) plt.close()</p> <p>print(\"\u2705 Treino do MLP finalizado. Resultados salvos em 'model_outputs/'.\")</p> <p>```</p>"},{"location":"projeto2/main/#52-principais-resultados","title":"5.2 Principais resultados","text":"<ul> <li>O modelo atingiu R\u00b2 superior a 0.97 em todos os conjuntos (treino, valida\u00e7\u00e3o e teste), mostrando excelente capacidade explicativa.</li> <li>As m\u00e9tricas de erro (RMSE e MAE) permaneceram baixas e pr\u00f3ximas entre treino e valida\u00e7\u00e3o, evidenciando boa generaliza\u00e7\u00e3o.</li> <li>O MLP superou amplamente o baseline de m\u00e9dia hist\u00f3rica, confirmando ganho preditivo real.</li> <li>A curva de aprendizado revelou converg\u00eancia r\u00e1pida e est\u00e1vel, com early stopping atuando de forma eficaz para evitar overfitting.</li> </ul>"},{"location":"projeto2/main/#53-limitacoes","title":"5.3 Limita\u00e7\u00f5es","text":"<p>Apesar do alto desempenho, o modelo possui limita\u00e7\u00f5es inerentes ao problema: - Os dados s\u00e3o puramente t\u00e9cnicos (baseados em pre\u00e7os), sem incorporar fatores macroecon\u00f4micos, not\u00edcias ou indicadores externos. - O modelo n\u00e3o considera depend\u00eancias temporais de longo prazo, que poderiam ser melhor capturadas por arquiteturas como RNNs ou LSTMs. - A previs\u00e3o \u00e9 univariada no tempo (t \u2192 t+1), n\u00e3o explorando janelas maiores de hist\u00f3rico, o que pode limitar a sensibilidade a tend\u00eancias.</p>"},{"location":"projeto2/main/#54-proximos-passos","title":"5.4 Pr\u00f3ximos passos","text":"<p>Para trabalhos futuros, prop\u00f5e-se: - Expandir o conjunto de features, incluindo indicadores econ\u00f4micos e dados de sentimento de mercado. - Testar arquiteturas mais profundas (ex.: 128\u201364\u201332) e otimizadores adaptativos como Adam e RMSProp. - Comparar o MLP com modelos lineares (regress\u00e3o, SVR) e sequenciais (LSTM, GRU) para avaliar ganhos de complexidade. - Submeter o modelo a uma competi\u00e7\u00e3o p\u00fablica (ex.: Kaggle) para validar seu desempenho em ambiente competitivo.</p>"},{"location":"projeto2/main/#55-reflexao-final","title":"5.5 Reflex\u00e3o final","text":"<p>O trabalho demonstrou dom\u00ednio completo do fluxo de aprendizado supervisionado com redes neurais \u2014 desde o pr\u00e9-processamento at\u00e9 a avalia\u00e7\u00e3o cr\u00edtica do modelo \u2014, consolidando conceitos fundamentais de aprendizado profundo aplicado a regress\u00e3o. Al\u00e9m do resultado t\u00e9cnico, o projeto refor\u00e7a a import\u00e2ncia de preparar adequadamente os dados e ajustar a arquitetura de forma criteriosa, antes de recorrer a solu\u00e7\u00f5es mais complexas.</p>"},{"location":"projeto2/main/#uso-de-ia","title":"Uso de IA","text":"<p>Usamos IA para corre\u00e7\u00e3o de c\u00f3digo assim como para auxiliar na constru\u00e7\u00e3o do markdown.</p>"}]}