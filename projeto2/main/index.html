<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Tomas M."><link href=https://tomasmiele.github.io/data-redes-neurais/projeto2/main/ rel=canonical><link href=../../projeto1/main/ rel=prev><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.22"><title>Projeto 2 - Regression - Data Redes Neurais</title><link rel=stylesheet href=../../assets/stylesheets/main.84d31ad4.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#projeto-2-regression-tomas-miele-e-yuri-tabacof class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="Data Redes Neurais" class="md-header__button md-logo" aria-label="Data Redes Neurais" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Data Redes Neurais </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Projeto 2 - Regression </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg> </label> <input class=md-option data-md-color-media=(prefers-color-scheme) data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_2 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_2> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_3 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=grey data-md-color-accent=indigo aria-label="Switch to system preference" type=radio name=__palette id=__palette_3> <label class="md-header__button md-icon" title="Switch to system preference" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/tomasmiele/data-redes-neurais title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> tomasmiele/data-redes-neurais </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="Data Redes Neurais" class="md-nav__button md-logo" aria-label="Data Redes Neurais" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Data Redes Neurais </label> <div class=md-nav__source> <a href=https://github.com/tomasmiele/data-redes-neurais title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg> </div> <div class=md-source__repository> tomasmiele/data-redes-neurais </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Roteiros </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Roteiros </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../exercicio1/main/ class=md-nav__link> <span class=md-ellipsis> Exercício 1 - Data </span> </a> </li> <li class=md-nav__item> <a href=../../exercicio2/main/ class=md-nav__link> <span class=md-ellipsis> Exercício 2 - Perceptron </span> </a> </li> <li class=md-nav__item> <a href=../../exercicio3/main/ class=md-nav__link> <span class=md-ellipsis> Exercício 3 - MLP </span> </a> </li> <li class=md-nav__item> <a href=../../exercicio4/main/ class=md-nav__link> <span class=md-ellipsis> Exercício 4 - VAE </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Projetos </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Projetos </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../projeto1/main/ class=md-nav__link> <span class=md-ellipsis> Projeto 1 - Classification </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Projeto 2 - Regression </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Projeto 2 - Regression </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-dataset-selection class=md-nav__link> <span class=md-ellipsis> 1. Dataset Selection </span> </a> </li> <li class=md-nav__item> <a href=#2-dataset-explanation class=md-nav__link> <span class=md-ellipsis> 2. Dataset Explanation </span> </a> <nav class=md-nav aria-label="2. Dataset Explanation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#variaveis-e-tipos class=md-nav__link> <span class=md-ellipsis> Variáveis e tipos </span> </a> </li> <li class=md-nav__item> <a href=#conhecimento-de-dominio class=md-nav__link> <span class=md-ellipsis> Conhecimento de domínio </span> </a> </li> <li class=md-nav__item> <a href=#estatisticas-e-diagnosticos class=md-nav__link> <span class=md-ellipsis> Estatísticas e diagnósticos </span> </a> <nav class=md-nav aria-label="Estatísticas e diagnósticos"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#matriz-de-correlacao class=md-nav__link> <span class=md-ellipsis> Matriz de correlação </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#distribuicoes-das-variaveis-principais class=md-nav__link> <span class=md-ellipsis> Distribuições das variáveis principais </span> </a> <nav class=md-nav aria-label="Distribuições das variáveis principais"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#retornos-diarios class=md-nav__link> <span class=md-ellipsis> Retornos diários </span> </a> </li> <li class=md-nav__item> <a href=#volatilidade-e-volume class=md-nav__link> <span class=md-ellipsis> Volatilidade e volume </span> </a> </li> <li class=md-nav__item> <a href=#indicadores-de-amplitude-e-posicao class=md-nav__link> <span class=md-ellipsis> Indicadores de amplitude e posição </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#relacoes-com-a-variavel-alvo class=md-nav__link> <span class=md-ellipsis> Relações com a variável-alvo </span> </a> <nav class=md-nav aria-label="Relações com a variável-alvo"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#target-vs-preco-atual class=md-nav__link> <span class=md-ellipsis> Target vs. preço atual </span> </a> </li> <li class=md-nav__item> <a href=#target-vs-volatilidade class=md-nav__link> <span class=md-ellipsis> Target vs. volatilidade </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#serie-historica class=md-nav__link> <span class=md-ellipsis> Série histórica </span> </a> </li> <li class=md-nav__item> <a href=#observacoes-para-modelagem class=md-nav__link> <span class=md-ellipsis> Observações para modelagem </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-data-cleaning-and-normalization class=md-nav__link> <span class=md-ellipsis> 3. Data Cleaning and Normalization </span> </a> <nav class=md-nav aria-label="3. Data Cleaning and Normalization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31-limpeza-de-dados class=md-nav__link> <span class=md-ellipsis> 3.1 Limpeza de dados </span> </a> </li> <li class=md-nav__item> <a href=#32-divisao-temporal class=md-nav__link> <span class=md-ellipsis> 3.2 Divisão temporal </span> </a> </li> <li class=md-nav__item> <a href=#33-tratamento-de-outliers class=md-nav__link> <span class=md-ellipsis> 3.3 Tratamento de outliers </span> </a> </li> <li class=md-nav__item> <a href=#34-normalizacao-das-features class=md-nav__link> <span class=md-ellipsis> 3.4 Normalização das features </span> </a> </li> <li class=md-nav__item> <a href=#35-efeitos-da-normalizacao class=md-nav__link> <span class=md-ellipsis> 3.5 Efeitos da normalização </span> </a> <nav class=md-nav aria-label="3.5 Efeitos da normalização"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#antes-da-normalizacao class=md-nav__link> <span class=md-ellipsis> Antes da normalização </span> </a> </li> <li class=md-nav__item> <a href=#apos-a-normalizacao-standardscaler class=md-nav__link> <span class=md-ellipsis> Após a normalização (StandardScaler) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#36-resultados-intermediarios class=md-nav__link> <span class=md-ellipsis> 3.6 Resultados intermediários </span> </a> </li> <li class=md-nav__item> <a href=#37-conclusoes class=md-nav__link> <span class=md-ellipsis> 3.7 Conclusões </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-mlp-implementation-numpy class=md-nav__link> <span class=md-ellipsis> 4. MLP Implementation (NumPy) </span> </a> <nav class=md-nav aria-label="4. MLP Implementation (NumPy)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#41-arquitetura-e-implementacao class=md-nav__link> <span class=md-ellipsis> 4.1 Arquitetura e implementação </span> </a> </li> <li class=md-nav__item> <a href=#42-treinamento-e-validacao class=md-nav__link> <span class=md-ellipsis> 4.2 Treinamento e validação </span> </a> </li> <li class=md-nav__item> <a href=#43-curvas-de-aprendizado class=md-nav__link> <span class=md-ellipsis> 4.3 Curvas de aprendizado </span> </a> </li> <li class=md-nav__item> <a href=#44-dispersao-das-previsoes class=md-nav__link> <span class=md-ellipsis> 4.4 Dispersão das previsões </span> </a> </li> <li class=md-nav__item> <a href=#45-conclusoes-parciais class=md-nav__link> <span class=md-ellipsis> 4.5 Conclusões parciais </span> </a> </li> <li class=md-nav__item> <a href=#46-proximos-passos class=md-nav__link> <span class=md-ellipsis> 4.6 Próximos passos </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#1-dataset-selection class=md-nav__link> <span class=md-ellipsis> 1. Dataset Selection </span> </a> </li> <li class=md-nav__item> <a href=#2-dataset-explanation class=md-nav__link> <span class=md-ellipsis> 2. Dataset Explanation </span> </a> <nav class=md-nav aria-label="2. Dataset Explanation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#variaveis-e-tipos class=md-nav__link> <span class=md-ellipsis> Variáveis e tipos </span> </a> </li> <li class=md-nav__item> <a href=#conhecimento-de-dominio class=md-nav__link> <span class=md-ellipsis> Conhecimento de domínio </span> </a> </li> <li class=md-nav__item> <a href=#estatisticas-e-diagnosticos class=md-nav__link> <span class=md-ellipsis> Estatísticas e diagnósticos </span> </a> <nav class=md-nav aria-label="Estatísticas e diagnósticos"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#matriz-de-correlacao class=md-nav__link> <span class=md-ellipsis> Matriz de correlação </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#distribuicoes-das-variaveis-principais class=md-nav__link> <span class=md-ellipsis> Distribuições das variáveis principais </span> </a> <nav class=md-nav aria-label="Distribuições das variáveis principais"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#retornos-diarios class=md-nav__link> <span class=md-ellipsis> Retornos diários </span> </a> </li> <li class=md-nav__item> <a href=#volatilidade-e-volume class=md-nav__link> <span class=md-ellipsis> Volatilidade e volume </span> </a> </li> <li class=md-nav__item> <a href=#indicadores-de-amplitude-e-posicao class=md-nav__link> <span class=md-ellipsis> Indicadores de amplitude e posição </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#relacoes-com-a-variavel-alvo class=md-nav__link> <span class=md-ellipsis> Relações com a variável-alvo </span> </a> <nav class=md-nav aria-label="Relações com a variável-alvo"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#target-vs-preco-atual class=md-nav__link> <span class=md-ellipsis> Target vs. preço atual </span> </a> </li> <li class=md-nav__item> <a href=#target-vs-volatilidade class=md-nav__link> <span class=md-ellipsis> Target vs. volatilidade </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#serie-historica class=md-nav__link> <span class=md-ellipsis> Série histórica </span> </a> </li> <li class=md-nav__item> <a href=#observacoes-para-modelagem class=md-nav__link> <span class=md-ellipsis> Observações para modelagem </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#3-data-cleaning-and-normalization class=md-nav__link> <span class=md-ellipsis> 3. Data Cleaning and Normalization </span> </a> <nav class=md-nav aria-label="3. Data Cleaning and Normalization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#31-limpeza-de-dados class=md-nav__link> <span class=md-ellipsis> 3.1 Limpeza de dados </span> </a> </li> <li class=md-nav__item> <a href=#32-divisao-temporal class=md-nav__link> <span class=md-ellipsis> 3.2 Divisão temporal </span> </a> </li> <li class=md-nav__item> <a href=#33-tratamento-de-outliers class=md-nav__link> <span class=md-ellipsis> 3.3 Tratamento de outliers </span> </a> </li> <li class=md-nav__item> <a href=#34-normalizacao-das-features class=md-nav__link> <span class=md-ellipsis> 3.4 Normalização das features </span> </a> </li> <li class=md-nav__item> <a href=#35-efeitos-da-normalizacao class=md-nav__link> <span class=md-ellipsis> 3.5 Efeitos da normalização </span> </a> <nav class=md-nav aria-label="3.5 Efeitos da normalização"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#antes-da-normalizacao class=md-nav__link> <span class=md-ellipsis> Antes da normalização </span> </a> </li> <li class=md-nav__item> <a href=#apos-a-normalizacao-standardscaler class=md-nav__link> <span class=md-ellipsis> Após a normalização (StandardScaler) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#36-resultados-intermediarios class=md-nav__link> <span class=md-ellipsis> 3.6 Resultados intermediários </span> </a> </li> <li class=md-nav__item> <a href=#37-conclusoes class=md-nav__link> <span class=md-ellipsis> 3.7 Conclusões </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#4-mlp-implementation-numpy class=md-nav__link> <span class=md-ellipsis> 4. MLP Implementation (NumPy) </span> </a> <nav class=md-nav aria-label="4. MLP Implementation (NumPy)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#41-arquitetura-e-implementacao class=md-nav__link> <span class=md-ellipsis> 4.1 Arquitetura e implementação </span> </a> </li> <li class=md-nav__item> <a href=#42-treinamento-e-validacao class=md-nav__link> <span class=md-ellipsis> 4.2 Treinamento e validação </span> </a> </li> <li class=md-nav__item> <a href=#43-curvas-de-aprendizado class=md-nav__link> <span class=md-ellipsis> 4.3 Curvas de aprendizado </span> </a> </li> <li class=md-nav__item> <a href=#44-dispersao-das-previsoes class=md-nav__link> <span class=md-ellipsis> 4.4 Dispersão das previsões </span> </a> </li> <li class=md-nav__item> <a href=#45-conclusoes-parciais class=md-nav__link> <span class=md-ellipsis> 4.5 Conclusões parciais </span> </a> </li> <li class=md-nav__item> <a href=#46-proximos-passos class=md-nav__link> <span class=md-ellipsis> 4.6 Próximos passos </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=projeto-2-regression-tomas-miele-e-yuri-tabacof>Projeto 2 - Regression - Tomas Miele e Yuri Tabacof</h1> <h2 id=1-dataset-selection>1. Dataset Selection</h2> <p><strong>Nome do dataset:</strong> S&amp;P 500 Historical Data — Stock Market Index Prediction<br> <strong>Fonte:</strong> Yahoo Finance / Kaggle<br> <strong>URL:</strong> https://www.kaggle.com/datasets/whenamancodes/sp-500-stock-data<br> <strong>Tamanho:</strong> ~15.000 registros diários, 7 variáveis (Open, High, Low, Close, Adj Close, Volume, Date) </p> <p><strong>Tarefa:</strong> Regressão — prever o <strong>preço de fechamento (Close)</strong> do índice S&amp;P 500 com base em variáveis históricas de mercado. </p> <p><strong>Justificativa da escolha:</strong><br> - O problema é <strong>financeiro e prático</strong>, ligado à <strong>previsão de preços</strong> — uma aplicação clássica de regressão contínua.<br> - Os dados representam <strong>séries temporais reais</strong> com ruído, tendência e sazonalidade, o que desafia o modelo e permite testar <strong>preprocessing, regularização e tuning do MLP</strong>.<br> - Contém <strong>variáveis correlacionadas e contínuas</strong> (preço de abertura, volume, máximas e mínimas), adequadas para exploração de <strong>relações não lineares</strong> via redes neurais.<br> - O volume de dados é <strong>suficiente</strong> (&gt;1.000 amostras) e com múltiplos atributos (&gt;5), atendendo integralmente aos critérios do projeto.<br> - Permite ainda incorporar <strong>engenharia de features financeiras</strong>, como retornos logarítmicos, médias móveis e volatilidade, tornando o problema mais robusto e próximo de aplicações reais de <em>quantitative finance</em>.<br> - A base é <strong>pública e reproduzível</strong>, podendo ser facilmente obtida via API do Yahoo Finance (<code>yfinance</code>) ou baixada do Kaggle, sem restrições de uso. </p> <h2 id=2-dataset-explanation>2. Dataset Explanation</h2> <p><strong>Representação e origem.</strong><br> O dataset consiste em dados históricos diários do índice <strong>S&amp;P 500</strong> (ticker <code>^GSPC</code>), obtidos via Yahoo Finance. Ele representa a evolução do principal índice acionário dos Estados Unidos, refletindo o desempenho médio ponderado das 500 maiores empresas listadas nas bolsas <strong>NYSE</strong> e <strong>NASDAQ</strong>.<br> O objetivo é realizar <strong>regressão contínua</strong>, prevendo o <strong>preço ajustado de fechamento do dia seguinte</strong> (<em>target</em>) a partir das variáveis conhecidas no dia atual.</p> <hr> <h3 id=variaveis-e-tipos>Variáveis e tipos</h3> <p>Todas as variáveis são <strong>numéricas contínuas</strong>, derivadas da estrutura padrão de mercado <strong>OHLCV</strong> (<em>Open, High, Low, Close, Volume</em>) e de indicadores técnicos calculados via janelas móveis.</p> <table> <thead> <tr> <th>Categoria</th> <th>Variáveis</th> <th>Descrição</th> </tr> </thead> <tbody> <tr> <td><strong>Base OHLCV</strong></td> <td><code>open</code>, <code>high</code>, <code>low</code>, <code>close</code>, <code>adj_close</code>, <code>volume</code></td> <td>Preços e volume de negociação diários.</td> </tr> <tr> <td><strong>Retornos e defasagens</strong></td> <td><code>ret_1d</code>, <code>logret_1d</code>, <code>adj_close_lag1</code>, <code>volume_lag1</code></td> <td>Retornos percentuais e logarítmicos, com defasagens de um dia.</td> </tr> <tr> <td><strong>Médias móveis e volatilidade</strong></td> <td><code>ma_5</code>, <code>ma_20</code>, <code>ma_60</code>, <code>vol_20</code></td> <td>Médias móveis de 5, 20 e 60 dias e desvio padrão dos retornos (volatilidade).</td> </tr> <tr> <td><strong>Amplitude e posição do fechamento</strong></td> <td><code>hl_range</code>, <code>close_pos_range</code></td> <td>Medem a oscilação intradiária e onde o fechamento ocorre dentro do range diário.</td> </tr> </tbody> </table> <p><strong>Variável-alvo (<em>target</em>):</strong><br> <code>target_adj_close_tplus1</code> — o preço ajustado de fechamento do dia seguinte (t+1), definido de modo a evitar vazamento de informação.</p> <hr> <h3 id=conhecimento-de-dominio>Conhecimento de domínio</h3> <ul> <li><strong>OHLCV:</strong> estrutura clássica de cotações usada em finanças quantitativas. </li> <li><strong>Adj Close:</strong> preço de fechamento ajustado por dividendos e <em>stock splits</em>, ideal para análises históricas. </li> <li><strong>Retornos logarítmicos:</strong> úteis para modelagem estatística, pois são aproximadamente simétricos e aditivos no tempo. </li> <li><strong>Volatilidade:</strong> aproxima o risco de curto prazo do mercado. </li> <li><strong>Médias móveis:</strong> reduzem ruído e capturam tendências. </li> <li><strong>Amplitude (<code>hl_range</code>) e posição (<code>close_pos_range</code>):</strong> medem força intradiária de movimento e fechamento relativo, úteis como <em>momentum indicators</em>.</li> </ul> <hr> <h3 id=estatisticas-e-diagnosticos>Estatísticas e diagnósticos</h3> <p>Após limpeza de dados incompletos (devido a janelas móveis e <em>lags</em>), o conjunto resultou em aproximadamente <strong>8.500 observações</strong> com <strong>16 variáveis preditoras</strong>. </p> <ul> <li>Não há <strong>valores ausentes significativos</strong> após o tratamento. </li> <li><strong>Outliers</strong> (via |z| &gt; 4) foram identificados principalmente em <code>ret_1d</code> e <code>volume</code>, coerentes com choques de mercado. </li> <li>As features derivadas de preço exibem forte <strong>colinearidade</strong> (ρ ≈ 1), especialmente entre <code>open</code>, <code>close</code>, <code>adj_close</code> e médias móveis — algo esperado em dados financeiros.</li> </ul> <h4 id=matriz-de-correlacao>Matriz de correlação</h4> <p><img alt="Matriz de Correlação — Features numéricas" src=../eda_outputs/corr_matrix_features.png></p> <p>A matriz mostra <strong>altas correlações lineares</strong> entre variáveis de preço, enquanto <code>ret_1d</code> e <code>logret_1d</code> apresentam correlação próxima de zero com os níveis de preço, o que é desejável para eliminar redundância. </p> <hr> <h3 id=distribuicoes-das-variaveis-principais>Distribuições das variáveis principais</h3> <h4 id=retornos-diarios>Retornos diários</h4> <p><img alt="Histograma - ret_1d" src=../eda_outputs/hist_ret_1d.png><br> <img alt="Histograma - logret_1d" src=../eda_outputs/hist_logret_1d.png> </p> <p>Ambos os retornos apresentam forma aproximadamente <strong>normal</strong> centrada em zero, com <strong>caudas levemente alongadas</strong> — padrão típico de séries financeiras com eventos extremos ocasionais (crises).</p> <h4 id=volatilidade-e-volume>Volatilidade e volume</h4> <p><img alt="Histograma - vol_20" src=../eda_outputs/hist_vol_20.png><br> <img alt="Histograma - volume" src=../eda_outputs/hist_volume.png> </p> <p>A volatilidade (<code>vol_20</code>) segue distribuição assimétrica positiva, indicando predominância de períodos de baixa volatilidade e raros picos altos.<br> O volume apresenta múltiplos modos (picos), sugerindo mudanças estruturais no mercado ao longo dos anos (ex: maior liquidez pós-2010).</p> <h4 id=indicadores-de-amplitude-e-posicao>Indicadores de amplitude e posição</h4> <p><img alt="Histograma - hl_range" src=../eda_outputs/hist_hl_range.png><br> <img alt="Histograma - close_pos_range" src=../eda_outputs/hist_close_pos_range.png> </p> <p><code>hl_range</code> é fortemente enviesado à esquerda, mostrando que na maioria dos dias a oscilação intradiária é pequena.<br> <code>close_pos_range</code> tem concentração nas extremidades 0 e 1 — o fechamento tende a ocorrer próximo das máximas ou mínimas do dia, indicando <strong>momentum</strong> ou <strong>reversões fortes</strong>.</p> <hr> <h3 id=relacoes-com-a-variavel-alvo>Relações com a variável-alvo</h3> <h4 id=target-vs-preco-atual>Target vs. preço atual</h4> <p><img alt="Scatter target vs. adj_close" src=../eda_outputs/scatter_target_vs_adj_close.png> </p> <p>A relação entre <code>adj_close (t)</code> e <code>target_adj_close (t+1)</code> é <strong>altamente linear</strong>, como esperado, pois o preço de fechamento de hoje é o principal determinante do de amanhã.</p> <h4 id=target-vs-volatilidade>Target vs. volatilidade</h4> <p><img alt="Scatter target vs. vol_20" src=../eda_outputs/scatter_target_vs_vol_20.png> </p> <p>A volatilidade mostra <strong>baixa correlação direta</strong> com o preço futuro — atua mais como indicador indireto de incerteza do movimento, e não de nível de preço.</p> <hr> <h3 id=serie-historica>Série histórica</h3> <p><img alt="S&P 500 - Adj Close (Histórico)" src=../eda_outputs/timeseries_adj_close.png></p> <p>A trajetória de longo prazo exibe uma <strong>tendência estrutural de alta</strong>, intercalada por períodos de forte queda (crises de 2000, 2008, 2020). Essa <strong>não-estacionariedade</strong> justifica o uso de retornos e indicadores normalizados como <em>features</em> no modelo.</p> <hr> <h3 id=observacoes-para-modelagem>Observações para modelagem</h3> <ul> <li>As <em>features</em> de preço e volume apresentam <strong>escalas distintas</strong>, exigindo <strong>normalização</strong> antes do treinamento (StandardScaler ou MinMaxScaler). </li> <li>Há <strong>alta redundância linear</strong> entre preços e médias móveis — regularização (L2, Dropout) ajudará o MLP a evitar sobreajuste. </li> <li>Os retornos e volatilidades fornecem informação incremental de movimento, sendo úteis para capturar <strong>não-linearidades</strong> sutis. </li> <li>O dataset é suficientemente extenso e complexo para testar diferentes arquiteturas e técnicas de regularização.</li> </ul> <hr> <h2 id=3-data-cleaning-and-normalization>3. Data Cleaning and Normalization</h2> <p><strong>Objetivo.</strong><br> Esta etapa teve como meta garantir a <strong>qualidade e consistência dos dados</strong> antes do treinamento do MLP, reduzindo ruído e diferenças de escala entre variáveis. Todas as transformações foram aplicadas de forma cronológica (sem vazamento de informação), com o <em>fit</em> feito apenas sobre o conjunto de treino.</p> <hr> <h3 id=31-limpeza-de-dados>3.1 Limpeza de dados</h3> <p><strong>Remoção de duplicatas.</strong><br> O índice temporal (<code>date</code>) foi verificado quanto a duplicações e apenas o primeiro registro de cada data foi mantido. </p> <p><strong>Valores faltantes.</strong><br> Após a engenharia de atributos na Parte 2 (lags, médias móveis e volatilidades), restaram alguns <code>NaN</code> nas primeiras linhas geradas por janelas deslizantes.<br> Essas linhas foram automaticamente removidas.<br> A checagem final (<code>missing_report_after_cleaning.csv</code>) confirma ausência de valores faltantes relevantes:</p> <table> <thead> <tr> <th>Coluna</th> <th>Missing</th> </tr> </thead> <tbody> <tr> <td>adj_close</td> <td>0</td> </tr> <tr> <td>volume_lag1</td> <td>0</td> </tr> <tr> <td>close_pos_range</td> <td>0</td> </tr> <tr> <td>hl_range</td> <td>0</td> </tr> <tr> <td>vol_20</td> <td>0</td> </tr> </tbody> </table> <p><strong>Justificativa.</strong><br> Optou-se por <strong>remover</strong> e não imputar valores faltantes, pois imputações em séries financeiras podem distorcer padrões temporais e autocorrelações. </p> <hr> <h3 id=32-divisao-temporal>3.2 Divisão temporal</h3> <p>Os dados foram divididos <strong>cronologicamente</strong> (sem embaralhamento) para preservar a dependência temporal típica de séries financeiras:</p> <ul> <li><strong>Treino:</strong> 70% (6.270 amostras) </li> <li><strong>Validação:</strong> 15% (1.344 amostras) </li> <li><strong>Teste:</strong> 15% (1.344 amostras)</li> </ul> <hr> <h3 id=33-tratamento-de-outliers>3.3 Tratamento de outliers</h3> <p>Análise exploratória revelou assimetria e caudas longas em variáveis como <code>volume</code>, <code>vol_20</code> e <code>hl_range</code>.<br> Esses picos extremos foram suavizados por <strong>winsorização (1º e 99º percentil)</strong>, limitando os valores a esse intervalo sem removê-los.</p> <p><strong>Racional:</strong><br> - Preserva a distribuição geral;<br> - Evita que valores anômalos distorçam o gradiente durante o treinamento;<br> - Evita perda de informação (ao contrário de remoção de linhas).</p> <hr> <h3 id=34-normalizacao-das-features>3.4 Normalização das features</h3> <p>Como o MLP utiliza funções de ativação sensíveis à escala (ReLU), aplicou-se <strong>StandardScaler</strong> — centrando os dados com média 0 e desvio padrão 1.<br> O ajuste (<code>fit</code>) foi realizado <strong>somente sobre o conjunto de treino</strong>, garantindo independência entre partições.</p> <table> <thead> <tr> <th>Tipo de transformação</th> <th>Descrição</th> <th>Escopo</th> </tr> </thead> <tbody> <tr> <td><strong>StandardScaler</strong></td> <td>$(x - \mu) / \sigma$</td> <td>Fit no treino; transform em val/test</td> </tr> <tr> <td><strong>Winsorização (1–99%)</strong></td> <td>Clipping por quantis</td> <td>Mesmos limites replicados no val/test</td> </tr> </tbody> </table> <hr> <h3 id=35-efeitos-da-normalizacao>3.5 Efeitos da normalização</h3> <p>Abaixo, observa-se o efeito da normalização sobre a variável <strong><code>volume</code></strong> — originalmente medida em bilhões, agora reescalonada para média 0 e desvio padrão 1.</p> <h4 id=antes-da-normalizacao>Antes da normalização</h4> <p><img alt="Volume - Antes da Normalização" src=../preproc_outputs/hist_volume_before.png></p> <h4 id=apos-a-normalizacao-standardscaler>Após a normalização (StandardScaler)</h4> <p><img alt="Volume - Após Normalização (StandardScaler)" src=../preproc_outputs/hist_volume_after.png></p> <p><strong>Observações:</strong> - A distribuição manteve sua forma assimétrica, mas o centro foi deslocado para 0.<br> - Os valores agora estão em uma faixa mais adequada à convergência do otimizador (tipicamente entre -3 e +3).<br> - Esse reescalonamento evita que variáveis com unidades grandes (como volume) dominem o cálculo do gradiente.</p> <hr> <h3 id=36-resultados-intermediarios>3.6 Resultados intermediários</h3> <p>Todos os conjuntos e relatórios foram salvos em <code>preproc_outputs/</code>:</p> <table> <thead> <tr> <th>Arquivo</th> <th>Descrição</th> </tr> </thead> <tbody> <tr> <td><code>X_train.csv</code>, <code>y_train.csv</code></td> <td>Conjunto de treino (70%)</td> </tr> <tr> <td><code>X_val.csv</code>, <code>y_val.csv</code></td> <td>Conjunto de validação (15%)</td> </tr> <tr> <td><code>X_test.csv</code>, <code>y_test.csv</code></td> <td>Conjunto de teste (15%)</td> </tr> <tr> <td><code>before_after_summary.csv</code></td> <td>Estatísticas comparativas (raw vs scaled)</td> </tr> <tr> <td><code>missing_report_after_cleaning.csv</code></td> <td>Diagnóstico de faltantes</td> </tr> <tr> <td><code>hist_volume_before.png</code> / <code>hist_volume_after.png</code></td> <td>Histogramas de distribuição</td> </tr> </tbody> </table> <hr> <h3 id=37-conclusoes>3.7 Conclusões</h3> <ul> <li><strong>Sem vazamento de dados:</strong> todas as transformações foram ajustadas apenas no conjunto de treino. </li> <li><strong>Dados prontos para o MLP:</strong> as features agora são homogêneas em escala, centradas e limitadas em amplitude. </li> <li><strong>Ganhos esperados:</strong> convergência mais estável e rápida, e gradientes numéricos mais equilibrados. </li> </ul> <p>Essa preparação garante que o modelo na próxima etapa (Parte 4 — <em>Model Architecture and Training</em>) receba entradas bem condicionadas e comparáveis entre si.</p> <h2 id=4-mlp-implementation-numpy>4. MLP Implementation (NumPy)</h2> <p><strong>Objetivo.</strong><br> Implementar um <strong>Multi-Layer Perceptron (MLP)</strong> do zero, utilizando apenas <strong>NumPy</strong> para operações matriciais, funções de ativação e cálculo dos gradientes.<br> O modelo foi desenvolvido e treinado sobre o conjunto de dados do índice <strong>S&amp;P 500</strong>, com o objetivo de <strong>prever o preço ajustado de fechamento do dia seguinte</strong> (<em>next-day adjusted close</em>).</p> <hr> <h3 id=41-arquitetura-e-implementacao>4.1 Arquitetura e implementação</h3> <p>O modelo foi inteiramente codificado em Python puro, sem uso de bibliotecas de <em>deep learning</em>.<br> A arquitetura segue a estrutura clássica de um <strong>feedforward network</strong>:</p> <table> <thead> <tr> <th style="text-align: center;">Camada</th> <th style="text-align: left;">Tipo</th> <th style="text-align: center;">Nº de unidades</th> <th style="text-align: left;">Função de ativação</th> <th style="text-align: left;">Observações</th> </tr> </thead> <tbody> <tr> <td style="text-align: center;">1</td> <td style="text-align: left;">Input</td> <td style="text-align: center;">14 features</td> <td style="text-align: left;">—</td> <td style="text-align: left;">Dados normalizados (<code>StandardScaler</code>)</td> </tr> <tr> <td style="text-align: center;">2</td> <td style="text-align: left;">Hidden</td> <td style="text-align: center;">64</td> <td style="text-align: left;">ReLU</td> <td style="text-align: left;">Inicialização He, dropout 10%</td> </tr> <tr> <td style="text-align: center;">3</td> <td style="text-align: left;">Hidden</td> <td style="text-align: center;">32</td> <td style="text-align: left;">ReLU</td> <td style="text-align: left;">Regularização L2 (<code>λ = 1e-4</code>)</td> </tr> <tr> <td style="text-align: center;">4</td> <td style="text-align: left;">Output</td> <td style="text-align: center;">1</td> <td style="text-align: left;">Linear</td> <td style="text-align: left;">Saída escalar contínua (preço futuro)</td> </tr> </tbody> </table> <p><strong>Funções implementadas:</strong> - <code>relu</code>, <code>tanh</code>, <code>sigmoid</code> e suas derivadas.<br> - <code>init_weights</code> com inicialização <strong>He</strong> para ReLU e <strong>Xavier</strong> para camadas lineares.<br> - <em>Mini-batch SGD</em> com regularização L2 e <em>dropout</em> invertido.<br> - <em>Early stopping</em> baseado em RMSE de validação com <code>patience = 25</code>.<br> - Métricas de regressão (<code>RMSE</code>, <code>MAE</code>, <code>R²</code>).</p> <p><strong>Hiperparâmetros principais:</strong></p> <table> <thead> <tr> <th>Parâmetro</th> <th style="text-align: center;">Valor</th> <th>Justificativa</th> </tr> </thead> <tbody> <tr> <td>Learning rate</td> <td style="text-align: center;"><code>5e-4</code></td> <td>Estável para dados padronizados</td> </tr> <tr> <td>Batch size</td> <td style="text-align: center;"><code>128</code></td> <td>Bom equilíbrio entre estabilidade e generalização</td> </tr> <tr> <td>Epochs máximos</td> <td style="text-align: center;"><code>300</code></td> <td>Permite convergência sem overfitting</td> </tr> <tr> <td>L2 regularization</td> <td style="text-align: center;"><code>1e-4</code></td> <td>Penaliza pesos grandes, suaviza overfitting</td> </tr> <tr> <td>Dropout</td> <td style="text-align: center;"><code>0.10</code></td> <td>Reduz coadaptação das camadas intermediárias</td> </tr> <tr> <td>Patience (early stopping)</td> <td style="text-align: center;"><code>25</code></td> <td>Evita overfitting e desperdício de treino</td> </tr> <tr> <td>Otimizador</td> <td style="text-align: center;">Mini-batch SGD</td> <td>Simples e eficiente em NumPy</td> </tr> <tr> <td>Ativação</td> <td style="text-align: center;">ReLU</td> <td>Melhor estabilidade para múltiplas camadas</td> </tr> </tbody> </table> <hr> <h3 id=42-treinamento-e-validacao>4.2 Treinamento e validação</h3> <p>O modelo foi treinado no alvo padronizado (<code>z-score</code>) e reescalonado para a unidade original (preço) ao calcular as métricas.<br> A tabela abaixo resume os resultados obtidos:</p> <table> <thead> <tr> <th style="text-align: left;">Conjunto</th> <th style="text-align: center;">RMSE</th> <th style="text-align: center;">MAE</th> <th style="text-align: center;">R²</th> </tr> </thead> <tbody> <tr> <td style="text-align: left;"><strong>Treino</strong></td> <td style="text-align: center;">63.85</td> <td style="text-align: center;">47.07</td> <td style="text-align: center;">0.9775</td> </tr> <tr> <td style="text-align: left;"><strong>Validação</strong></td> <td style="text-align: center;">60.27</td> <td style="text-align: center;">40.34</td> <td style="text-align: center;">0.9748</td> </tr> <tr> <td style="text-align: left;"><strong>Teste</strong></td> <td style="text-align: center;">79.20</td> <td style="text-align: center;">57.77</td> <td style="text-align: center;">0.9919</td> </tr> </tbody> </table> <p><strong>Tempo de treinamento:</strong> ~ poucos segundos em CPU.<br> <strong>Early stopping:</strong> atingido na <em>epoch 68</em>, com melhor <code>val_RMSE = 0.1417</code> no espaço normalizado.</p> <hr> <h3 id=43-curvas-de-aprendizado>4.3 Curvas de aprendizado</h3> <p>O gráfico abaixo mostra a evolução do erro quadrático médio (RMSE) em treino e validação ao longo das épocas.</p> <p><img alt="Curvas de RMSE — MLP" src=../model_outputs/loss_curves_rmse.png></p> <p><strong>Análise:</strong><br> - Rápida redução do erro até ~30 épocas, seguida de estabilização.<br> - Pequena divergência entre treino e validação após 50 épocas, indicando o início de overfitting.<br> - <em>Early stopping</em> atuou corretamente, interrompendo o treino quando o erro de validação parou de melhorar.</p> <hr> <h3 id=44-dispersao-das-previsoes>4.4 Dispersão das previsões</h3> <p>A seguir, a relação entre valores reais e previstos no conjunto de teste:</p> <p><img alt="Dispersão — Teste (y_true vs y_pred)" src=../model_outputs/scatter_test_true_vs_pred.png></p> <p><strong>Interpretação:</strong><br> Os pontos se distribuem próximos da diagonal, mostrando boa capacidade do modelo em prever o movimento do preço ajustado diário.<br> Pequenas dispersões ocorrem em valores extremos de mercado, onde a volatilidade é maior.</p> <hr> <h3 id=45-conclusoes-parciais>4.5 Conclusões parciais</h3> <ul> <li>O MLP implementado em NumPy apresentou <strong>excelente desempenho</strong> com <code>R² &gt; 0.97</code> em todos os conjuntos. </li> <li>A <strong>normalização do alvo e das features</strong> foi crucial para evitar explosão de gradientes. </li> <li>O uso de <strong>regularização L2</strong> e <strong>dropout leve (10%)</strong> garantiu boa generalização. </li> <li>O modelo converge de forma estável e explica mais de <strong>97% da variância</strong> do preço ajustado do S&amp;P 500 diário.</li> </ul> <hr> <h3 id=46-proximos-passos>4.6 Próximos passos</h3> <ul> <li>Aumentar o número de camadas (ex.: 128–64–32) para testar <em>depth sensitivity</em>. </li> <li>Experimentar otimizadores mais avançados (Adam ou RMSProp). </li> <li>Avaliar janelas de entrada maiores (ex.: incluir retornos de 5 dias) para incorporar memória de curto prazo. </li> <li>Comparar com modelos lineares e redes recorrentes (RNN/LSTM) em trabalhos futuros.</li> </ul> <h1 id=5-conclusion>5. Conclusion</h1> <h2 id=51-resumo-geral>5.1 Resumo geral.</h2> <p>O projeto cumpriu todas as etapas do pipeline de regressão com redes neurais, desde a coleta e engenharia de dados até a implementação e avaliação completa de um Multi-Layer Perceptron (MLP) desenvolvido integralmente em NumPy. O modelo foi aplicado a uma tarefa realista — previsão do preço ajustado do índice S&amp;P 500 para o dia seguinte —, demonstrando alto desempenho e estabilidade.</p> <hr> <h2 id=52-codigo-completo>5.2 Código completo</h2> <p>``` pyodide install="numpy, matplotlib" import os import numpy as np import pandas as pd import matplotlib.pyplot as plt from scipy.stats import zscore import json import time from sklearn.preprocessing import StandardScaler import warnings import yfinance as yf</p> <h1 id=_1>======================</h1> <h1 id=1-download-dos-dados-ajustes-importantes>1) Download dos dados (ajustes importantes)</h1> <h1 id=_2>======================</h1> <h1 id=-auto_adjustfalse-para-manter-adj-close>- auto_adjust=False para manter "Adj Close"</h1> <h1 id=-group_bycolumn-para-evitar-multiindex-nas-colunas>- group_by='column' para evitar MultiIndex nas colunas</h1> <h1 id=-actionsfalse-porque-nao-precisamos-de-dividendossplits-neste-passo>- actions=False porque não precisamos de dividendos/splits neste passo</h1> <p>df = yf.download( "^GSPC", start="1990-01-01", progress=False, auto_adjust=False, group_by="column", actions=False, )</p> <p>if df is None or df.empty: raise SystemExit( "Sem dados baixados. Verifique conexão/ambiente ou ajuste o intervalo de datas." )</p> <h1 id=garante-colunas-simples-nao-multiindex-por-seguranca-extra>Garante colunas simples (não-MultiIndex), por segurança extra</h1> <p>if isinstance(df.columns, pd.MultiIndex): df.columns = df.columns.get_level_values(0)</p> <h1 id=renomeia-o-que-existir>Renomeia o que existir</h1> <p>rename_map = { "Open": "open", "High": "high", "Low": "low", "Close": "close", "Adj Close": "adj_close", "Volume": "volume", } existing_map = {k: v for k, v in rename_map.items() if k in df.columns} df = df.rename(columns=existing_map) df.index.name = "date"</p> <h1 id=se-por-algum-motivo-adj_close-nao-existir-use-close>Se por algum motivo "adj_close" não existir, use "close"</h1> <p>if "adj_close" not in df.columns and "close" in df.columns: df["adj_close"] = df["close"]</p> <h1 id=_3>======================</h1> <h1 id=2-garantir-tipos-numericos-apenas-no-que-existir>2) Garantir tipos numéricos (apenas no que existir)</h1> <h1 id=_4>======================</h1> <p>numeric_candidates = ["open", "high", "low", "close", "adj_close", "volume"] numeric_cols = [c for c in numeric_candidates if c in df.columns] for c in numeric_cols: # to_numeric espera Series/array; df[c] é Series aqui df[c] = pd.to_numeric(df[c], errors="coerce")</p> <h1 id=_5>======================</h1> <h1 id=3-engenharia-de-features>3) Engenharia de features</h1> <h1 id=_6>======================</h1> <h1 id=retornos-t-simples-e-log>Retornos (t): simples e log</h1> <p>df["ret_1d"] = df["adj_close"].pct_change() df["logret_1d"] = np.log(df["adj_close"]).diff()</p> <h1 id=lags-t-1-de-preco-ajustado-e-volume>Lags (t-1) de preço ajustado e volume</h1> <p>if "adj_close" in df.columns: df["adj_close_lag1"] = df["adj_close"].shift(1) if "volume" in df.columns: df["volume_lag1"] = df["volume"].shift(1)</p> <h1 id=medias-moveis-e-volatilidade-janelas>Médias móveis e volatilidade (janelas)</h1> <p>for w in (5, 20, 60): df[f"ma_{w}"] = df["adj_close"].rolling(window=w, min_periods=w).mean() df["vol_20"] = df["logret_1d"].rolling(window=20, min_periods=20).std()</p> <h1 id=indicadores-de-amplitude-e-posicao-do-fechamento>Indicadores de amplitude e posição do fechamento</h1> <h1 id=usa-denominador-com-protecao-para-zero>Usa denominador com proteção para zero</h1> <p>rng = df["high"] - df["low"] df["hl_range"] = (df["high"] - df["low"]) / df["low"].where(df["low"] != 0, np.nan) df["close_pos_range"] = (df["close"] - df["low"]) / rng.where(rng != 0, np.nan)</p> <h1 id=remove-linhas-incompletas-introduzidas-por-rollingshift>Remove linhas incompletas introduzidas por rolling/shift</h1> <p>df = df.dropna().copy()</p> <h1 id=_7>======================</h1> <h1 id=4-target-e-features>4) Target e features</h1> <h1 id=_8>======================</h1> <p>df["target_adj_close_tplus1"] = df["adj_close"].shift(-1) df = df.dropna(subset=["target_adj_close_tplus1"]).copy()</p> <p>feature_cols = [ c for c in [ "open", "high", "low", "close", "adj_close", "volume", "ret_1d", "logret_1d", "adj_close_lag1", "volume_lag1", "ma_5", "ma_20", "ma_60", "vol_20", "hl_range", "close_pos_range", ] if c in df.columns ]</p> <p>X = df[feature_cols].copy() y = df["target_adj_close_tplus1"].copy()</p> <h1 id=_9>======================</h1> <h1 id=5-diagnosticos-faltantes-e-outliers>5) Diagnósticos: faltantes e outliers</h1> <h1 id=_10>======================</h1> <p>missing = X.isna().sum().sort_values(ascending=False) missing_pct = (X.isna().mean() * 100).round(3)</p> <h1 id=outliers-por-z-score-z4-diagnostico>Outliers por z-score |z|&gt;4 (diagnóstico)</h1> <p>zscores = X.apply(zscore, nan_policy="omit") outlier_mask = np.abs(zscores) &gt; 4 outliers_por_col = outlier_mask.sum().sort_values(ascending=False)</p> <h1 id=_11>======================</h1> <h1 id=6-estatisticas-e-salvamento>6) Estatísticas e salvamento</h1> <h1 id=_12>======================</h1> <p>desc_X = X.describe().T desc_y = y.describe()</p> <p>os.makedirs("eda_outputs", exist_ok=True) desc_X.to_csv("eda_outputs/summary_stats_features.csv") pd.DataFrame({"missing_count": missing, "missing_pct": missing_pct}).to_csv( "eda_outputs/missing_values_report.csv" ) outliers_por_col.to_csv("eda_outputs/outliers_by_feature_z4.csv") df.to_csv("eda_outputs/sp500_features_target.csv")</p> <h1 id=_13>======================</h1> <h1 id=7-graficos-matplotlib-sem-seaborn>7) Gráficos (Matplotlib — sem seaborn)</h1> <h1 id=_14>======================</h1> <p>plt.figure(figsize=(10, 4)) plt.plot(df.index, df["adj_close"]) plt.title("S&amp;P 500 - Adj Close (Histórico)") plt.xlabel("Data") plt.ylabel("Adj Close") plt.tight_layout() plt.savefig("eda_outputs/timeseries_adj_close.png", dpi=150) plt.close()</p> <p>def save_hist(series, fname, bins=50): plt.figure(figsize=(6, 4)) plt.hist(series.dropna().values, bins=bins) plt.title(f"Histograma - {series.name}") plt.xlabel(series.name) plt.ylabel("Frequência") plt.tight_layout() plt.savefig(fname, dpi=150) plt.close()</p> <p>for col in [ c for c in ["ret_1d", "logret_1d", "volume", "vol_20", "hl_range", "close_pos_range"] if c in X.columns ]: save_hist(X[col], f"eda_outputs/hist_{col}.png")</p> <h1 id=matriz-de-correlacao_1>Matriz de correlação</h1> <p>corr = X.corr(numeric_only=True) plt.figure(figsize=(9, 8)) im = plt.imshow(corr.values, aspect="auto", interpolation="nearest") plt.colorbar(im, fraction=0.046, pad=0.04) plt.xticks(ticks=np.arange(len(corr.columns)), labels=corr.columns, rotation=90) plt.yticks(ticks=np.arange(len(corr.index)), labels=corr.index) plt.title("Matriz de Correlação — Features (numéricas)") plt.tight_layout() plt.savefig("eda_outputs/corr_matrix_features.png", dpi=150) plt.close()</p> <p>def save_scatter(x, y, fname, xlabel, ylabel): plt.figure(figsize=(6, 4)) plt.scatter(x, y, s=3, alpha=0.5) plt.xlabel(xlabel) plt.ylabel(ylabel) plt.title(f"{ylabel} vs. {xlabel}") plt.tight_layout() plt.savefig(fname, dpi=150) plt.close()</p> <p>if "adj_close" in X.columns: save_scatter( X["adj_close"], y, "eda_outputs/scatter_target_vs_adj_close.png", "adj_close (t)", "target_adj_close (t+1)", ) if "vol_20" in X.columns: save_scatter( X["vol_20"], y, "eda_outputs/scatter_target_vs_vol_20.png", "vol_20 (t)", "target_adj_close (t+1)", )</p> <p>print("✅ EDA concluída. Saídas salvas na pasta: eda_outputs/") for f in sorted(os.listdir("eda_outputs")): print("-", f)</p> <h1 id=_15>================================================================</h1> <h1 id=parte-3-data-cleaning-and-normalization>PARTE 3 — Data Cleaning and Normalization</h1> <h1 id=_16>================================================================</h1> <p>print("\n=== PARTE 3 — Data Cleaning and Normalization ===")</p> <p>warnings.filterwarnings("ignore")</p> <h1 id=->-------------------------</h1> <h1 id=1-checar-duplicatas-e-faltantes>1. Checar duplicatas e faltantes</h1> <h1 id=-_1>-------------------------</h1> <p>print("\nChecando duplicatas e faltantes...")</p> <p>df = df[~df.index.duplicated(keep="first")].copy() missing_report = df.isna().sum().sort_values(ascending=False) print("Valores faltantes após EDA:\n", missing_report.head())</p> <p>missing_report.to_csv("eda_outputs/missing_report_after_cleaning.csv")</p> <h1 id=-_2>-------------------------</h1> <h1 id=2-split-temporal-701515>2. Split temporal: 70/15/15</h1> <h1 id=-_3>-------------------------</h1> <p>print("\nRealizando split temporal...")</p> <p>dates = df.index.sort_values() n = len(dates) train_end = dates[int(0.70 * n)] val_end = dates[int(0.85 * n)]</p> <p>train_df = df.loc[:train_end].copy() val_df = df.loc[train_end:].loc[:val_end].copy() test_df = df.loc[val_end:].copy()</p> <p>print(f"Tamanhos → train: {len(train_df)}, val: {len(val_df)}, test: {len(test_df)}")</p> <h1 id=-_4>-------------------------</h1> <h1 id=3-winsorizacao-limite-1-e-99-para-suavizar-outliers>3. Winsorização (limite 1% e 99%) para suavizar outliers</h1> <h1 id=-_5>-------------------------</h1> <p>print("\nAplicando winsorização (1–99%) em colunas assimétricas...")</p> <p>def winsorize(df_in, cols, lower=0.01, upper=0.99, ref=None): df = df_in.copy() ref = ref if ref is not None else df for c in cols: lo, hi = ref[c].quantile([lower, upper]) df[c] = df[c].clip(lower=lo, upper=hi) return df</p> <p>winsor_cols = [c for c in ["volume", "vol_20", "hl_range"] if c in df.columns]</p> <p>train_df = winsorize(train_df, winsor_cols) val_df = winsorize(val_df, winsor_cols, ref=train_df) test_df = winsorize(test_df, winsor_cols, ref=train_df)</p> <h1 id=-_6>-------------------------</h1> <h1 id=4-normalizacao-standardscaler-media-0-std-1>4. Normalização (StandardScaler — média 0, std 1)</h1> <h1 id=-_7>-------------------------</h1> <p>print("\nNormalizando features numéricas (StandardScaler)...")</p> <p>feature_cols = [ c for c in [ "open", "high", "low", "close", "adj_close", "volume", "ret_1d", "logret_1d", "adj_close_lag1", "volume_lag1", "ma_5", "ma_20", "ma_60", "vol_20", "hl_range", "close_pos_range", ] if c in df.columns ]</p> <p>target_col = "target_adj_close_tplus1"</p> <p>X_train, y_train = train_df[feature_cols], train_df[target_col] X_val, y_val = val_df[feature_cols], val_df[target_col] X_test, y_test = test_df[feature_cols], test_df[target_col]</p> <p>scaler = StandardScaler() scaler.fit(X_train)</p> <p>X_train_scaled = pd.DataFrame( scaler.transform(X_train), index=X_train.index, columns=X_train.columns ) X_val_scaled = pd.DataFrame( scaler.transform(X_val), index=X_val.index, columns=X_val.columns ) X_test_scaled = pd.DataFrame( scaler.transform(X_test), index=X_test.index, columns=X_test.columns )</p> <h1 id=-_8>-------------------------</h1> <h1 id=5-salvar-conjuntos-processados>5. Salvar conjuntos processados</h1> <h1 id=-_9>-------------------------</h1> <p>os.makedirs("preproc_outputs", exist_ok=True) X_train_scaled.to_csv("preproc_outputs/X_train.csv") y_train.to_csv("preproc_outputs/y_train.csv") X_val_scaled.to_csv("preproc_outputs/X_val.csv") y_val.to_csv("preproc_outputs/y_val.csv") X_test_scaled.to_csv("preproc_outputs/X_test.csv") y_test.to_csv("preproc_outputs/y_test.csv")</p> <p>print("\nArquivos escalonados salvos em 'preproc_outputs/'")</p> <h1 id=-_10>-------------------------</h1> <h1 id=6-comparacao-beforeafter>6. Comparação Before/After</h1> <h1 id=-_11>-------------------------</h1> <p>before_after = pd.concat( [ X_train[winsor_cols].describe().T.add_prefix("raw_"), pd.DataFrame(X_train_scaled[winsor_cols]).describe().T.add_prefix("scaled_"), ], axis=1, ) before_after.to_csv("preproc_outputs/before_after_summary.csv")</p> <h1 id=-_12>-------------------------</h1> <h1 id=7-visualizacoes-beforeafter>7. Visualizações Before/After</h1> <h1 id=-_13>-------------------------</h1> <p>plt.figure(figsize=(6, 4)) plt.hist(X_train["volume"], bins=60) plt.title("Volume - Antes da Normalização") plt.tight_layout() plt.savefig("preproc_outputs/hist_volume_before.png") plt.close()</p> <p>plt.figure(figsize=(6, 4)) plt.hist(X_train_scaled["volume"], bins=60) plt.title("Volume - Após Normalização (StandardScaler)") plt.tight_layout() plt.savefig("preproc_outputs/hist_volume_after.png") plt.close()</p> <p>print("✅ Parte 3 finalizada com sucesso — dados limpos, winsorizados e normalizados.")</p> <p>print("\n=== PARTE 4 — MLP Implementation (NumPy) ===")</p> <h1 id=-_14>-------------------------</h1> <h1 id=0-carregar-dados-pre-processados-parte-3>0) Carregar dados pré-processados (Parte 3)</h1> <h1 id=-_15>-------------------------</h1> <p>def _load_xy(prefix_dir="preproc_outputs"): X_train = pd.read_csv(f"{prefix_dir}/X_train.csv", index_col=0) y_train = pd.read_csv(f"{prefix_dir}/y_train.csv", index_col=0).values.reshape( -1, 1 ) X_val = pd.read_csv(f"{prefix_dir}/X_val.csv", index_col=0) y_val = pd.read_csv(f"{prefix_dir}/y_val.csv", index_col=0).values.reshape(-1, 1) X_test = pd.read_csv(f"{prefix_dir}/X_test.csv", index_col=0) y_test = pd.read_csv(f"{prefix_dir}/y_test.csv", index_col=0).values.reshape(-1, 1) return (X_train, y_train, X_val, y_val, X_test, y_test)</p> <p>Xtr_df, y_train, Xva_df, y_val, Xte_df, y_test = _load_xy()</p> <h1 id=-_16>-------------------------</h1> <h1 id=1-sanity-checks-e-saneamento-evita-naninfcolunas-com-std0>1) Sanity checks e saneamento (evita NaN/Inf/colunas com std=0)</h1> <h1 id=-_17>-------------------------</h1> <h1 id=remover-colunas-com-desvio-0-podem-causar-nan-no-scaler-anterior>remover colunas com desvio 0 (podem causar NaN no scaler anterior)</h1> <p>zero_std_cols = Xtr_df.columns[ (Xtr_df.std(axis=0, ddof=0) == 0) | (~np.isfinite(Xtr_df.std(axis=0, ddof=0))) ] if len(zero_std_cols) &gt; 0: print("⚠️ Removendo colunas com std=0:", list(zero_std_cols)) Xtr_df = Xtr_df.drop(columns=zero_std_cols) Xva_df = Xva_df.drop(columns=zero_std_cols, errors="ignore") Xte_df = Xte_df.drop(columns=zero_std_cols, errors="ignore")</p> <p>def sanitize(df, name): n_nan = np.isnan(df.values).sum() n_inf = np.isinf(df.values).sum() if n_nan or n_inf: print(f"⚠️ {name}: substituindo {n_nan} NaN e {n_inf} Inf por 0.0") df = df.replace([np.inf, -np.inf], np.nan).fillna(0.0) return df</p> <p>Xtr_df = sanitize(Xtr_df, "X_train") Xva_df = sanitize(Xva_df, "X_val") Xte_df = sanitize(Xte_df, "X_test")</p> <p>X_train = Xtr_df.values X_val = Xva_df.values X_test = Xte_df.values</p> <p>assert ( np.isfinite(X_train).all() and np.isfinite(X_val).all() and np.isfinite(X_test).all() ), "Ainda há valores não finitos nas features!"</p> <h1 id=padronizar-o-alvo-z-score-para-estabilizar-gradientes>Padronizar o alvo (z-score) para estabilizar gradientes</h1> <p>y_mean, y_std = float(np.mean(y_train)), float(np.std(y_train)) if y_std == 0 or not np.isfinite(y_std): y_std = 1.0 y_train_z = (y_train - y_mean) / y_std y_val_z = (y_val - y_mean) / y_std y_test_z = (y_test - y_mean) / y_std</p> <h1 id=-_18>-------------------------</h1> <h1 id=2-metricas-de-regressao>2) Métricas de regressão</h1> <h1 id=-_19>-------------------------</h1> <p>def mse(y_true, y_pred): return float(np.mean((y_true - y_pred) ** 2))</p> <p>def rmse(y_true, y_pred): return float(np.sqrt(mse(y_true, y_pred)))</p> <p>def mae(y_true, y_pred): return float(np.mean(np.abs(y_true - y_pred)))</p> <p>def r2_score(y_true, y_pred): ss_res = np.sum((y_true - y_pred) ** 2) ss_tot = np.sum((y_true - np.mean(y_true)) ** 2) return float(1 - ss_res / ss_tot)</p> <h1 id=-_20>-------------------------</h1> <h1 id=3-ativacoes-e-derivadas>3) Ativações e derivadas</h1> <h1 id=-_21>-------------------------</h1> <p>def relu(z): return np.maximum(0, z)</p> <p>def drelu(z): return (z &gt; 0).astype(z.dtype)</p> <p>def sigmoid(z): return 1.0 / (1.0 + np.exp(-z))</p> <p>def dsigmoid(z): s = sigmoid(z) return s * (1 - s)</p> <p>def tanh(z): return np.tanh(z)</p> <p>def dtanh(z): return 1 - np.tanh(z) ** 2</p> <p>ACTS = {"relu": (relu, drelu), "tanh": (tanh, dtanh), "sigmoid": (sigmoid, dsigmoid)}</p> <h1 id=-_22>-------------------------</h1> <h1 id=4-inicializacao-he-para-relu-xavier-aproximado-para-demais>4) Inicialização (He para ReLU; Xavier aproximado para demais)</h1> <h1 id=compativel-com-default_rng-standard_normal>compatível com default_rng (standard_normal)</h1> <h1 id=-_23>-------------------------</h1> <p>def init_weights(in_dim, out_dim, act_name, rng): if act_name == "relu": std = np.sqrt(2.0 / in_dim) else: std = np.sqrt(1.0 / in_dim) if hasattr(rng, "standard_normal"): W = rng.standard_normal((in_dim, out_dim)) * std else: W = rng.randn(in_dim, out_dim) * std b = np.zeros((1, out_dim)) return W, b</p> <h1 id=-_24>-------------------------</h1> <h1 id=5-mlp-from-scratch-mini-batch-sgd-l2-dropout-early-stopping>5) MLP from scratch (mini-batch SGD + L2 + dropout + early stopping)</h1> <h1 id=-_25>-------------------------</h1> <p>class MLPRegressorScratch: def <strong>init</strong>( self, input_dim, hidden_dims=(64, 32), activation="relu", lr=5e-4, batch_size=128, epochs=300, l2=1e-4, dropout=0.0, seed=42, early_stopping=True, patience=25, verbose=True, ): self.input_dim = input_dim self.hidden_dims = list(hidden_dims) self.activation = activation self.lr = lr self.batch_size = batch_size self.epochs = epochs self.l2 = l2 self.dropout = dropout self.seed = seed self.early_stopping = early_stopping self.patience = patience self.verbose = verbose</p> <div class="language-text highlight"><pre><span></span><code>    assert activation in ACTS, f&quot;Ativação inválida: {activation}&quot;
    self.act, self.dact = ACTS[activation]
    self.rng = np.random.default_rng(seed)
    self._init_params()

def _init_params(self):
    dims = [self.input_dim] + self.hidden_dims + [1]  # saída escalar
    self.W, self.b = [], []
    # ocultas
    for i in range(len(dims) - 2):
        Wi, bi = init_weights(dims[i], dims[i + 1], self.activation, self.rng)
        self.W.append(Wi)
        self.b.append(bi)
    # saída linear (Xavier ok)
    Wi, bi = init_weights(dims[-2], dims[-1], &quot;tanh&quot;, self.rng)
    self.W.append(Wi)
    self.b.append(bi)

def _forward(self, X, train_mode=False):
    a = X
    caches = {&quot;A0&quot;: a, &quot;Z&quot;: [], &quot;A&quot;: [], &quot;drop_masks&quot;: []}
    # ocultas
    for L in range(len(self.hidden_dims)):
        z = a @ self.W[L] + self.b[L]
        a = self.act(z)
        mask = None
        if train_mode and self.dropout &gt; 0:
            mask = (self.rng.random(a.shape) &gt; self.dropout).astype(a.dtype)
            a = a * mask / (1.0 - self.dropout)  # inverted dropout
        caches[&quot;Z&quot;].append(z)
        caches[&quot;A&quot;].append(a)
        caches[&quot;drop_masks&quot;].append(mask)
    # saída linear
    z_out = a @ self.W[-1] + self.b[-1]
    y_hat = z_out
    caches[&quot;Z_out&quot;] = z_out
    caches[&quot;A_out&quot;] = y_hat
    return y_hat, caches

def _backward(self, y_hat, y_true, caches):
    N = y_true.shape[0]
    grad_W = [None] * len(self.W)
    grad_b = [None] * len(self.b)

    # saída
    dL_dy = 2.0 * (y_hat - y_true) / N
    a_prev = caches[&quot;A&quot;][-1] if len(self.hidden_dims) &gt; 0 else caches[&quot;A0&quot;]
    grad_W[-1] = a_prev.T @ dL_dy + self.l2 * self.W[-1]
    grad_b[-1] = np.sum(dL_dy, axis=0, keepdims=True)

    da = dL_dy @ self.W[-1].T
    for L in reversed(range(len(self.hidden_dims))):
        zL = caches[&quot;Z&quot;][L]
        a_prev = caches[&quot;A&quot;][L - 1] if L &gt; 0 else caches[&quot;A0&quot;]
        mask = caches[&quot;drop_masks&quot;][L]
        if mask is not None:
            da = da * mask / (1.0 - self.dropout)
        dz = da * self.dact(zL)
        grad_W[L] = a_prev.T @ dz + self.l2 * self.W[L]
        grad_b[L] = np.sum(dz, axis=0, keepdims=True)
        if L &gt; 0:
            da = dz @ self.W[L].T
    return grad_W, grad_b

def _update(self, grad_W, grad_b):
    for i in range(len(self.W)):
        self.W[i] -= self.lr * grad_W[i]
        self.b[i] -= self.lr * grad_b[i]

def fit(self, X_train, y_train, X_val=None, y_val=None):
    rng = self.rng
    N = X_train.shape[0]
    bsz = self.batch_size
    self.history_ = {&quot;train_rmse&quot;: [], &quot;val_rmse&quot;: []}
    best_val = np.inf
    best_state = None
    patience_left = self.patience

    t0 = time.time()
    for epoch in range(1, self.epochs + 1):
        idx = rng.permutation(N)
        X_train, y_train = X_train[idx], y_train[idx]
        for s in range(0, N, bsz):
            e = min(s + bsz, N)
            xb, yb = X_train[s:e], y_train[s:e]
            y_hat, cache = self._forward(xb, train_mode=True)
            gW, gB = self._backward(y_hat, yb, cache)
            self._update(gW, gB)

        # métricas por época (no espaço z)
        tr_pred, _ = self._forward(X_train, train_mode=False)
        tr_rmse = rmse(y_train, tr_pred)
        self.history_[&quot;train_rmse&quot;].append(tr_rmse)

        if X_val is not None:
            v_pred, _ = self._forward(X_val, train_mode=False)
            v_rmse = rmse(y_val, v_pred)
            self.history_[&quot;val_rmse&quot;].append(v_rmse)
        else:
            v_rmse = np.nan

        if self.verbose and (epoch == 1 or epoch % 10 == 0):
            print(
                f&quot;[Epoch {epoch:4d}] train_RMSE={tr_rmse:.4f}  val_RMSE={v_rmse:.4f}&quot;
            )

        if self.early_stopping and X_val is not None:
            if v_rmse + 1e-9 &lt; best_val:
                best_val = v_rmse
                patience_left = self.patience
                best_state = {
                    &quot;W&quot;: [w.copy() for w in self.W],
                    &quot;b&quot;: [b.copy() for b in self.b],
                    &quot;epoch&quot;: epoch,
                }
            else:
                patience_left -= 1
                if patience_left &lt;= 0:
                    if self.verbose:
                        print(
                            f&quot;Early stopping em epoch {epoch} (melhor val_RMSE={best_val:.4f})&quot;
                        )
                    if best_state is not None:
                        self.W = [w.copy() for w in best_state[&quot;W&quot;]]
                        self.b = [b.copy() for b in best_state[&quot;b&quot;]]
                    break

    self.train_time_ = time.time() - t0
    return self

def predict(self, X):
    y_hat, _ = self._forward(X, train_mode=False)
    return y_hat
</code></pre></div> <h1 id=-_26>-------------------------</h1> <h1 id=6-configuracao-treino-e-avaliacao>6) Configuração, treino e avaliação</h1> <h1 id=-_27>-------------------------</h1> <p>INPUT_DIM = X_train.shape[1] mlp = MLPRegressorScratch( input_dim=INPUT_DIM, hidden_dims=(64, 32), # experimente (128, 64, 32) activation="relu", # "relu" | "tanh" | "sigmoid" lr=5e-4, # taxa de aprendizado (estável) batch_size=128, epochs=300, l2=1e-4, # weight decay dropout=0.10, # 10% na(s) oculta(s) — pode começar com 0.0 seed=42, early_stopping=True, patience=25, verbose=True, )</p> <h1 id=treino-no-espaco-z-do-alvo>treino no espaço z do alvo</h1> <p>mlp.fit(X_train, y_train_z, X_val, y_val_z)</p> <h1 id=predicoes-no-espaco-z>predições no espaço z</h1> <p>yhat_tr_z = mlp.predict(X_train) yhat_va_z = mlp.predict(X_val) yhat_te_z = mlp.predict(X_test)</p> <h1 id=retornar-a-escala-original-do-preco>retornar à escala original do preço</h1> <p>yhat_tr = yhat_tr_z * y_std + y_mean yhat_va = yhat_va_z * y_std + y_mean yhat_te = yhat_te_z * y_std + y_mean</p> <p>def eval_and_print(tag, y_true, y_pred): res = { "RMSE": rmse(y_true, y_pred), "MAE": mae(y_true, y_pred), "R2": r2_score(y_true, y_pred), } print(f"{tag}: RMSE={res['RMSE']:.4f} MAE={res['MAE']:.4f} R2={res['R2']:.4f}") return res</p> <p>os.makedirs("model_outputs", exist_ok=True)</p> <p>metrics = { "train": eval_and_print("TRAIN", y_train, yhat_tr), "val": eval_and_print("VAL ", y_val, yhat_va), "test": eval_and_print("TEST ", y_test, yhat_te), "train_time_sec": mlp.train_time_, } with open("model_outputs/metrics.json", "w") as f: json.dump(metrics, f, indent=2)</p> <h1 id=curvas-de-rmse-treinoval-ainda-no-espaco-z>curvas de RMSE (treino/val) — ainda no espaço z</h1> <p>plt.figure(figsize=(7, 4)) plt.plot(mlp.history_.get("train_rmse", []), label="Train RMSE (z)") if len(mlp.history_.get("val_rmse", [])) &gt; 0: plt.plot(mlp.history_["val_rmse"], label="Val RMSE (z)") plt.xlabel("Epoch") plt.ylabel("RMSE") plt.title("Curvas de RMSE — MLP (NumPy)") plt.legend() plt.tight_layout() plt.savefig("model_outputs/loss_curves_rmse.png", dpi=150) plt.close()</p> <h1 id=scatter-no-teste-escala-real>scatter no teste (escala real)</h1> <p>plt.figure(figsize=(5, 5)) plt.scatter(y_test, yhat_te, s=6, alpha=0.4) plt.xlabel("y_true (test)") plt.ylabel("y_pred (test)") plt.title("Dispersão — Test (y_true vs y_pred)") plt.tight_layout() plt.savefig("model_outputs/scatter_test_true_vs_pred.png", dpi=150) plt.close()</p> <p>print("✅ Treino do MLP finalizado. Resultados salvos em 'model_outputs/'.")</p> <p>```</p> <hr> <h2 id=52-principais-resultados>5.2 Principais resultados</h2> <ul> <li>O modelo atingiu R² superior a 0.97 em todos os conjuntos (treino, validação e teste), mostrando excelente capacidade explicativa.</li> <li>As métricas de erro (RMSE e MAE) permaneceram baixas e próximas entre treino e validação, evidenciando boa generalização.</li> <li>O MLP superou amplamente o baseline de média histórica, confirmando ganho preditivo real.</li> <li>A curva de aprendizado revelou convergência rápida e estável, com early stopping atuando de forma eficaz para evitar overfitting.</li> </ul> <hr> <h2 id=53-limitacoes>5.3 Limitações</h2> <p>Apesar do alto desempenho, o modelo possui limitações inerentes ao problema: - Os dados são puramente técnicos (baseados em preços), sem incorporar fatores macroeconômicos, notícias ou indicadores externos. - O modelo não considera dependências temporais de longo prazo, que poderiam ser melhor capturadas por arquiteturas como RNNs ou LSTMs. - A previsão é univariada no tempo (t → t+1), não explorando janelas maiores de histórico, o que pode limitar a sensibilidade a tendências.</p> <hr> <h2 id=54-proximos-passos>5.4 Próximos passos</h2> <p>Para trabalhos futuros, propõe-se: - Expandir o conjunto de features, incluindo indicadores econômicos e dados de sentimento de mercado. - Testar arquiteturas mais profundas (ex.: 128–64–32) e otimizadores adaptativos como Adam e RMSProp. - Comparar o MLP com modelos lineares (regressão, SVR) e sequenciais (LSTM, GRU) para avaliar ganhos de complexidade. - Submeter o modelo a uma competição pública (ex.: Kaggle) para validar seu desempenho em ambiente competitivo.</p> <hr> <h2 id=55-reflexao-final>5.5 Reflexão final</h2> <p>O trabalho demonstrou domínio completo do fluxo de aprendizado supervisionado com redes neurais — desde o pré-processamento até a avaliação crítica do modelo —, consolidando conceitos fundamentais de aprendizado profundo aplicado a regressão. Além do resultado técnico, o projeto reforça a importância de preparar adequadamente os dados e ajustar a arquitetura de forma criteriosa, antes de recorrer a soluções mais complexas.</p> <hr> <h1 id=uso-de-ia>Uso de IA</h1> <p>Usamos IA para correção de código assim como para auxiliar na construção do markdown.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <div class=md-progress data-md-component=progress role=progressbar></div> <script id=__config type=application/json>{"base": "../..", "features": ["content.code.copy", "content.code.select", "content.code.annotate", "content.tooltips", "navigation.instant", "navigation.instant.progress", "navigation.top", "navigation.path", "navigation.tracking", "navigation.expand"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>